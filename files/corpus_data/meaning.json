{
  "labels" : {
    "selected" : 69,
    "candidate" : 218,
    "list" : [
      {
        "id" : 17,
        "text" : "natural language",
        "display" : "natural language",
        "score" : 1039.862,
        "df" : 9,
        "documents" : [ ]
      },
      {
        "id" : 8,
        "text" : "fallacy",
        "display" : "fallacy",
        "score" : 925.8887,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 15,
        "text" : "model",
        "display" : "model",
        "score" : 818.41736,
        "df" : 21,
        "documents" : [ ]
      },
      {
        "id" : 3,
        "text" : "Wittgenstein",
        "display" : "Wittgenstein",
        "score" : 550.62366,
        "df" : 9,
        "documents" : [ ]
      },
      {
        "id" : 37,
        "text" : "dataset",
        "display" : "dataset",
        "score" : 516.18744,
        "df" : 10,
        "documents" : [ ]
      },
      {
        "id" : 4,
        "text" : "aesthetic",
        "display" : "aesthetic",
        "score" : 428.91165,
        "df" : 8,
        "documents" : [ ]
      },
      {
        "id" : 51,
        "text" : "noise",
        "display" : "noise",
        "score" : 415.7372,
        "df" : 5,
        "documents" : [ ]
      },
      {
        "id" : 16,
        "text" : "multi-hop",
        "display" : "multi-hop",
        "score" : 414.90295,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 5,
        "text" : "answer",
        "display" : "answer",
        "score" : 392.05194,
        "df" : 12,
        "documents" : [ ]
      },
      {
        "id" : 19,
        "text" : "reception",
        "display" : "reception",
        "score" : 388.69855,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 20,
        "text" : "rule",
        "display" : "rule",
        "score" : 370.30634,
        "df" : 14,
        "documents" : [ ]
      },
      {
        "id" : 24,
        "text" : "writing",
        "display" : "writing",
        "score" : 343.6819,
        "df" : 12,
        "documents" : [ ]
      },
      {
        "id" : 7,
        "text" : "decision",
        "display" : "decision",
        "score" : 321.7981,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 30,
        "text" : "Qur'an",
        "display" : "Qur'an",
        "score" : 314.45276,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 36,
        "text" : "compositional",
        "display" : "compositional",
        "score" : 310.08536,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 58,
        "text" : "religious",
        "display" : "religious",
        "score" : 266.22104,
        "df" : 8,
        "documents" : [ ]
      },
      {
        "id" : 59,
        "text" : "robot",
        "display" : "robot",
        "score" : 262.04398,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 67,
        "text" : "ugliness",
        "display" : "ugliness",
        "score" : 262.04398,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 43,
        "text" : "hermeneutics",
        "display" : "hermeneutics",
        "score" : 256.78268,
        "df" : 7,
        "documents" : [ ]
      },
      {
        "id" : 48,
        "text" : "logical",
        "display" : "logical",
        "score" : 255.08241,
        "df" : 15,
        "documents" : [ ]
      },
      {
        "id" : 32,
        "text" : "awareness",
        "display" : "awareness",
        "score" : 253.5379,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 31,
        "text" : "Saint",
        "display" : "Saint",
        "score" : 252.72668,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 42,
        "text" : "generic",
        "display" : "generic",
        "score" : 252.72668,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 55,
        "text" : "picture",
        "display" : "picture",
        "score" : 239.59892,
        "df" : 8,
        "documents" : [ ]
      },
      {
        "id" : 38,
        "text" : "disease",
        "display" : "disease",
        "score" : 214.53207,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 12,
        "text" : "inductive",
        "display" : "inductive",
        "score" : 205.66936,
        "df" : 3,
        "documents" : [ ]
      },
      {
        "id" : 44,
        "text" : "images",
        "display" : "images",
        "score" : 197.25146,
        "df" : 17,
        "documents" : [ ]
      },
      {
        "id" : 1,
        "text" : "Christian",
        "display" : "Christian",
        "score" : 194.1234,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 61,
        "text" : "state",
        "display" : "state",
        "score" : 193.31021,
        "df" : 19,
        "documents" : [ ]
      },
      {
        "id" : 63,
        "text" : "subfield",
        "display" : "subfield",
        "score" : 191.77867,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 40,
        "text" : "generalization",
        "display" : "generalization",
        "score" : 185.62604,
        "df" : 7,
        "documents" : [ ]
      },
      {
        "id" : 23,
        "text" : "tokens",
        "display" : "tokens",
        "score" : 185.2777,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 6,
        "text" : "circuit",
        "display" : "circuit",
        "score" : 179.06339,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 49,
        "text" : "mask",
        "display" : "mask",
        "score" : 161.15904,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 29,
        "text" : "Proposition",
        "display" : "Proposition",
        "score" : 160.38489,
        "df" : 12,
        "documents" : [ ]
      },
      {
        "id" : 11,
        "text" : "individual",
        "display" : "individual",
        "score" : 160.05254,
        "df" : 19,
        "documents" : [ ]
      },
      {
        "id" : 39,
        "text" : "elucidation",
        "display" : "elucidation",
        "score" : 158.2072,
        "df" : 3,
        "documents" : [ ]
      },
      {
        "id" : 14,
        "text" : "logic",
        "display" : "logic",
        "score" : 155.89532,
        "df" : 19,
        "documents" : [ ]
      },
      {
        "id" : 60,
        "text" : "senses",
        "display" : "senses",
        "score" : 153.83363,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 22,
        "text" : "soul",
        "display" : "soul",
        "score" : 146.34166,
        "df" : 3,
        "documents" : [ ]
      },
      {
        "id" : 25,
        "text" : "written",
        "display" : "written",
        "score" : 140.2677,
        "df" : 17,
        "documents" : [ ]
      },
      {
        "id" : 9,
        "text" : "field",
        "display" : "field",
        "score" : 138.68219,
        "df" : 18,
        "documents" : [ ]
      },
      {
        "id" : 56,
        "text" : "reader",
        "display" : "reader",
        "score" : 129.83539,
        "df" : 12,
        "documents" : [ ]
      },
      {
        "id" : 64,
        "text" : "transformer",
        "display" : "transformer",
        "score" : 128.19469,
        "df" : 4,
        "documents" : [ ]
      },
      {
        "id" : 33,
        "text" : "body",
        "display" : "body",
        "score" : 126.62088,
        "df" : 14,
        "documents" : [ ]
      },
      {
        "id" : 41,
        "text" : "generation",
        "display" : "generation",
        "score" : 124.88371,
        "df" : 9,
        "documents" : [ ]
      },
      {
        "id" : 28,
        "text" : "Hegel",
        "display" : "Hegel",
        "score" : 120.65692,
        "df" : 7,
        "documents" : [ ]
      },
      {
        "id" : 62,
        "text" : "statement",
        "display" : "statement",
        "score" : 119.45366,
        "df" : 14,
        "documents" : [ ]
      },
      {
        "id" : 46,
        "text" : "input",
        "display" : "input",
        "score" : 117.439476,
        "df" : 10,
        "documents" : [ ]
      },
      {
        "id" : 50,
        "text" : "moral",
        "display" : "moral",
        "score" : 117.017494,
        "df" : 6,
        "documents" : [ ]
      },
      {
        "id" : 21,
        "text" : "said",
        "display" : "said",
        "score" : 109.584145,
        "df" : 17,
        "documents" : [ ]
      },
      {
        "id" : 57,
        "text" : "reading",
        "display" : "reading",
        "score" : 108.81219,
        "df" : 18,
        "documents" : [ ]
      },
      {
        "id" : 10,
        "text" : "generated",
        "display" : "generated",
        "score" : 108.43429,
        "df" : 13,
        "documents" : [ ]
      },
      {
        "id" : 52,
        "text" : "output",
        "display" : "output",
        "score" : 105.37011,
        "df" : 11,
        "documents" : [ ]
      },
      {
        "id" : 35,
        "text" : "causal structure",
        "display" : "causal structure",
        "score" : 101.05857,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 34,
        "text" : "building",
        "display" : "building",
        "score" : 101.052574,
        "df" : 10,
        "documents" : [ ]
      },
      {
        "id" : 26,
        "text" : "Bioethics",
        "display" : "Bioethics",
        "score" : 100.45019,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 66,
        "text" : "type",
        "display" : "type",
        "score" : 87.301384,
        "df" : 19,
        "documents" : [ ]
      },
      {
        "id" : 27,
        "text" : "Ethics",
        "display" : "Ethics",
        "score" : 80.43795,
        "df" : 7,
        "documents" : [ ]
      },
      {
        "id" : 68,
        "text" : "wisdom",
        "display" : "wisdom",
        "score" : 79.1036,
        "df" : 3,
        "documents" : [ ]
      },
      {
        "id" : 65,
        "text" : "transition",
        "display" : "transition",
        "score" : 65.28105,
        "df" : 5,
        "documents" : [ ]
      },
      {
        "id" : 53,
        "text" : "parody",
        "display" : "parody",
        "score" : 56.776196,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 18,
        "text" : "random",
        "display" : "random",
        "score" : 53.92706,
        "df" : 9,
        "documents" : [ ]
      },
      {
        "id" : 45,
        "text" : "information",
        "display" : "information",
        "score" : 53.335415,
        "df" : 15,
        "documents" : [ ]
      },
      {
        "id" : 47,
        "text" : "legal",
        "display" : "legal",
        "score" : 52.408794,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 54,
        "text" : "philosophers",
        "display" : "philosophers",
        "score" : 51.96511,
        "df" : 19,
        "documents" : [ ]
      },
      {
        "id" : 0,
        "text" : "Bakhtin",
        "display" : "Bakhtin",
        "score" : 48.041393,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 2,
        "text" : "Spinoza",
        "display" : "Spinoza",
        "score" : 48.041393,
        "df" : 2,
        "documents" : [ ]
      },
      {
        "id" : 13,
        "text" : "know",
        "display" : "know",
        "score" : 46.37862,
        "df" : 15,
        "documents" : [ ]
      }
    ]
  },
  "documents" : {
    "inScope" : 54,
    "totalMatches" : 54,
    "labeled" : 53,
    "fields" : [
      {
        "name" : "id",
        "type" : "text",
        "id" : true
      },
      {
        "name" : "text",
        "type" : "text"
      },
      {
        "name" : "date",
        "type" : "date"
      },
      {
        "name" : "source",
        "type" : "text"
      }
    ],
    "list" : [
      {
        "id" : 438,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254926851"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Language Models as Inductive Reasoners\n\nInductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, logic language is used as representations of knowledge (facts and rules, more specifically). However, logic language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new task, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of logic language and use pretrained language models as ''reasoners''. Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations.\n\n\nIntroduction\nInductive reasoning is to reach to a hypothesis (usually a rule that explains an aspect of the law of nature) based on pieces of evidence (usually observed facts of the world), where the observations can not provide conclusive support to the hypothesis (Salmon 1989). It is ampliative, which means that the hypothesis supports more than mere reformulation of the content of the evidence (Norton 2005). An example is shown in Table 1 that after observing three carnivorous plants each having a trapping structure, one might reach to a hypothesis (rule) that every carnivorous plant has a trapping structure. Inductive reasoning was firstly proposed by Aristotle in the 4th century B.C. in his Posterior Analytics (Aristotle 1994). Since then it is used as a fundamental tool to obtain axioms, and therefore subjects can be developed from these axioms. It is also recognized as a core component of human intelligence (Mercier 2018 Past research works on inductive reasoning within computer science are investigated by Inductive Logic Programming (ILP) (Muggleton and De Raedt 1994). ILP investigates the inductive construction of first-order logic rules from examples and background knowledge (Muggleton and De Raedt 1994). However, ILP uses logic language as representation and uses symbolic algorithms as method, which results in systematic disadvantages (Cropper et al. 2022). Specifically, ILP systems heavily rely on human effort, since it typically assumes that the input has already been preprocessed into symbolic declarative form, otherwise ILP systems cannot handle raw inputs such as natural language and images. In addition, ILP systems are very sensitive to label error and ambiguity in data, since the final induced rules are required to satisfy all input facts, and symbolic systems can not recognize different symbols with the same meaning (e.g. be capable of, has the capability of, be able to).\nTo overcome the challenges above, we present a novel paradigm for inductive reasoning based entirely on natural language, i.e., inducing natural language rules from natural language facts. In particular, we create a first-of-its-kind natural language inductive reasoning dataset named DEER containing 1.2k rule-fact pairs 1 . Specifically, human-written natural language rule sentences are first collected. Based on the collected rules, we then ask human annotators to collect existing natural language texts as facts from the web where each fact can be possibly enough to induce the given rule. With this dataset, we investigate a modern approach to inductive reasoning where both facts and rules are in natural language, and pretrained language models (PLMs) are used as the inductive reasoner to induce natural language rules from natural language facts. Note that the inductive reasoning considered in this paper has several distinctions considered by other reasoning tasks over text (Clark, Tafjord, and Richardson 2020;Bhagavatula et al. 2020;Sinha et al. 2019). We defer a more detailed discussion to section 2.\nWith natural language as representation and PLMs as the reasoner, such an inductive reasoning system can avoid the systematic disadvantages of logic language and symbolic methods. Specifically, with natural language as representation, it can naturally handle raw input as natural language text. In addition, different from symbolic methods, PLMs 1 We will release our code and data after publication Drosera, which is commonly known as the sundews, is one of the largest genera of carnivorous plants, with at least 194 species. The trapping and digestion mechanism of Drosera usually employs two types of glands: stalked glands that secrete sweet mucilage to attract and ensnare insects and enzymes to digest them, and sessile glands that absorb the resulting nutrient soup.\nIf a plant is carnivorous , then it probably has a trapping structure. Table 1: An example of inductive reasoning in DEER dataset. We embolden the words in facts that contain the key information to induce this rule (just to explain the relation between facts and rule, in DEER there's no special word annotations for fact).\ncontain knowledge via pretraining (Davison, Feldman, and Rush 2019) and use embedding for concepts (Mikolov et al. 2013), making it less affected by input errors (Meng et al. 2021) and more robust to paraphrasing.\nBased on the proposed dataset, we study the PLM's ability to induce (generate) natural language rules from natural language facts. Specifically, we analyze the performance of PLMs to induce natural language rules based on different First-Order Logic (Smullyan 1995) rule types and topics (e.g., zoology, botany, history), with varying input facts and PLM model sizes.\nWe also propose a new framework for this task, named chain-of-language-models (CoLM) which is shown in Figure 1. It draws insights from the requirements of rule induction in philosophy literature (Norton 2005). Specifically, CoLM consists of five modules all based on PLMs, where one model proposes rules (rule proposer M1), and the other four models (M2, M3, M4, M5) each classify whether a generated rule satisfies one particular requirement of induction. In our experiments, we find that our framework surpasses the baselines in terms of both automatic and human evaluations.\nTo sum up, our contributions are three-fold. • We propose a new paradigm (task) of inducing natural language rules from natural language facts, which naturally overcomes three systematic disadvantages of past works on inductive reasoning. In particular, we create a first-of-its-kind natural language inductive reasoning dataset DEER containing 1.2k rule-fact pairs, where fact and rule are both written in natural language. New automatic metrics are also proposed for task evaluation, which shows strong consistency with human evaluation. • We provide the first and comprehensive analysis of how well PLMs can induce natural language rules from natural language facts. • Drawing insights from philosophy literature (Norton 2005), we propose a framework for inductive reasoning. Empirically, we show that it surpasses baselines substantially in both automatic and human evaluations.\n\nDefinition of Inductive Reasoning\nAll non-fallacious arguments (an argument consisting of a premise and a conclusion) can be classified as a deductive argument or inductive argument (Flach and Kakas 2000). If the premise can provide conclusive support for the conclusion, which means that if the premises of the argument were all true, it would be impossible for the conclusion of the argument to be false, the argument is called a deductive argument. Similarly, if the premise can only provide partial support for the conclusion, the argument is called inductive argument (Salmon 1989). Conclusions of inductive arguments amplify or go beyond the information found in their premises (Salmon 1989). In this paper, we call the premises as \"fact\", and conclusions as \"rule\". Sinha et al. (2019) proposes CLUTRR dataset, which requires NLU system to make classification on kinship relations between characters in short stories. In many examples of this dataset, a set of facts that can make conclusive support to the target kinship relation is included in background information as input for each target relation, hence from the philosophical definition (Salmon 1989), these examples require to perform deductive reasoning more than inductive reasoning. Misra, Rayz, and Ettinger (2022) investigates using neural networks for the classification of synthetic language of sentences containing an object and a property. In contrast to our work, they only focus on synthetic language and classification problems. In addition, their classification targets are not more general rules, and most are irrelevant facts compared to input facts. One line of research that is related to induction is \"inductive relation induction\" (Teru, Denis, and Hamilton 2020). However, this task focuses on prediction of relation that involves unseen entities, which only involves an induction from specific entities to specific entities, where we focus on the induction from specific entities or individual phenomenons to general knowledge. Yang and Deng (2021) also works on rule induction, but their induced rule in in quasi-natural language but not natural language. The reasoner they adopted is symbolic, while we use neural methods as PLM as inductive reasoner.\n\nInductive Logic Programming\nInductive Logic Programming (ILP) is a subfield of machine learning that uses first-order logic to represent hypotheses and data. It relies on logic language for knowledge representation and reasoning purposes (De Raedt 2010). We propose a new paradigm that can naturally avoid three systematic disadvantages of ILP (Cropper et al. 2022). If and , then .\nIf or , then .\n\nRelation with Other Reasoning Tasks\nThe goal is quite different from deductive reasoning as given facts and rules and reach to new facts (Clark, Tafjord, and Richardson 2020;Liu et al. 2020;Talmor et al. 2020;Porada, Sordoni, and Cheung 2021). Rather, we want to induce rules from facts, where rules are more general statements than given facts. Our goal is also different from past works on abductive reasoning as given facts and finding the casual reasons for the facts (Bhagavatula et al. 2020). Rather, we want to induce rules that generalize over fact itself and possibly can fit other circumstances.\n\nDataset Collection and Our Proposed Evaluation Metrics\nIn this section, we discuss the data collection process for our proposed dataset and our proposed metrics for automatic and human evaluation of the models developed for the task. In general, we propose two datasets. The first one, termed DEER (inDuctive rEasoning with natural languagE Representation), contains 1.2k rule-fact pairs, where rules are written by human annotators in English, and facts are existing English sentences on the web. The other one, termed with DEERLET (classification of inDucEd rulEs with natuRal LanguagE representaTion), including (fact, rule, label0, label1, label2, label3) tuples, where facts are the same as in DEER, rules are generated output from PLMs, and label0/1/2/3 are classification labels describing different aspects of induced rules. Specifically, rules in DEERLET are collected from GPT-J (Wang and Komatsuzaki 2021) using the in-context few-shot setting. We choose this setting because GPT-J is powerful enough (6 billion parameters) so that a proportion of the generated rules are reasonable, but not very accurate so that these generations and their annotations can benefit the models finetuned on it.\nDEER is used as the main dataset for the task, and DEER-LET is used to measure the classification performance of specific capabilities that are required by inductive reasoning according to philosophy literature (Norton 2005).\n\nDataset Collection of DEER\nWe collect 1.2k natural language rule-fact pairs where rules cover 6 topics and 4 common rule types of First-Order Logic (Russell 2010). The 6 topics are zoology, botany, geology, astronomy, history, and physics. The 4 First-Order Logic rule types are implications with universal quantifier (∀x, condition(x) =⇒ conclusion), implications with existential quantifier (∃x, condition(x) =⇒ conclusion), conjunctive implications with universal quantifier (∀x, condition(x) [∧ condition(x)] + =⇒ conclusion), disjunctive implications with universal quantifier (∀x, condition(x) [∨ condition(x)] + =⇒ conclusion). As we hope to collect rules written in natural language, we translate logic rules to natural language using templates as shown in Table 2.\nNatural language rule is firstly written by human experts, then for each rule 6 supporting facts, which consist of 3 long facts and 3 short facts are collected from existing human-written text from commercial search engines and Wikipedia. Long facts are paragraphs collected from different web pages to ensure their difference, and short facts are core sentences selected from corresponding long facts. Each fact itself should contain enough information that is possible to induce the full corresponding rule (an example of short facts for a rule is shown in Table 1).\nSixty percent of the rules in DEER are more general than any of their facts alone at least in one dimension. We describe this process as \"inducing general rules from specific facts\". However, we find that there are many general statements (also referred to as general fact) of a rule on the web. Therefore, for rule induction systems to be able to utilize both \"specific facts\" and \"general facts\", forty percent of the rules in DEER are equipped with general facts. We describe this process as \"inducing general rules from general facts\".\nTo validate the correctness of the DEER dataset, we randomly split DEER data to 4 subsets, and 4 graduate students manually check each of the subsets on whether each fact contains enough information that is possible to induce the given rule and whether the specifc/general labels are correct. The overall correctness of the sampled DEER data is 95.5%.\nThe reason that DEER is not larger is that it requires experts who are familiar enough with inductive reasoning and possesses a relatively high level of science knowledge to annotate.\n\nDataset Collection of DEERLET\nDEERLET is a dataset collected by a human expert in inductive reasoning for classification tasks to evaluate the specific capabilities required by inductive reasoning. It contains 846 tuples with format (fact, rule, label0, label1, label2, label3). Among the tuples, 546 are used for training, 100 for validation, and 200 for testing. Here, facts are directly from DEER, but the corresponding rules are collected from PLMs, and label0 to label3 are classification labels evaluating specific aspects of the generated rules. The reason in DEERLET we collect rules from the generation of PLMs is that we want to avoid human annotation biases (Amidei, Piwek, and Willis 2020).\nWe develop label 0/1/2 based on the requirements of induced rules in philosophy literature (Norton 2005), and develop label 3 based on a NLP aspect of the problem. In particular, label0 measures whether a rule is not in conflict with its fact; Label1 measures whether a rule fits commonsense;  Table 3: Illustration of the weights and recalls in WRecall, one of our proposed automatic evaluation metrics. Here weights reflect the importance of blocks of generated rules.\nLabel2 measures whether a rule is more general than its fact, as inductive reasoning is \"ampliative\", and requires the induced rule to have higher coverage than facts (Norton 2005). Appendix 7 illustrates label2 with more details. Label3 measures whether a rule is not trivial (mostly incomplete sentence or the latter part is a repetition of its former part). Inspired by Obeid and Hoque (2020), label 0/1/2 are annotated on a 3-point scale (true / partially true / false), and label 3 are annotated on a 2-point scale (true / false). More details on annotation of DEERLET are illustrated in Appendix 7.\n\nAdopted and Our Proposed Evaluation Metrics\nHuman Evaluation Metric DEERLET provides human annotations for evaluation of the generated rules from four different aspects. Here we use precision / recall / f1, and the four aspects in DEERLET for human evaluation, Automatic Evaluation Metric For the DEER dataset, as it requires generating rules based on input facts, the first metric we adopt is METEOR (Banerjee and Lavie 2005), which has been widely used for evaluating machine-generated text quality. Appendix 7 compares METEOR and BLEU (Papineni et al. 2002), and illustrates the reasons why METEOR should be a better metric for this task.\nMore specifically, we calculate the averaged METEOR score of the generated rules (after filtering, if a model had a filtering phase). From the observation that even humans still constantly make mistakes on inductive reasoning, we assume any framework for this task might (but not necessarily) contain two phases as generation and filtering to obtain higher performance. However, if with a filtering phase, ME-TEOR only considers the rules that are not filtered.\nIt makes the METEOR metric here a similar metric to \"precision\", as it only calculates the score for rules that are classified as \"true\". As a result, the model might have a low recall in that it might only keep the rule with the highest confidence score, and classify many reasonable good rules as \"false\".\nTo measure the \"recall\" of inductive reasoning models, we propose \"weighted recall (WRecall)\" as the second automatic evaluation metric for this task. The difficulty lies in that we don't have the ground truth labels for generated rules without human evaluation. To calculate WRecall, we make an assumption, which is that the higher METEOR a rule has, generally the higher probability it is a reasonable rule for given facts. This assumption is reasonable given the relatively high correlation coefficient between METEOR and human evaluation shown in Appendix 7. Specifically, as shown in table 3, we can first calculate the METEOR for each generated rule, and sort them based on the value of METEOR. Then we calculate the recall value for each block of generated rules, during which we assume only the rules in that block have \"true\" ground truth label. We also add a linearly changing weight for each block according to their importance. To ensure WRecall is in the range [0,1], WRecall is linearly normalized: Now that we have a METEOR metric that provides a similar measurement of \"precision\", and WRecall for \"recall\", we propose GREEN (GeometRic mEan of METEOR aNd WRecall) to consider METEOR and WRecall together. It is defined as a geometric mean instead of a harmonic mean because METEOR is not in the range [0, 1]. More specifically, GREEN is calculated as: (2) In general, compared with METEOR, GREEN gives a more comprehensive evaluation of the induced rules. Therefore GREEN can be a more favorable metric when the recall is an important factor (e.g., when computational power is limited). However, when the precision of the induced rules is more favored, METEOR should be a more proper metric than GREEN. Appendex 7 discusses more on the importance of each metric for this task.\n\nMethodology\nIn this section, we formally present the task definition and our proposed framework for natural language inductive reasoning. Figure 1 illustrates the general architecture of our proposed approach.\n\nTask Definition\nDEER dataset is used as the dataset for the natural language inductive reasoning task. The data format for DEER is (rule, f act), where both rule and f act are natural language sentences. The goal of the task is to generate reasonable natural language rules given f act, where the rules should be more general and therefore cover more information than f act.\n\nOur Framework\nHypothetical Induction is an important induction type in inductive reasoning (Norton 2005). It can be understood as when people make observations, they might propose a hypothesis as a general rule that can entail the observations. For example, when people observe that the Sun rises and falls every day, they might induce a hypothesis that the Earth is rotating itself, which is more general than the observations as the hypothesis can also help to explain the observable movements of the other Milky Way stars relative to the Earth. Hypothetical induction fits our task well, as in DEER we also want to induce a hypothesis as a more general rule that  Figure 1: Our proposed framework (CoLM) for inductive reasoning with natural language representation task. Rule Proposer is a generative model based on input facts and desired rule template, aiming at generating (a large number of) rule candidates. Deductive consistency evaluator, indiscriminate confirmation handler, generalization checker, and triviality detector are classification models that filter improper rules according to four requirements of the induced rules in inductive reasoning.\ncan entail the facts. We borrow insights from the requirements for the induced rules in hypothetical induction to develop our framework. Specifically, there are mainly three requirements (Salmon 1989;Norton 2005). The first is that a correct hypothesis should be able to entail deductively as many observations as possible. The second is that the hypothesis should follow the laws of nature, as one could always concoct some imaginary hypothesis that is able to explain the observations but violates reality (e.g., the Earth is the center of the Universe so that the Sun orbits around the Earth). In inductive reasoning, the failure to recognize a rule that runs counter to reality is called \"indiscriminate confirmation\". The third is a basic requirement for inductive reasoning, where the hypothesis should be a more general statement than the observations (Appendex 7 illustrates the meaning of \"general\"). We additionally introduce a fourth requirement from NLP aspects since this task uses natural language as knowledge representation. It is that a rule should not be trivial (e.g. incomplete sentence or the latter sub-sentence simply repeats its former sub-sentence). More concretely, we define the requirements for designing our framework as 1) there should be as fewer contradictions between facts and the rule as possible, and 2) the rule should comply with commonsense, 3) the content in facts should be specific statements that are covered by the rule, 4) the rule should not be trivial.\nBased on this, we develop our framework as shown in Figure 1. It consists of five modules, where module 1 (M1) is the rule proposer, module 2 (M2) is the deductive consistency evaluator, module 3 (M3) is the indiscriminate confirmation handler, module 4 (M4) is the generalization checker, and module 5 (M5) is the triviality detector. Specifically, M1 is in charge of the generation of rules. M2, M3, M4, M5 are independent classification models each verifying rules with different requirement. The role of M2/3/4/5 is similar to the verifier developed for deductive reasoning to make more solid reasoning steps (Yang, Deng, and Chen 2022). The independence of M2/3/4/5 makes it possible to run them in parallel.\nIn practice, we implement all five modules with pretrained language models. We call our implementation as CoLM (Chain-of-Language-Models). The goal of M1 is to generate rules based on the input facts and a given rule template. Thus, M1's input contains facts, a rule template, and prompts that demonstrate the rule induction task.M2 and M4's inputs include prompts that explain the rule-fact compatibility, a rule, and a fact; M3 and M5's input includes again prompts that explain the task and a rule, as their targets are independent of fact.\nMore interestingly, although our framework is solely based on the insights from philosophy literature, we also find a mathematical interpretation of this approach. Here, we denote P (A) as the probability indicating whether A is valid for simplicity. Thus, M2 and M4 jointly measure the validness of a fact given the corresponding rule P (f act|rule) ≈ P M 24 (f act|rule) = P M 2 (f act|rule)P M 4 (f act|rule), M3 and M5 directly measure the validness of the rule itself P (rule) ≈ P M 35 (rule) = P M 3 (rule)P M 5 (rule). By using Bayes' rule, we can easily show that the validness of a rule based on the input fact is P (rule|f act) ≈ PM24(f act|rule)PM35(rule). (3) Note that this score is merely a discrimination score and thus different from the generation probability from M1. In other words, the rules proposed by M1 are then selected by M2/3/4/5 in a Bayesian inference fashion.\n\nExperiments\nIn this section, we discuss the evaluation metrics and baselines and then present the main results of our framework.\n\nEvaluation Metrics\nWe carry out evaluations for the overall framework (the rule generation task with DEER) and individual modules for classification using DEERLET. For evaluation of the rule generation of the overall framework, we use METEOR, WRecall, and GREEN as automatic evaluation metrics; And use precision, recall, f1, and the four metrics in DEERLET as human evaluation metrics. WRecall, GREEN, and the four metrics in DEERLET are our newly proposed metrics for inductive reasoning introduced in Section 3.\nFor evaluation of the classification tasks on DEERLET, we use accuracy, f1, and averaged precision as metrics.\n\nBaselines\nWe use a non-neural method and a neural method as baselines for the framework. We call the non-neural baseline  Table 4: Result of our proposed framework and baselines on DEER under in-context few-shot / finetuning setting. The first three metrics are automatic metrics, and the last seven metrics are human evaluation metrics.\n\"R+F\", as it randomly fills the given rule template with sentences or phases from the given fact. The neural baseline we use is the rule proposer itself in Figure 1. We use majority class and TF-IDF (Jones 1972) as baselines for individual modules. The majority class baseline always predicts \"yes\", which is equivalent to not using M2/3/4/5 to filter rules from M1. TF-IDF is another reasonable baseline as the induced rules contain similar contents compared to input facts. In practice, each input factrule pair is assigned a TF-IDF value, and a threshold for correctness (to compare with the TF-IDF value) is tuned on the DEERLET validation set.\n\nMain Results\nAll modules are implemented with GPT-J (Wang and Komatsuzaki 2021), a pre-trained language model with 6 billion parameters. For better analysis, we conduct the experiments in three settings, including zero-shot setting, incontext few-shot setting (Liu et al. 2021;Brown et al. 2020a) and finetuning setting. The only exception is that we do not test finetuning setting on M1 (the only generative module), since we are mainly investigating (out of box) pretrained large language model's ability. However if with finetuning, language model might perform worse on out-of-distribution data and lose their generality for input facts from different topics (Kumar et al. 2022). For this reason we do not implement with T5 (Raffel et al. 2020) but with GPT-J.\nTo save space, we only report the results of in-context few-shot setting and finetuning setting in Table 4 and Table 5, leaving the zero-shot results in the appendix. The thresholds of M2/3/4/5 used in Table 4 and Table 5 are tuned on the DEERLET validation set. More details on setting up thresholds are illustrated in Appendix 7.\nThe results on DEER are shown in Table 4. As expected, the M1 alone outperforms the R+F baseline across the board, indicating that the PLM has some rule induction capability. Augmenting the M1 with some filtering mechanism can reliably improve the generated rule quality further. Lastly, our full model, CoLM, outperforms all baselines justifying the effectiveness of our proposed framework for natural language inductive reasoning.\nThe results on DEERLET are summarized in Table 5. In this experiment, we investigate the classification performance of language models in terms of different aspects required by inductive reasoning, which includes deductive consistency, indiscriminate confirmation, and generalization  / triviality classification. It shows that TF-IDF achieves the same performance with majority class baseline in accuracy and f1 metrics. The reason is that the best thresholds obtained for TF-IDF are all zero, which means that TF-IDF value is not effective for the four tasks. It also shows that the few-shot GPTJ performs worse than the majority class baseline, while finetuned GPTJ steadily performs better.\n\nAnalysis\nIn this section, we investigate the question of \"how well can pretrained language models perform inductive reasoning?\". Specifically, we provide analyses in terms of rule types, topics, variations of input fact, and scales of language models. Except for Table 9, the input used is short fact, 3 fact, full fact. Except for Table 2, the model used is GPT-J. All experiments in this section are based on the in-context few-shot setting, each averaged by 5 runs. Similar trends are also observed in other settings. We report METEOR and GREEN as metrics in this section.\nDifferent Rule Types Table 6 shows the breakdown evaluation of CoLM based on four basic rule types in logic language (Russell 2010   mapping between the logic forms and corresponding natural language templates can be found in Table 2. The table shows that \"there exists , which \" achieves the best performance. It is reasonable, as simply copying the contents of facts to compose a rule will be acceptable for ∃ quantifier in logic. Table 7 shows the performance of CoLM over different topics. CoLM performs much worse on History and Physics than on the other topics. We attribute the reasons to that the rules in history and physics have high variance, demand a higher level of abstraction, and are not very similar to the input facts. For example, in physics, many rules are natural language descriptions of physical laws such as Newton's law of universal gravitation, while the input facts might be the values of gravitational force and mass of specific objects. In contrast, CoLM achieves better performance in Botany. One possible reason is that many rules in botany can be very similar to the input facts (an example is shown in Table 1 Table 9: Analysis of PLM (GPT-J)'s performance (measured in METEOR / GREEN) with different input lengths and whether fact contains enough information. Table 8 shows the result from specific vs general facts. In section 3 we have discussed that a rule induction system would be more widely applicable if it can utilize both specific fact and general fact. In table 8, general facts cases result in lower performance. We think one of the most possible reasons is that in DEER many general facts do not directly contain the content of the corresponding gold rules. For example, general facts can be mottos from philosophers such as Socrates, and rules can be an understandable description of such mottos in natural language rule format.\n\nVariations of Input Facts\nIn table 9, long facts mean the paragraph-level facts in DEER, and short facts mean the core sentence-level facts selected from corresponding paragraph-level facts. The different number of facts indicates the different number of facts given as input that exhibit similar rule patterns (e.g. Lemon tree / orange tree / apple tree can conduct photosynthesis). We consider the number of facts as an important factor because psychological research shows that more facts with similar patterns can help with inductive reasoning (Heit 2000). Missing fact experiments are also conducted, where for each fact we randomly throw the former half or the latter half of the sentences. It is an important setting as it is hard for the input facts to cover all the elements of the desired rule in a realistic scenario. As a result, it might be common that some required pieces of fact are missing. The results indicate that larger number of concise but full facts are beneficial for rule induction, while too many facts with similar patterns might not be helpful. Figure 2 shows the influence of the scale of pre-trained language models (under in-context few-shot setting) on induction. Here, we consider GPT-Neo 125M, GPT-Neo 1.3B, GPT-Neo 2.7B, GPT-J 6B and GPT-NeoX 20B (Wang and Komatsuzaki 2021). The figure shows that generally performance of M1 steadily improves as the scale being larger, and M2/3/4/5 are only helpful since 6B parameters. The only exception is that both M1 and M2/3/4/5 might reach a plateau in 20B parameters. We did not use GPT-3 (Brown et al. 2020b) to analyze scale since M2/3/4/5 need embeddings for prediction, but the API of GPT-3 does not support return full embeddings.\n\nFuture Work and Challenges\nThe new paradigm of using natural language as the representation of knowledge and using PLMs as the inductive reasoner for inductive reasoning opens the possibility of automatically inducing rules on the countless web corpus. On the other hand, there are still remaining challenges in this direction as not all facts can be used to induce rules. Many fact pieces in DEER for a single rule are collected from different places on the web, so that the input contains enough and proper information to induce rules. However, when using the web corpus, it is hard to ensure that input facts contain such information. As a result, it is challenging to reliably obtain high-quality facts that can be utilized to induce rules.\n\nConclusion\nTo overcome the systematic problems of using logic language for inductive reasoning, we propose a new paradigm (task) of inducing natural language rules from natural language facts, and correspondingly propose a dataset DEER and new evaluation metrics for this task. We provide the first and comprehensive analysis of pretrained language models' ability to induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature, which show in the experiment section that surpasses baselines in both automatic and human evaluations."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-21"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 2792,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254408501"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "A Comprehensive Survey on Multi-hop Machine Reading Comprehension Datasets and Metrics\n\nMulti-hop Machine reading comprehension is a challenging task with aim of answering a question based on disjoint pieces of information across the different passages. The evaluation metrics and datasets are a vital part of multi-hop MRC because it is not possible to train and evaluate models without them, also, the proposed challenges by datasets often are an important motivation for improving the existing models. Due to increasing attention to this field, it is necessary and worth reviewing them in detail. This study aims to present a comprehensive survey on recent advances in multi-hop MRC evaluation metrics and datasets. In this regard, first, the multi-hop MRC problem definition will be presented, then the evaluation metrics based on their multi-hop aspect will be investigated. Also, 15 multi-hop datasets have been reviewed in detail from 2017 to 2022, and a comprehensive analysis has been prepared at the end. Finally, open issues in this field have been discussed.\n\n\n1-INTRODUCTION\nMachine reading comprehension (MRC) is one of the most important and long-standing topics in Natural Language Processing (NLP). MRC provides a way to evaluate an NLP system's capability for natural language understanding. An MRC task, in brief, refers to the ability of a computer to read and understand natural language context and then find the answer to questions about that context. The emergence of large-scale single-document MRC datasets, such as SQuAD (Rajpurkar et al., 2016), CNN/Daily mail (Hermann et al., 2015), has led to increased attention to this topic and different models have been proposed to address the MRC problem, such as (D. Chen et al., 2016) (Dhingra et al., 2017) (Cui et al., 2017) (Shen et al., 2017).\nHowever, for many of these datasets, it has been found that models don't need to comprehend and reason to answer a question. For example, Khashabi et al (Khashabi et al., 2016) proved that adversarial perturbation in candidate answers has a negative effect on the performance of the QA systems. Similarly, (Jia & Liang, 2017) showed that adding an adversarial sentence to the SQuAD (Rajpurkar et al., 2016) context will drop the result of many existing models. Also (D. Chen et al., 2016) pointed out that the required reasoning in the CNN/Daily Mail (Hermann et al., 2015) dataset is so simple that even a relatively simple algorithm can perform well on this dataset. Min et al. (Min et al., 2018) have shown that 90% of the questions in SQuAD (Rajpurkar et al., 2016), are answerable given only one sentence in a document, which does not involve complex reasoning.\nThe above problems seem to be due to the fact that answering the questions of these datasets doesn't require a deep understanding and reasoning (Khashabi et al., 2018). In other words, these datasets have focused only on answering questions based on a single or few nearby sentences of the context, mostly by matching information in the question and the context (known as single-hop MRC). However, in real-world cases, to answer a question it is required to read and comprehend multiple parts of disjoint evidence to find the valid information. Therefore, there are gaps between single-hop MRC, and real-world cases.\nMulti-Hop Reading Comprehension (MHRC) is a more challenging extension of MRC in which it is needed to properly integrate multiple pieces of evidence and reason over them to correctly answer a question. In contrast to the question in single-hop MRC that can be answered by matching information, the multi-hop MRC task requires answering more complex questions based on a deep understanding of the full information. The reasoning ability is considered the first key in multi-hop MRC (Feng Gao et al., 2021). Figure 1 shows two MRC triples. Figure 1(a) shows a single-hop question from (Rajpurkar et al., 2016) that as you can see this question can be answered with one sentence and mostly used matching information in the question and the context (precipitation, fall). Figure 1(b) shows an example from the WikiHop dataset (Song et al., 2020), where the machine has to gather information from three passages and reason over them to choose the correct answer among candidate answers. Passages contain two relevant facts: \"The Hanging Gardens are in Mumbai\" and \"Mumbai is a city in India\" and also some irrelevant facts, like: \"The Hanging Gardens provide sunset views over the Arabian Sea\" and \"The Arabian Sea is bounded by Pakistan and Iran\". As it is clear, finding the final answer in this kind of scenario is more challenging.\n[ In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called \\showers\". The first attempt to improve the simple single-hop MRC dataset happened with emerging of some datasets like TriviaQA (Joshi et al., 2017) and NarrativeQA (Kočisky et al., 2018). These datasets impose more challenges by introducing multiple passages per each question, and also presenting the questions that couldn't be answered with one single sentence. Although they present multiple passages per each question, still the most portion of the question in these datasets could be answered by exactly one sentence or a few nearby sentences within one passage, which means they do not need multi-hop reasoning for most percent of the questions.\nThey are generally known as multi-passage or multi-document dataset that is closer to open-domain Question Answering or retrieving-reading problem, which means models have to focus on retrieving the most related passage and then answer the question based on that instead of reasoning over disjoint information. HotpotQA (Yang et al., 2018a) and WikiHop (Song et al., 2020) can be mentioned as the first and most popular multi-hop datasets which in addition to providing multiple passages per each question, ensure that the question can only be answered by reasoning over disjoint pieces of information. It has been shown that the models with successful results in single-hop MRC datasets have limited success on these datasets .\nRecently, a lot of studies have been focused on different aspect of the multi-hop MRC task. Datasets are a vital part of the MRC task because, without a proper dataset, it is not possible to train and evaluate the models. It can be claimed that the emergence of datasets is the main motivation for much attention and progress in the MRC field after 2016. (Chen, 2018). Usually, the aim of each new dataset is to propose some new challenges that have been neglected by previous datasets, and then the existing models usually can't achieve the acceptable result in new datasets, and are needed to be improved. Therefore, it can be said that the highquality multi-hop datasets promote the multi-hop models as well.\nTo have an accurate view of the growing trend of multi-hop dataset, the multi-hop models have to be considered as well ( Figure   2). Several datasets with more complicated questions than single-hop datasets were introduced in 2017(like TriviaQA (Joshi et al., 2017)). Although they are not considered real multi-hop datasets, but they could attract models in 2018 to pay attention to multihop MRC task. Proposing multi-hop models has been beginning from 2018, but there was a shortage of real multi-hop datasets, so in 2018 more studies focused on creating multi-hop MRC datasets. These datasets made a proper situation to present the multi-hop MRC models, as you can see a significant number of models were been proposed in 2019. The trend of introducing new multi-hop datasets in 2020 to 2022 continued with 7 new datasets. It can be inferred from Figure 2 that whenever a new dataset has been created with new challenges, many studies focus on addressing those challenges, which shows the importance of the datasets. There are some review papers on the MRC task. Liu et al. (Liu et al., 2019) reviewed 85 studies from 2015 to 2018 with a focus on neural network solutions for the MRC problem to investigate the neural network methods in the MRC task. Baradaran, Ghiasi and Amirkhani (Baradaran et al., 2020) presented a survey on the MRC field based on 124 reviewed papers from 2016 to 2018 with a focus on presenting a comprehensive survey on different aspects of machine reading comprehension systems, including their approaches, structures, input/outputs, and research novelties. Thayaparan, Valentino and Freitas (Thayaparan et al., 2020) a systematic review about explainable MRC, from 2014 to 2020 with a focus on the explainable feature of the recent MRC methods. Zhang, Zhao and Wang  presented a survey on the role of contextualized language models (CLMs) on MRC from 2015 to 2019. Bai and Wang (Bai & Daisy Zhe Wang, 2021) presented a survey on textual question answering with a focus on datasets and metrics, they investigate 47 datasets and 8 metrics. Also (Mavi, Jangra, and Jatowt 2022) presented a survey on multi-hop QA with 8 datasets and 29 models.\nAlthough the mentioned surveys investigate different aspects of MRC/QA, none of them have focused on the multi-hop datasets and evaluation metrics. Due to increasing attention to multi-hop MRC, also the important role and the large number of recent studies on multi-hop datasets and evaluation metrics, it is necessary to investigate them separately. Our contribution in this paper is to propose a comprehension survey to investigate the existing studies on multi-hop evaluation metrics and datasets, their growth trend, and also the important challenges in this area. In this regard, we first introduce the problem definition of the multi-hop MRC task and investigate the existing evaluation metrics, then review 15 multi-hop MRC datasets from 2017 to 2022. Also, a fine-grain analysis of the datasets will be presented from different aspects. Finally, open issues in this field have been discussed.\nIt is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering (Zeng et al., 2020), also MRC is known as a basic task of textual question answering (Liu et al., 2019). Figure 3 shows the relationship between QA, MRC and multi-hop MRC (Bai & Daisy Zhe Wang, 2021). The rest of this paper is organized as follows: In Section 2, the definition of the problem and different types of the multi-hop MRC task are explained. In Section 3, evaluation metrics for multi-hop MCR are investigated, Next, in section 4, existing multihop MRC datasets are reviewed in detail and with a focus on their multi-hop aspects. Also, a fine-grain comparison of reviewed datasets will be presented in section 5. Open issues are expressed in Section 6, and finally Section 7 concludes the paper.\n\n2-PROBLEM DEFINITION\nIn general, the multi-hop MRC problem can be defined as: Given a collection of training examples ( ; ; ), the goal is to find a function which takes a context and a corresponding question as inputs, and gives answer as output.\nSince the problem is multi-hop, = ( 1 , 2 , … , ) can be a set of paragraphs (or documents) where denotes the number of paragraphs (or documents) and also question is such a way that needs the multiple disjoint pieces of information from to be answered.\nLike general MRC task, Answer in multi-hop MRC can be in different forms, where have been divided into four categories(D.\n\nChen, 2018):\nSpan-extraction: The span extraction task needs to extract the subsequence from ( ∈ ) as the correct answer of question by learning the function , such that = ( , ).\n\nMultiple-choice:\nGiven a set of candidate answers = { 1 , 2 , … , }, the multiple-choice task needs to select the correct answer from possible answer by learning the function , such that = ( , ).\n\nFree-form:\nThe correct answer is that ⊆ ⊈ . In other words, the answer is not necessarily limited to be a part of the passage. The free-form task needs to predict the correct answer by learning the function , such that = ( , ).\n\nCloze-style:\nThe correct answer is part of the question (usually a word or an entity) that is removed from question. The cloze style task needs to fill in the blank with the correct word or entity by learning the function , such that = ( , ).\nAll General MRC tasks have the potential to be used in form of multi-hop MRC form as well. But as Figure 4 shows, most existing multi-hop datasets focus on the first two tasks (span-extraction and multiple-choice). it should be said that paying attention to the two first tasks is not specific for multi-hop and it happens for general MRC too. The great attention to these tasks may be because they are more natural and similar to the real-world case in comparison to the cloze-style task. Although the free-from task is more similar to the real-world case among all tasks, it didn't achieve proper attention which can be because of the complexity of this task for creating datasets and proposing models.\n\n3-EVALUATION METRICS\nSince evaluation metrics are an important aspect of each task, in this section, the evaluation metrics used in the multi-hop MRC task will be reviewed and investigated.\n\nExact Match:\nThis metric is used to show whether two texts (predicated answer and the ground truth) are exactly the same. If they are exactly the same, EM will be 1, otherwise, it will be 0. This metric is a popular and useful metric in the span-extraction task, as in this task the predicated answer should be exactly matched with the ground-truth answer. F1 Score: The F1 score metric measures the average overlap between the predicted answer and the ground-truth one which is widely used in the span-extraction task. It is calculated with precision and recall measurements, that we briefly show as follows: where TP is True Positive, FP is False Positive, TN is True Negative and FN is False Negative. Consider the predicated and ground-truth answers as a bag of tokens. Accordingly, the definition of these parameter is shown in Table 1. Two previous metrics are common and popular in many NLP tasks as well as general MRC. There are three sets of metrics based on them that are more common and important in multi-hop MRC/QA models that have been introduced by Yang et al. (Yang et al., 2018a). The main goal of multi-hop MRC models is to find the answer then EM and F1 are used to evaluate this task known as the Answer task. Also, multi-hop MRC should present a series of evidence sentences that have led to the final answer known as the Support task as proof of multi-hop reasoning, this task is for evidence sentence extraction and could be seen as a kind of reasoning interpretability (Wu et al., 2021). Also, there is a task to evaluate both above tasks known as the Joint task. Each metric is explained in detail as follows: • Answer F1: It focusses on the answer which is a span of the context. It measures the average overlap between the predicted answer span and the ground-truth one.\n\n•\nAnswer EM: It focuses on the answer which is a span of the context. It is used to show whether the predicated answer span and the ground-truth are exactly the same.\n• Joint EM: is to combine the evaluation of answer span and supporting facts. It is 1 only if both tasks achieve an exact match and otherwise 0.\nAccuracy: Accuracy is a popular and fairly common metric to evaluate the performance of the multiple-choice and cloze-style MRC tasks. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers, and in the Cloze-style task, it is required to check whether the correct words have been selected for the missing words.\nThe accuracy metric is calculated as follows: where N is the number of questions answered correctly, and M is the number of all queried questions. Unlike EM and F1 score, there is no any supporting fact version of accuracy and it is only used for the answer prediction task.\n\nBLEU:\nThe BLEU (BiLingual Evaluation Understudy) metric was firstly proposed by Papineni et al. (Papineni et al., 2002) for machine translation, but it can be easily used in the free-form MRC task too. BLEU is used to calculate the similarity of two sentences. In the free-form MRC task, the predicted answer and the ground truth one is those two sentences whose similarity should be calculated. In this case, the probability of n-grams in the candidate sentence that appear in the ground truth will be calculated: Where ℎ ( ) counts the number of ℎ n-gram appearing in the candidate answer . In a similar way, ℎ ( ) denotes the occurrence count of that n-gram in the ground-truth answer . If the length of the candidate sentence is very short, the accuracy of the BLEU score will decrease. To alleviate this problem, the penalty factor is introduced, which can be calculated as follow (Liu et al., 2019): Finally, the BLEU is computed as: where means n-grams up to length and equals 1 ⁄ . The BLEU score is the weighted average of each n-gram, and in most cases the maximum of is 4, namely BLEU-4.\n\nROUGE-L: ROUGE (Recall-Oriented Understudy for\nGisting Evaluation) has been proposed by Lin (Lin, 2004) and is commonly used for the machine translation and summarization tasks, but it can be used for free-form MRC as well. L in Rouge-L is longest Common Subsequence (LCS). Rouge-L means applying the longest common subsequence to measure the similarity between text and can be calculated as follows (Liu et al., 2019): where is the ground-truth answer, m is the number of tokens of , is the predicated answer, n is the number of tokens of , ( , ) is the length of the longest common subsequence of and , and is a parameter to control the importance of precision and recall .\nMeteor: Meteor (Metric for Evaluation of Translation with Explicit ORdering) has been proposed by Banerjeeand and Lavie (Banerjee & Lavie, 2005). Like two previous metrics, it was introduced for machine translation and can be used for free-form task as well. It is claimed that it has a high level of correlation with human judgments of translation quality in comparison to BLEU. It is calculated as follows: Where is a weighted F-score and is calculated as follows: Also, is calculated as follows: where ℎ is the total number of matching chunks and m is the total number of matched uniforms between the prediction and the ground truth. The parameters , , and are tuned to maximize the correlation with human judgments.\nAll three metrics correspond with the way humans would evaluate the same text and also, and they are language-independent. They have a great potential to use in multi-hop MRC, but there are some weaknesses for them. The most important one is they do not consider the meaning of the words. For example, \"watchman\" and \"guard\" would consider two different words. Besides they do not consider the importance of the words in the corpus for scoring. Figure 5 has summarized metrics and multi-hop MRC. Each metric has some features that make them suitable for one of the MRC tasks described in Section 2, then it cannot be said that one metric is better than the others as we cannot decide which one of the MRC tasks is better than the others. The frequency of usage of the evaluation metrics depends on the datasets. For example, Figure 4 showed that the number of the Span-extraction dataset is the most, then it can be concluded that EM and F1 scores are also the most used evaluation metrics which doesn't show their superiority, but it shows that there are more Span-prediction models and datasets.\n\n4-DATASETS\nIn this section, 115 datasets have been reviewed in detail. We choose the datasets that focus on multi-hop challenges, which means it is required to gather information and reason on multiple disjoint pieces of information to answer the question. Datasets are sorted mainly by release date, and we also start with the simplest datasets. These datasets introduced the question which cannot be answered with a single sentence and require gathering information from more than one sentence. Although they are not considered a multi-hop dataset today, due to the simplicity of the questions in comparison to recent multi-hop questions, they can be considered the first attempt to move from simple single-hop datasets (such as SQUAD (Rajpurkar et al., 2016)) to existing multi-hop datasets, then investigating them in this section will be useful. After a brief introduction of these simple datasets, we have investigated more complex multi-hop datasets that will be reviewed. In the following, at first, each dataset will be introduced separately, in addition to general information, we focus on the points that make them suitable for multi-hop MRC. The end of section, contains a fine-grain comparison of reviewed datasets using several figures and tables.\nNewsQA (Trischler et al., 2017) contains 119,633 plain-text questions on 12,744 news articles from CNN. The answers are spans that have to be extracted from context, but the length of spans is not fixed. This dataset also contains some questions that have no answer to examine the ability of the model to recognize inadequate information. The main goal of NewsQA is to emphasize reasoning behaviors. Table 2 shows the reasoning mechanism needed to answer questions in this dataset in comparison with the SQUAD dataset (Rajpurkar et al., 2016) which is a single-hop dataset. As you can see 20.7% of questions needs synthesizing information distributed across multiple sentences, and 13.2 % of questions need to be inferred from incomplete information or by recognizing conceptual overlap. However, in 27% of questions, a single sentence in the article entails the question, and 32.7% of questions can be answered with word matching. Although the number of complex questions (3 and 4) is more than SQUAD, in comparison to the number of the other type of questions (type 1 and 2) doesn't seem enough. besides, most of the questions in type 3 can be answered with a few sentences in one single paragraph and don't need complicated reasoning.  (Lai et al., 2017) has been collected from the English exams for middle and high school Chinese students and consists of 28,000 passages and 100,000 questions. Passages have different domains such as news, stories, ads, biography, philosophy, etc.\nThe answers should be chosen from four candidate answers where only one of them is correct. One important point about this dataset is that the questions and answers are not limited to being from the original passage, they can be described in any words.\nRACE-M and RACE-H are two subgroups of RACE, where RACE-M denotes the Middle-school examinations and RACE-H denotes the High-school examinations. Some useful statistics of the RACE dataset are shown in Table 3. Also, Table 4 shows the type of reasoning mechanisms of this dataset in comparing two single-hop MRC datasets and also the NewsQA dataset (the mechanism the same as NewsQA). The number of questions from types 3 and 4 is remarkably more than CNN and SQUAD (two single-hop MRC datasets), and also is more than the NewsQA dataset, but still not enough portions of questions in RACE are in type 1(which means don't need reasoning over multiple disjoint information).  TriviaQA (Joshi et al., 2017) contains over 650K question-answer-evidence tuples, the context consists of the web documents and wiki documents while the older dataset like SQUAD (Rajpurkar et al., 2016) is limited to wiki documents and the News is limited to the news article. Some general statistics of TriviaQA are shown in Table 5. One of the advantages of this dataset is the syntactic and lexical variability between questions, answers, and context sentences which makes the reasoning difficult. Besides, there are six documents per question and also 40% of the questions need reasoning over multiple sentences, which is about three times more than SQUAD, and also is more than NewsQA (Trischler et al., 2017) and RACE (Lai et al., 2017). Figure 6 shows the percentage of the multi-sentences question in 5 datasets. However, most of these questions can be answered by a few nearby sentences in one single paragraph, and does not require complex reasoning (Yang et al., 2018b). But this dataset is still (2021) used by some multihop MRC models that shows the importance of this dataset.   (Bollacker et al., 2008), but since the questions are simple, they first generated automatically questions that are understandable to AMT workers, and then have them paraphrase those into natural language. They make sure that questions include phenomena such as composition questions (45%), conjunctions (45%), superlatives (5%), and comparatives (5%). A drawback of this method for question-generating is that because queries are generated automatically the question distribution is artificial from a semantic perspective.  (Talmor & Berant, 2018) OpenBookQA (Mihaylov et al., 2018) consists of 6000 plain-text questions based on 1326 elementary-level science facts. The answers are plain text which should be chosen from multiple candidate answers. Unlike other datasets that are self-contained answers, an open book fact and broad common knowledge are used together to answer the questions they require multi-hop reasoning over core facts and common knowledge to answer the question. However, it should be said that reasoning over a whole document is rarely needed actually, and also it is unclear how many additional facts are needed (Khot et al., 2020). Some usefull statistics of OpenBookQA are shown in Table 6.  (Mihaylov et al., 2018) # of questions 5957\n\nVocabulary (q+c+f) 12839\nAnswer is the longest choice 1108(18.6%) Answer is the shortest chioce 216(3.6%) MultiRC (Khashabi et al., 2018) consists of ∼9k high-quality multiple-choice RC questions. It has been ensured that answering each of the questions requires reasoning over multiple sentences. This dataset is constructed using 7 different domains (news, Wikipedia articles, Articles on society, law and justice, Articles on history and anthropology, Elementary school science textbooks, 9/11 reports, and Fiction). You can see the statistics of this dataset in Table 7. Answering 60% of the questions of this dataset requires reasoning over multiple sentences. also, the required information to answer the questions is not explicitly stated in the context but is only inferable from it (e.g., implied counts, sentiments, and relationships). The number of multi-sentence questions is 5825 (from 9,872) and the number of sentences that are used to answer a question is 2.58 on average, these sentences are not continuous and the average distance between them is 2.4. However, this dataset has focused on passage discourse and entity tracking, rather than relation composition (Khot et al., 2020).\n\n2.58\nAverage number of sentences used for questions\n\n2.37\nAverage distance between the sentences used for each question\n\n2.4\nQAngaroo (Welbl et al., 2018) is composed of two multi-hop reading comprehension datasets: MedHop which is a cloze-style dataset, and WikiHop which is a multiple-choice dataset. The MedHop dataset is about molecular biology and consists of 1.6K instances for training, 342 instances for development, and 546 instances for testing. WikiHOP dataset consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia. The size of WikiHop and MedHop datasets for different learning phases are shown in Table 8. In these datasets, models have to combine evidence across multiple documents and perform multi-hop inference. WikiHop is more popular Which is probably due to being open-domain and also the number of its questions. Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity. As Table 9 shows, for 45% of cases, the answer can be found from multiple contexts, and for 9% of the cases, a single document suffices. Also, for 26% of cases, more than one candidate is plausible, (this is often due to hypernymy).\nHowever, the questions of the WikiHop dataset are not in natural text form, which makes it different from real-world cases.\nAlso, it is created by documents (from Web or Wikipedia) and a knowledge base (KB), as a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited (Yang et al., 2018a). also, these datasets have no information to explain the predicted answers (Ho et al., 2020).   (Welbl et al., 2018) Type of question Percentage (%) multi-step answer 45 Multiple plausible answers 15 Ambiguity due to hypernymy 11 Only single document required 9 HotpotQA (Yang et al., 2018a) has become one of the most popular multi-hop MRC datasets in recent years. It has 113k Wikipedia-based question-answer pairs. To answer the questions of this dataset, it is required to collect and reason over information from multiple supporting documents. In contrast to WikiHop (Welbl et al., 2018), the question and answer are in plain text form. The answers are variable-length spans, and there are 10 wiki passages per question. One of the distinguishing features of this database is that there are sentence-level supporting facts for each answer which is the sentences containing information that supports the answer. Thus, models should extract a span as the answer and also provide the supporting facts for the answers. The types of answers are shown in Table 10.\nHowever, there are some drawbacks to this dataset, for example, QASC (Khot et al., 2020) discussed that since the questions of this dataset were authored in a similar way, due to their domain and task setup, they are easy to decompose, Also, as discussed in Inoue et al (Inoue, 2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the reasoning and inference skills of the model. Besides, (Chen and Durrett, 2019;Min et al., 2019) revealed that many examples in this dataset do not require multi-hop reasoning to solve (Ho et al., 2020).  (Inoue, 2020) is proposed to evaluate the internal reasoning of the reading comprehension system in HotpotQA (Yang et al., 2018a). As it's said before, HotpotQA requires identifying supporting facts (SFs), but since only a subset of SFs contributes to the necessary reasoning, thus, achieving high accuracy performance in the Support task (it has been explained in section ) cannot prove a model's reasoning ability. R 4 C contains 4,588 questions and aims to evaluate a model's internal reasoning in a finer-grained manner than the Support task in HotPotQA. It requires giving not only answers but also derivations. A derivation is a semistructured natural language form that is used to explain and justify the predicted answers. Each question is annotated with 3 reference derivations (i.e., 13.8k derivations). However, the small size of the dataset is a serious limitation to train and evaluate end-to-end systems (Ho et al., 2020). Table 11 shows the statistics of the R 4 C corpus. \"st.\" denotes the number of derivation steps.\nEach instance is annotated with 3 golden derivations. 2WikiMultiHopQA (Ho et al., 2020) is a multi-hop dataset based on Wikipedia and WikiData 1 , and consists of 192k multihop questions. An important point about this dataset is that it contains both structured and unstructured data. There are four types of questions in this dataset: 1) Comparison questions which are used to compare two or more entities, 2) Inference questions to infer relation r from the two relations r1 and r2, 3) Compositional questions to achieve the answer from an entity and two relations without any inference relations, 4) Bridge comparison which requires finding bridge entities. each sample contains some information, namely evidence that is used to explain the answer. Evidence is in from of triples, where each triple is structured data (subject entity, property, object entity) obtained from WikiData. You can see the number of examples in each type along with the average length of questions and answers of 2WikiMultiHopQA dataset in Table 12. This dataset like HotPotQA (Yang et al., 2018a) uses the Answer, Support and Joint evaluation metrics.    Type of reasoning Percentage (%)  (Khot et al., 2020) is a multi-hop dataset that contains 9,980 8-way multiple-choice questions from elementary and middle school level science, and ach question is produced by composing two facts from an existing text corpus. To generate this dataset, crowd workers have first received only the first fact fS. Then, they have freely chosen the second fact fL from other available facts. Finally, the questions have been created by compromising these two facts. Thus, the models require commonsense reasoning for the composition of these facts to find the answer. You can see the number of questions, and unique fS and fL in Table   14. In contrast to OpenbookQA that which the number of facts to answer a question is unclear, in QASC, it is clear that two facts are sufficient to answer a question. The number of questions, fS and fL has been shown in Table 14. Also a comparison information among this dataset and previous multi-hop dataset has been shown in Table 15. As you can see QASC in addition to preparing supporting facts, annotated them too, also Decomposition is not evident.\n\nSupporting facts are annotated\nMuSeRC (Fenogenova et al., 2020) is the first multi-hop MRC Russian dataset. The questions in Russian Multi-Sentence Reading Comprehension (MuSeRC) dataset rely on multiple sentences and commonsense knowledge. It contains more than 900 paragraphs and 12,805 sentences across 5 different domains, namely: (1) elementary school texts, (2) news, (3) fiction stories, (4) fairy tales, and (5) summaries of TV series and books (Figure 8). In this dataset, the answers are not necessarily a span of the passage, and there may also be more than one answer to a question. Thus, the models should choose the best possible answer.\nQuestions and answers have been created by crowd workers and, questions that can be answered with only one sentence have been removed to ensure that all questions are multi-hop. Although the number of the question in MuSeRC are less than MultiRC (Khashabi et al., 2018) but all of its questions are multi-hop (less than 60% question of MultiRC is multi-hop question), you can see some comparison information between MuSeRC and MultiRC in Table 16.  (Fenogenova et al., 2020)  ComQA ) is a large-scale Chinese dataset that contains more than 120k human-labeled questions. They focus on compositional QA which means the answers have been obtained from multiple but discontinuous segments in the documents and the answers are not limited to one span but can be a combination of several separate sentences. Besides, as in this dataset HTML pages are used as the context, even a table or an image can be selected as a part of the answer. To generate the ComQA dataset, a question-document pair is obtained first from a Chinese search engine called Sogou Search 2 , the content of the retrieved page is then converted into a list of sentences, and the crowd workers find the answer to the question through the context. In Figure   9, you can see the frequency and type of questions in this dataset.\nThere are different kinds of questions in this dataset: 1) Simple: questions about a single property of an entity. 2)Compositional: questions that require answering more primitive questions and combining them. 2)Temporal: questions that require temporal reasoning for deriving the answer. 3)Comparison: questions that need some kind of comparison. 4)Telegraphic: short questions in an informal manner similar to keyword queries. 4) Answer tuple: Where an answer is a tuple of connected entities as opposed to a single entity. Table 17 shows Results of the manual analysis of 300 questions.      For the last part of this section, Figure 10 has been prepared to summarize of the reviewed datasets. NewsQA, Race, and TriviaQA has known as multi-passages datasets but because of the multi-sentences question, they were reviewed here. The rest of the datasets has known as multi-hop datasets based on the problem definition in section 2.\n\n5-ANALYSIS:\nAfter reviewing each dataset in detail, some fine-grained comparisons will be presented in this section by preparing some figures and tables.\n\n5-1 Frequency\nFirst of all, the frequency usage of each dataset in recent multi-MRC models has been shown in Figure 11 (we used the same studies as Figure 2 which presented as Appendix1). As you can see, HotpotQA (Yang et al., 2018a) and WikiHop (Welbl et al., 2018) have received the most attention among the multi-hop datasets. There are several reasons for this: 1. They are considered pioneers in multi-hop MRC because They were the first successful and serious attempt to cover multi-hop challenges and they attract a great attention to the multi-hop MRC task.\n\nMulti-hop datasets (multi-hop questions)\nNewsQA (Trischler et al., 2017) Race (Lai et al., 2017) TriviaQA (Joshi et al., 2017) COMPLEXWEBQUESTIONS (Talmor & Berant, 2018) OpenBookQA (Mihaylov et al., 2018) MultiRC (Khashabi et al., 2018) WikiHop (Welbl et al., 2018) MedHop (Welbl et al., 2018) HotpotQA (Yang et al., 2018a) R 4 C (Inoue, 2020) 2WikiMultiHopQA (Ho et al., 2020) HybridQA (W.  QASC (Khot et al., 2020) MuSeRC (Fenogenova et al., 2020) ComQA ( 2. Although some datasets were introduced recently, since these two datasets have been used by many multi-hop models, they provide the good situation for comparing the performance of the models. Therefore, models would like to use these two datasets to compare their performance with other models, and this leads to more use of these two datasets. We also prepare Figure 10 to investigate the growth trends of HotpotQA and WikiHop from 2018 to 2021 (note that both datasets have been introduced in 2018). As you can see, HotpotQA is more popular than WikiHop, there are some reasons for it: 1. Questions in the WikiHop dataset are in triple form, while questions in the HotpotQA dataset are in the plain text form, which makes it more similar to real-world questions.\n2. Answers in HotpotQA are in the form of Span-extraction while answers in WikiHop are multiple-choice. As mentioned in Section 2, the Span-extraction task is more popular among models so the frequency usage of HotpotQA will be more.\n3. The HotpotQA dataset presents a new task called the Supporting fact task while WikiHop only focuses on the Answer task. So HotpotQA prepares a good situation to evaluate the ability of reasoning of the models.\nBesides, as Figure 12 the number of usages of both datasets is maximum in 2019, which is due to the fact that the introduction of these two datasets in 2018 caused a lot of attention to the multi-hop MRC task, then in 2019, many multi-hop MRC models were proposed based on these two datasets. (It has been shown in Figure 2 too). In the following years, due to the introduction new multi-hop datasets, the usage of these two datasets got decreased but, both datasets are still the most popular datasets for multi-hop MRC models.  Also, there is a feature about how many portions of each dataset are multi-hop. As it has been said before, we begin with some simple datasets that the questions can't be answered with a single sentence, although the sentences are within one passage and only a portion of these datasets include multi-sentence questions. These datasets are considered the first attempts to move from the single-hop datasets. (Note that some datasets are still used by recent multi-hop models, for example, DRNQA (X. Li et al., 2020) which has been published in 2020, has used TriviaQA (\n\n5-3 Result\nAs the last review of multi-hop datasets in this section, the stat-of-the-art result of existing multi-hop models on the reviewed multi-hop dataset will be shown. we considered the multi-hop MRC models from 2020 to 2021. As shown in Section X, the two MRC tasks, Span-extraction and Multiple-choice, have received the most attention among datasets, so we will review them separately.It should be noted that due to the different models, the results are not expressed to compare the performance of datasets, but only to show the state-of-the-result on each database.\n\n5-3-1 Span-extraction datasets:\nSince the answers are in the span-extraction form the EM and F1 are used as the evaluation metrics (Answer, Support, and Joint task). Table 21 shows the stat-of-the-result of the span-extraction dataset alongside the models. HotPotQA (Yang et al., 2018a) and 2WikiMultiHopQA (Ho et al., 2020) prepared all three task evaluation results while TriviaQA (Joshi et al., 2017), MuScRC (Fenogenova et al., 2020), and Hybrid (W.  only focus on the Answer task. TriviaQa (Joshi et al., 2017) DRNQA (X. Li et al., 2020) 59.73 62.21 MuSeRC (Fenogenova et al., 2020) MuScRC (Fenogenova et al., 2020) 25.6 65.6 HybridQA (W.  HybridQA (W.  43.8 50.6 Table 22 contains the stat-of-the-result of multi-hop models on multiple-choice datasets. Since the answer type of this dataset is multiple-choice then Accuracy is the evaluation metric on the test and development set.\n\n6-OPEN ISSUES\nDatasets with the new challenges are a good motivation to improve current models. So, to reduce the gap between existing multi-hop MRC models and real-world cases, creating the datasets with more challenges is vital. Even the most popular multi-hop datasets still have shortcomings, then if models only focus on the existing dataset, the gap between them and real-world cannot be properly reduced, so it is important to focus on building datasets.\nThe lack of free and close-style datasets has made it impossible to present models for these two tasks, while both tasks have a good potential to use in form of multi-hop MRC.\nAmong all the types of MRC tasks described in Section 2, the free-form task is the most similar task to the real-world case but due to its complexity, there is no proper dataset for it. Since this type of answer is more similar to real-world scenarios, focusing on presenting free-form datasets to encourage models can improve the application of MRC systems.\n\n7-CONCLUSION\nIn this study, a comprehensive survey on the multi-hop MRC evaluation metrics and dataset as two vital and important aspects of multi-hop MRC has been presented with a focus on the existing evaluation metrics and datasets, their growth trend and the important challenges of this area. In the beginning, the definition of the multi-hop MRC problem has been presented then the evaluation metrics have been investigated in detail. In following, 15 multi-hop datasets from 2017 to 2022 had been reviewed in detail then and some comprehensive analyses were presented for a fine-grain comparison of the different features of datasets. Finally, open issues in this field had been discussed."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-08"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 5681,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255049872"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Analysis of the Building Ornaments of the Pendopo Agung of Majapahit (Mojokerto, East Java) and Mataram (Pura Mangkunegaran Solo, Central Java)\n\nThis study raises the comparison of the building of the Great Hall of Majapahit (Mojokerto, East Java) and the Great Hall of Mataram Pura Mangkunegaran (Solo, Central Java). philosophy in a different way. Both are historical buildings that are directly related to the history of the Mataram Kingdom and the Majapahit Kingdom, which in general, the two kingdoms still have ties to one another due to the journey of the Hindu-Buddhist religion. Inside the interior of the Pendopo Agung there are several ancient ornaments which have an aesthetic function and symbolic meaning. This study aims to analyze the comparison of forms and aesthetic and symbolic functions of the ornaments of the Great Hall. The method used is descriptive method. Data collection was obtained from observation and literature. The emergence of the Pendopo Agung building is a work created from generation to generation, guided by predetermined standard rules both technically and religiously which are interrelated to the Javanese standard. The conclusion of this study is to add insight that in each ornament on the same building has a way of conveying different meanings to the people, these differences can later be used as points of interest for each building of the Great Majapahit and Mataram Halls.\n\n\nINTRODUCTION\nIndonesia has a lot of cultural heritage that is diverse and full of philosophical values in it along with the vast expanses of the archipelago. Starting from the architecture of the Banda Aceh part to Papua. In Javanese architecture, there are decorative ornaments in the interior as aesthetic and symbolic for the Javanese people as a benchmark for building Javanese buildings. Ornament or decoration is a work of art inspired by an object whose shape is changed in such a way as to fulfill a certain purpose or function. Ornaments themselves consist of two types, namely modern and traditional ornaments. Just as modern ornaments are compositions between several combinations of traditional ornaments and present-day ornaments into a certain motif, while traditional ornaments are ornaments that have developed since ancient times/ancestors which have been preserved and guarded from generation to generation until today (Sunaryo A, 2009). One of the traditional ornaments whose characteristics are still preserved is the Great Hall ornament. The existence of many Javanese architectural objects, Pendopo Agung, are only objects of heritage from the past, objects of historical heritage, which over time are eaten away by age. In general, the typical Javanese architecture of the Great Hall has several different versions in each region. According to Santosa, R. B. (2007) it is stated that there are two versions of the building of the Great Hall, namely the Great Hall of Mataram and the Great Hall of Majapahit. M Ruth, 2020 revealed that the Mataram and Majapahit kingdoms were still connected to one another because at that time there had been the influence of the entry of Hindu-Buddhism in the Majapahit era, the interconnection between the two kingdoms was allegedly at that time in the 16th century the Majapahit kingdom experienced the collapse caused by the disintegration of the region which eventually emerged a new dynastic kingdom, namely the Demak kingdom followed by the development of the Pajang Mataram kingdom in its time. That is why the Great Pendopo Building still has continuity between the Majapahit Great Hall and the Mataram Great Hall. Broadly speaking, the visual outline of the Pendopo Agung is one of the typical Javanese joglo buildings in the form of a building that has no walls with a number of pillars or poles that broadly Analysis of the Building Ornaments of the Pendopo Agung of Majapahit (Mojokerto, East Java) and Mataram (Pura Mangkunegaran Solo, Central Java) Trianita Anugerah Setiawan, Wanita Subadra Abioso Copyright (c) 2022 Trianita Anugerah Setiawan, Wanita Subadra Abioso 445 serve to support/momot, and has several ancient ornaments and collections of historical items in it. According to the Javanese genealogy, momot on a pavilion pole means to protect or accommodate all things (Hidayatun, M.I. 2004). In Figure 1 in the building structure at the highest end of the building Pendopo Agung is referred to as \"Brujung Gajah\" and \"Mala\", then in the middle of the building it is referred to as \"Pananggap\", the front of the Pendopo which functions as a drop off area is called \"Pangarak/ Tritis/Kuncung\" at the bottom after the middle part is referred to as \"Panitih/Emper\". This research includes several similar studies that discuss decorative ornaments. In the previous study, the discussion of traditional ornaments led to meanings with widely different functions, shapes and colors. The difference between this research and other similar studies is that in the content of the discussion, this research specifically discusses the aesthetic and symbolic functions of the Pendopo Agung ornaments as a whole, where the layout, colors, and symbolic aesthetic functions are contained in these ornaments. The discussion specifically can make people think critically in placing the placement of traditional ornaments on the rooms. Javanese decorative ornaments are located in several parts of the Pendopo Agung Mojokerto building. East Java, such as on the roof truss beams, door edges, windows, upper and lower building pillars, doors, as well as in the middle of the pillars or at the intersections of building beams. There are several types of ornaments on the building of the Pendopo Agung Majapahit consisting of; Flora Ornaments (Lung-Lungan, Wajikan, Saton, Tlacapan, Padma and Patron), Fauna Ornaments (Makutha and Peksi Garuda) and Natural Ornaments (Praba and Banyu Teles). Whereas in the decorative ornaments of the Mataram Agung Hall, Mangkunegaran Temple, Solo, Central Java, there is a difference in the building ornament which is more specific to the Mataram kingdom, namely the Kumudawati ornament, which is the god of life. The Kumudawati ornament means living in supernatural powers. This research is devoted to the form and function of ornament as a typical Javanese symbolic aesthetic to find out the values contained in it so that it can be understood by the wider community. The data sources used are literature data from related studies and observations.\n\nMETHOD\nThe research method used for this basic research is descriptive research method. The descriptive method is used to describe and explain in detail the various kinds of traditional interior ornament data from the building of the Pendopo Agung Majapahit Trowulan , Mojokerto, East Java and the Analysis of the Building Ornaments of the Pendopo Agung of Majapahit (Mojokerto, East Java) and Mataram (Pura Mangkunegaran Solo, Central Java) Trianita Anugerah Setiawan, Wanita Subadra Abioso Copyright (c) 2022 Trianita Anugerah Setiawan, Wanita Subadra Abioso 446 Mataram Pendopo Agung, Pura Mangkunegaran, Solo, Central Java, which refers to the function of the location of each of these interior ornaments on the parts of the building. The data obtained in this study came from direct observation, activity interviews, and data collection from related literature studies.\n\nRESULT AND DISCUSSION\nPendopo Agung is an open building located in front after Kuncung. When viewed from a vertical arrangement, the Jawi house is divided into three parts, namely the roof, pillars or walls, and singing or ompak. This arrangement is a transformation of the temple which is interpreted as a symbol of the upper world (gods), the middle world (life) and the underworld (death). The Pendopo Agung is generally a place for receiving guests, practicing karawitan, a place for gatherings, as well as a place for storing historical goods/relics of former ancestors. According to Javanese philosophy, the momot on the pavilion pole has a meaning, namely to protect or accommodate all things. The function of the Pendopo Agung Mojokerto Building is as a place to receive guests. The pillars that support this broad roof mean that the occupant of the Pendopo Agung is a wise person who is able to be generous and accommodate the various problems of his guests. Before discussing the ornaments, in general, the Javanese part of the building is one thing that absolutely must be known, therefore the research on the Javanese architecture of the Pendopo Agung itself has several sections in it such as; pavilion, peringgitan, dalem, sentong, bale roto/kuncung, pegong, and tratag. The parts of this building have meanings, among others:\n\nPendopo\nIn general, the pavilion is located in front, and is open as a gathering point or a place for people to gather. The size and shape of the pavilion building can reflect the rank, rank and degree of the owner 2. Peringgitan Taken from the word inggit which means building puppets, usually to hold puppet shows.\n\nDalem\nIs the arrangement of space in a Javanese building house. Its function as a family room that provides a calm and dignified atmosphere\n\nSentong\nThere are three rooms in a row, namely, sentong kiwo and sentong tengen as bedrooms and storage of valuables, while sentong tengah provides an atmosphere of worship of God to always provide a prosperous family atmosphere.\n\nBale Roto/kuncung\nIt is a drop off place or vehicle stop to drop off guests to the pavilion 6. Pagongan It is a terrace or in Javanese it is called an overhang for guests before entering the pavilion 7. Tratag Is a special space that is located between the pavilion and peringgitan to lower and place the vehicles of occupants inside (permanent)\n\nOrnaments / Ornamental Variety\nThe decoration of an ornament contained in Javanese architectural buildings aims to restore and restore ancestral beliefs, as well as realizing a purposeful relationship with God through symbols of greatness or rising upwards with meaning as protection. Ornaments / decorations are usually Derived from the word Lung which means the stem of a creeping plant and is still young, so it has a curved shape. The placement on the building is on the beams of the house, firewood, windows, doors, tebeng, patang aring, and door leavesDerived from the word Lung which means the stem of a creeping plant and is still young, so it has a curved shape. Placement on the building is on the beams of the house, firewood, windows, doors, tebeng, patang aring, and doors.  Derived from the profile shape of the Buddhist throne in the form of a lotus flower. Sunggingan (plain) color. Usually located on upak as plinth.\n\nDADA PEKSI\nThe chest peksi or chest manuk at the Trowulan Agung Hall is a cross beam that is located in the middle of the \"pemindangan\" or is located in the middle of the terraced tumpeng sari. Peksi chest serves to support the construction of Mala/Molo. The central chest is given a beautiful carving to give an aesthetic impression and has a meaning in terms of the Majapahit-an Trowulan, namely from the term \"dada peksi\" is \"dada\" and \"bird klau\" which are combined to resemble a bird's chest which means that the ornament in the middle a village that seems to expand as the sustenance of its inhabitants is well preserved from nature (food and clothing), humans and humans, as well as disputes with God.\n\nUMPAK\nThe umpaks or rocks at the Agung Trowulan Hall are the supporting mats of the Saka Guru which function to maintain the balance of the Pendopo building so that it is not shaken by an earthquake. Umpak itself has a meaning like humans who have footwear or shoes or sandals, which means that a leadership is not strong if the subordinates are not covered so as to form a strong unit. On the pedestals there is usually an ornamental floral ornament with a blooming flower pattern called padma. Padma is a red lotus flower, symbolizing a purity, strength and firmness that is not easily shaken by all kinds of disasters that befall.\n\nLung-Lungan\nLung-lungan is a decorative variety in the form of tendril-like vines. At the Trowulan Agung Hall where the Lung-Lungan ornaments are on the beams between the tumpeng sari, the ornaments on the Trowulan great hall have a neutral color, namely teak wood in accordance with the identical heritage of the Majapahit kingdom. The ornament of lung-lungan is made to bend left and right up and down. It means that we as human beings live in society, always remembering God who always loves goodness in social life. The floral motif which is in the middle of the lungs of the great Trowulan pavilion also symbolizes love which says that towards others, the environment or nature, and towards God.\n\nTLACAPAN\nThe tlacapan decoration on the grand pavilion of Trowulan is visualized by sunlight or sparkling light. Tlacapan is in the form of a triangle which is arranged into three repetitions, the location of the placement of the tlacapan ornament on the Trowulan grand pavilion is located on the saka guru or the 4 main pillars of the Trowulan grand pavilion building. The colors and materials are the same as before, using neutral colors and still original teak wood. In its philosophy, the meaning of the tlacapan ornament has the meaning of brightness or majesty as visually depicted by the tlacapan ornament itself which is in the form of light.\n\nPRABA\nThe praba ornament on the great Trowulan pavilion has a slightly different shape from the praba form in general, visually as shown in the picture beside it looks more tapered than the praba shape in general. the great hall of Trowulan, but this form does not reduce the meaning contained therein. The location for placing praba is on the lower saka guru pole above the pedestal. Praba means a light is a symbol of Tri Hita Karana which describes the concept of the philosophy of human life with the beginning of its birth, then life after birth, and the return of humans to their God. Which broadly explains the concept that humans who are born then enjoy their lives and will meet their death.\n\nSATON\nThe Saton ornament on the Trowulan grand pavilion has a square visual with flower/leaf decorations. The neutral color or the original color of the teak wood material on the saton ornaments in the Trowulan grand pavilion displays the legacy of the Majapahit kingdom. Ornament Saton is at the corner between the pillars and roof support. The meaning contained in the saton ornament is a symbol of unity.\n\nAnalysis of the Ornaments of the Pendopo Agung Mataram Puramangkunegaran Solo, Central Java\nThe Meaning of the Ornaments of the Great Hall of the Mangkunegaran Temple, Solo Field\n\nAnalysis of Meaning and Function of Ornament Structure\nThe saka guru has 4 pillars in the middle which function to support the intercropping but different from the intercropping inherited from the Majapahit kingdom, the intercropping on the legacy of the Mataram kingdom has a unique motif shaped like the tip of a fire in the middle of the intercropping there is a rectangular motif as a magical and religious symbol. The ornament is called Kumudawati which is a condition with strong Javanese philosophical values related to kejawen, namely teachings and philosophy regarding the Javanese policy of living in a certain way. The Kumudawati ornament means the tip of fire which is based on the Throne of \"Shiva\" which is the god of life. The Kumudawati ornament has a meaning as living in supernatural powers because it is directly related to closeness to God in order to avoid all dangers and evil. The Kumudawati ornament came out because during the reign of Mangkunegara VII Indonesia was still under Dutch colonialism, which later Mangkunegara VII wanted to show that Indonesia, especially the Javanese people, had their own personality and culture that was different from western culture. The letter A depicts a Kumudawati ornament named pethak (white). The color of the petals or white is usually referred to as Manikmaya which means rejection of feelings of disappointment in the heart and having a sense of tolerance for others. This white color encourages to always do good and can prevent bad deeds. If it is linked to the Tri Hita Karana concept, this meaning is directly related to the relationship between humans and humans by always doing good for Analysis of the Building Ornaments of the Pendopo Agung of Majapahit (Mojokerto, East Java) and Mataram (Pura Mangkunegaran Solo, Central Java) Trianita Anugerah Setiawan, Wanita Subadra Abioso Copyright (c) 2022 Trianita Anugerah Setiawan, Wanita Subadra Abioso 456 others, then an attitude of mutual love for fellow humans grows. The letter B depicts a Kumudawati ornament named dadhu (orange). The name is beadhardhataya which means rejecting doubt, worry, being kept away from bad prejudice and accusations, and as an antidote to poison. The letter C depicts an Abrit or red colored Kumudawati ornament called Manikmarakat, which means restraining one's passions and as a means of facilitating people to trade, this color has a direct relationship between humans and humans when trading is mutually beneficial between buyers and traders. The letter D depicts a wungu or purple Kumudawati ornament called Manikraja Magundring which means to cool the heart and an antidote to heartache due to romance. The color purple is usually seen as mourning, but the Javanese are the other way around.\nThe letter E depicts a Jene or yellow Kumudawati ornament, which means bringing continuous good fortune in the form of food, clothing and clothing. The letter F depicts a blue Kumudawati ornament referred to as bead endrataya which means health to avoid all diseases and as strength to face all the problems of its inhabitants. The letter G depicts a yellow or black Kumudawati ornament which means instilling an optimistic attitude to everyone in life so that this feeling of optimism can encourage a person to achieve what he dreams of. The letter H depicts a green or ijem colored Kumudawati ornament which means holding back emotions and continuing to do good to others."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 7794,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "253129466"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Critical clearing time estimation of multi-machine power system transient stability using fuzzy logic\n\nStudying network stability requires determining the best critical clearing time (CCT) for the network after the fault has occurred. CCT is an essential issue for transient stability assessment (TSA) in the operation, security, and maintenance of an electrical power system. This paper proposes an algorithm to obtain CCT based on fuzzy logic (FL) under fault conditions, for a multi-machine power system. CCT was estimated using a two-step fuzzy logic algorithm: the first step is to calculate Δt, which represents the output of the FL, while maximum angle deviation (δmax) represents the input. The second step is to classify the system if it is a stable or unstable system, based on two inputs for FL, the first mechanical input power (Pm), the second average accelerations (Aav). The results of the proposed method were compared with the time domain simulation (TDS) method. The results showed the accuracy and speed of the estimation using the FL method, with an error rate not exceeding 5%, and reduced the performance time by about half the time. The proposed approach is tested on both IEEE-9 bus and IEEE-39 bus systems using simulation in MATLAB.\n\n\nINTRODUCTION\nThe interest in controlling the transient situations that the electric power systems are exposed to has become an important issue that the generation, transmission and distribution of electric power companies seek. Interest in these aspects has increased over time, and methods and techniques have been continuously developed to keep pace with the complexities and breadth of electrical power systems [1], [2]. The study of stability and its analysis is very important to know the possibility of maintaining the stability of the system when disturbances occur, such as transmission line malfunctions, sudden change of electrical loads, sudden loss of units, as well as known malfunctions, which are cases of short circuits that the electrical system may be exposed to. Which may cause, in the event of the fault being large, to lose the synchronization state of a generator with the rest of the generators in the system, which leads to a state of imbalance or stability in the system, and these disturbances may affect frequency and voltage [3], [4].\nCritical clearing time (CCT) is the maximum time during which a fault must be cleared to maintain system stability. The CCT is measured and compared with the fault clearing time (FCT) in direct stability estimation methods. The transient system is known to be stable if the CCT is greater than the FCT [5]- [7].\nThe importance of CCT estimation is due to the development and expansion of the operational range of generators. Usually, the relays are tuned to trip a signal by calculating the CCT obtained from conventional methods and for different operating conditions. However, the relay may issue a wrong decision if there is a change in these operating conditions. Accordingly, the researchers tended to use artificial intelligence for calculating CCT and in different operating conditions [8], [9].\nSeveral techniques have been used to assess transient stability, including the traditional time-domain method, numerical integration, probabilistic methods based on the Labenov technique, recently artificial intelligence techniques. The effect of adding flexible alternating current transmission systems (FACTS) to enhance the transient stability of the system and increase the critical clearing time after a major fault or sudden change in load levels has been studied by studied by Azeez and Abdelfattah [10]. Priyadi et al. [11] suggested the control of unstable equilibrium point (CUEP) method to obtain CCT, and through this method, the critical power of each generator in the system is determined with an allowable error of 0.01% and the control of the unstable equilibrium point at any fault. The researchers proved that this method is more accurate to Determine stability through numerical operations. Abdelaziz [12] used fuzzy logic technique to classify the system, whether it is stable or not, and the results revealed that the proposed system is flexible and extendable. Nair et al. [13] consider the range for which the value of the CCT changes with the change of the fault location, the increase of the load systematically and the change of the value of the fault resistance. Variation of CCT is observed using eigen value analysis method in MATLAB/PSAT platform. Sulistiawati et al. [14] used two methods to calculate CCT, the first is numerical, which is the critical path method based on critical generation, and the second method the CCT is learned by extreme learning machine (ELM) and this method has the ability to calculate CCT with changing loads and for various faults, they showed that these methods give CCT is accurate with error rate 0.33% for the neural networks (NN) method an average error of 0.06% for the (ELM) method. After studying the transient stability of the oscillation equation and the equal area criterion, the researcher Lin [15] clarified that there is a relationship between power factor and frequency with CCT, and this relationship is direct with power factor and inverse with frequency. Sharma et al. [16] derive an equation or formula linking CCT with the system parameter, where this formula gives an insight into the effect of system components on transient stability such as system impedance and generator moment of inertia. The study was conducted on a system 39 bus. Fuzzy logic (FL) used to estimate CCT in a multi-machine both IEEE-9bus and IEEE-39 bus systems. These systems are modeled in MATLAB 2017/Simulink.\n\nSIGNIFICANCE OF THE RESEARCH\nCCT main the maximum allowable time for which the system remains stable after the occurrence of the fault in the power system, evaluating the CCT is very important to maintain stability and not prone to collapse after the fault. There are several methods used to calculate CCT, such as time domain simulation (TDS), and numerical analysis of nonlinear differential equations. These methods give accurate results for a long time as a result of the many iteration processes. This is so inefficient when utilized for transient stability analysis. Because the disturbances occur very quickly in the system. Therefore, we need methods that can reduce the required computing time to calculate CCT such as artificial intelligence methods. In this research, fuzzy logic was used to reduce the computation time to calculate CCT. The results proved a high degree of accuracy and speed of evaluation.\n\nFUZZY LOGIC\nFL is a way of dealing with undefined and uncertain data for problems that have more than one solution. Logic is two types: binary logic and fuzzy logic. It was used by the scientist Lutfi Zadeh at the University of California for the first time in 1965, as it was found that FL is multi-valued logic, as it builds intermediate values between traditional values such as true/false and high/low. Fuzzy systems are an alternative to traditional ideas they can be represented by organic and logical groups that have their origins in ancient Greek philosophy the structure of the FL system is shown in Figure 1 [17], [18].\n\nFuzzification\nIt is the first part in the structure of FL in which the process of converting the regular (Crisp) value entries into fuzzy variables of different degrees of belonging to the fuzzy groups. And these are ready for processing in the fuzzy deduction machine. The fuzzy consists of a set of functions belonging to the fuzzy groups including their shapes, number, maximum values, and the number of interventions between them in determining the linguistic values of the fuzzy variables [19].\n\nRules base\nIt is a set of fuzzy laws that relate fuzzy inputs to outputs. There can be multiple entries with one output. Setting rules is the vital and most important part when designing fuzzy logic, which is a set of logical semantic rules in the form: If…..And….Then [19].\n\nInference engine\nIt represents the basis of the structure of FL, as it has the ability to represent human decision making based on fundamentals of FL, using two main methods of inference. The first is the Min-Max method. The second is fuzzy additive to be able to deduce FL actions using fuzzy implication and inference rules in FL, i.e. the process of deduction is carried out based on the values of the inputs for fuzzy sets [20].\n\nDefuzzification\nReverse fuzzy this is the last stage in the structure of FL. It is the opposite process of fuzzy, i.e. converting fuzzy functions to regular functions (Crisp). There are several de-fuzzing methods that determine the final output value as the centroid or center of gravity technique to find the equilibrium point of the solution [21].  [23] The value of CCT is estimated from the simulation by increasing the value of the fault clearance time until the system reaches an unstable state, as follows:\n\nPROPOSED METHOD FOR OBTAINING CCT 4.1. TDS method\nStep 1: Set an initial value for FCT=to.\nStep 2: Impose initial time limits by decreasing the value of to by α to get the lower bound t1=to-α and increasing the value of to by α to get the upper bound t2=to+α.\nStep 3: The system is checked if it is stable or unstable. If it is stable, the lower bound value is replaced by t1=t1+α and the upper bound value is replaced with the value t2=t2+α. Then the system stability is checked at the new values and we continue to change the value of the lower and upper bound until we get that at one of these two values the system is stable and at the other value, the system is unstable. Move to the step 4.\nStep 4: The middle value between the upper and lower bounds is tested. If the system is stable, the lower bound value is replaced with the median value. If the system is unstable, the upper bound value is replaced by the median value to perform the time calculation again.\nStep 5: This process continues until we reach the value of the acceptable tolerance between the limits, then the value of CCT is determined to be the upper limit t2. Figure 2 represent the flowchart of the proposed algorithm to estimate the CCT using FL. The value of the CCT is estimated using two-step FL: the first step is to calculate Δt, which represents the output of the FL, while maximum angle deviation (δmax) represents the input of the FL with triangular membership functions for its mathematical simplicity for representation of eight of linguistic variable (δmax) with eight of the linguistic variables for output as shown Figure 3. For the input δmax and output Δt, the proposed fuzzy system is divided as subsets as follows: vs (very small), ms (medium small), ls (large small), sl (small large), mL (medium large), vl (very large), ll (large large), ll1 (large large1), ss (small small).\n\nFuzzy logic method\nThe second step is to classify the system if it is a stable or unstable system, based on two inputs for FL, the first input is the mechanical input power (Pm), and as it is known, the higher the load of the system, the greater the chance of the system going into an unstable state. 'Pm' get from load flow results. The second input is the average acceleration (Aav) during the fault, which represents the mean value of the two rotor angular accelerations A1 at the moment of fault, and A2 at the moment of clearing the fault. The two features are specified from a transient stability study for a machine. The two previous variables, which are the inputs of the proposed FL system, to classify the system if it is stable or unstable are divided as subsets as:\n\nRESULTS AND DISCUSSION\nThe fuzzy logic system (FLS) method and TDS method are carried out to estimate the CCT using the IEEE 9-Bus system and NE 39-Bus system as shown in Figures 5 and 6, respectively and the information of these systems are appearing in [24], [25]. Simulation is carried out by applying a three-phase fault to an evaluation of the performance of the proposed method. Figure 7 shows the voltage signals with time after three phase fault occurs between bus 8 and bus 9 in the IEEE 9-Bus system at a time equal to 0.5 sec. It is obvious that the voltage values decrease after the fault with different values depending on the location of the buses from the location of the fault. However, these values return to stability at values very near the nominal values at FCT is 0.714 sec. Increasing the value of FCT to 0.715 sec causes the system to lose its stability as shown in Figure 8. The same fault was applied to the NE 39-Bus system between bus 25 and bus 26. Figures 9 and 10 show the signal voltages with time for all buses when the FCT is equal to 0.71 and 0.72 sec respectively. We notice from Figure 9 that the system maintained its stability while it lost its stability in Figure 10.  Figure 6. NE 39-Bus system is used as the test system, which is simulated in MATLAB Table 1 shows the results of the estimation of CCT values and performance time obtained from the TDS method and the proposed FLS method for IEEE 9-Bus after applying a three-phase fault in different locations and Table 2 shows the same results for NE 39-Bus system. Tables 1 and 2 shows the estimated values of CCT and the performance time for each of the FL method and the TDS method. From the tables, we note that the error for all cases is a small percentage, and this proves the accuracy of estimating values of CCT in the FL method compared with the TDS method, in addition to reducing the time of performance by FL method to half of the time of performance using the TDS method. tables pointed that the fault is near that certain bus and the Fault bus number is The line that was taken out of the system"
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2023-01-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 8248,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255248782"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "THE CONCEPT OF WRITING IN THE QUR'AN: ANALYSIS OF THE TERMS KATABA, KHAṬ, AND SAṬARA\n\nIn the Qur'an there are many discussions about the various forms and meanings of writing. In this study, three terms are raised which directly refer to the written meaning, namely the words Kataba, Khaṭ, and Saṭara. Of the three terms, it is often translated into Indonesian with the meaning of 'write'. But whether the meaning is essential or just majazi. The purpose of this study is to understand the concepts of Kataba, Khaṭ, and Saṭara in the Qur'an, both interpreted literally and majazi and to know comprehensively about the importance of writing from the perspective of the Qur'an. This type of research is qualitative research, while the data used comes from the literature, namely all data and materials used are data or library materials that are appropriate to the issues raised. In this study, there are two data sources used by researchers, namely primary data sources and secondary data sources. Because this research is related to the Qur'an, the primary source is the Qur'an. While the secondary data sources are in the form of literature that can support data equipment in this study, such as the books of tafseer, articles, journals and related websites. The results of this study are that of the three concepts of writing in the Qur'an that are raised in this study, namely Kataba, Khaṭ, and Saṭara a, indicate a distinctive meaning among other writing terms contained in the Qur'an. From the results of observations, it is known that these three concepts are mentioned directly in the Qur'an, which means that writing is very important and prioritized in today's contemporary life.\n\n\nPreface\nGod gave birth to humans as caliphs on earth. This indicates that humans implicitly have a big and heavy duty regarding the task of their caliphate. Until the revelation of the Qur'an was revealed to the Prophet Muhammad. The Qur'an is a source of guidance that can lead people to a happy and prosperous life, both in this world and in the hereafter. The Qur'an is not only a \"collection of Divine revelations\" in it which contains Allah messages that are holy and of absolute value. But not only that, the Qur'an is a collection of wisdom and studies of the truth of God's \"down to earth\" pearls that can guide mankind towards a goal in accordance with his dignity. The study and content of the Qur'an cover various aspects, starting from stories, past history of mankind, and so on. In addition, the Qur'an also discusses many sciences, such as social sciences, health sciences, natural sciences, religious sciences, and other sciences. 1 Al-Maraghi, a well-known commentator, as quoted by Romdhoni once explained that the Qur'an has brought major changes to Muslims, especially to a nation that is the lowest to become a respected nation. If only until now there was no writing, it is certain that knowledge would not be traceable and religious knowledge would not develop rapidly. 2 Likewise, Quraish Shihab, an Indonesian interpreter scholar, in his book has provided an explanation that, apart from reading, writing is the main requirement in building civilization. The broader a person's interest in writing, the higher the level of literacy in that area. Through the ability to write, humans do not need to do whatever they want from the start, but can learn from the achievements and failures of previous people. 3 So many scholars discuss the virtues of writing. Therefore, it is very important to write in everyday human life. So that the Qur'an requires humans to always seek knowledge by reading and writing, because if someone does not seek knowledge without these two things, then it will be in vain for them to interpret the diversity of knowledge. They will become narrow-minded and lack the references to the knowledge they get. Apart from that, so that this can be carried out properly, the previous human being should instill a sense of interest in himself towards writing. 4 The reason for the researcher taking this theme is because the Qur'an discusses a lot about the various forms and meanings of writing in it, so it is very necessary for the researcher to examine it more deeply. In the Qur'an there are at least three words that directly refer to the meaning of 'write', namely the words Kataba, Khaṭ, and Saṭara. However, these three terms are often translated in Indonesian as 'write'. As understood, that in the Qur'an certainly does not only rely on the meaning of 'write' alone, but there are deeper meanings of each of these words.\nOn this basis, the researcher feels interested in studying this problem in order to gain comprehensive knowledge about 'writing' in the perspective of the Qur'an, by describing and analyzing verses that are related and contain the word 'write'. For that the formulation of the problem is how the concept of Kataba, Khaṭ, and Saṭara in the Qur'an and whether the meaning is essential or just majazi. The purpose of this study is to find out the meaning of 'write' in the Qur'an, both essential and majazi in nature and to know comprehensively about the concept of 'write' in the perspective of the Qur'an.\nIn connection with this study, researchers have conducted pre-research on some of the existing literature. This is done to find out the extent to which research and studies on 'writing' in the Qur'an have been carried out by previous researchers. As far as observing and searching for literature as a reference for this theme, there is very little that directly mentions the concept of writing in the Qur'an, which much of the discussion is related to literacy, then incorporates the concept of writing into that literacy. As for among scientific works that discuss 'writing' and literacy with various different perspectives such as; a) Siti Aisyah's research which discusses the importance of Al-Qur'an literacy to maintain the spiritual survival of the Ummah, 5 b) Zamakhsyari Abdul Majid's research which discusses the existence of the Qur'an in Global literacy, 6 c) Masykur H. Mansyur's research which is related to the word of Allah, namely the word 'Iqra'' as a form of literacy in Islam, 7 d) group research conducted by M. Faizul Akbar Surbakti, Wibowo, and Suci Fitriyani Barutu on the formation of the character of a literate society through a culture of literacy in the Qur'an, 8 e ) research conducted by Imas Kurniasih on the urgency of literacy in the Qur'an maqashidi tafsir perspective, 9 and f) research that is more directed to the theme of this research as collaborated by Ali Hamidi, Muhammad Syariful Anam, and Firdan Fadlan Sidik, they raised the theme of literacy development Kataba to support education. 10 From the literature review that has been mentioned, the provisional conclusion is that the study of 'writing' with its various dimensions is of course not a new thing, meaning that there has been previous research done. However, in this case, the researcher will not focus on a particular interpretation but rather focus on raising his research on the overall meaning of the concepts of Kataba, Khaṭ, and Saṭara contained in the Qur'an. However, as far as observations and reviews are made by researchers, they have not found the same specific discussion related to this research, namely research on the concept of writing in the perspective of the Qur'an. Therefore, it is this theme that the researcher adopts as research. Because writing activities are very important for scientific and social development in society. In addition to advancing and developing technology, in the current generation, writing activities are not in demand.\nThe method used in this study, because the data used is sourced from the library (library research), the nature of this research is qualitative research. The library in question is that all data and materials used are data or library materials that are in accordance with the issues raised. 11 Library materials used as research objects are books of tafseer, journals, magazines, or other writings related to 'writing' in the Qur'an. In this study, there are two data sources used by researchers, namely primary data sources and secondary data sources. Because this research is related to the Qur'an, the primary source is the Qur'an. While the secondary data sources are in the form of literature that can support the data equipment in this study, such as books, articles, journals and websites that are still related to the topic of this research. 12 As mentioned above, all data from this research is library material so that in data collection techniques the type of documentation. By paying attention to the formulation of the problem that already exists, the relation to this data collection technique is simply that there are several steps taken, such as collecting, sorting, then the results of the election the researcher will classify based on each sub-discussion. After that, a search was carried out on the books that explained about 'writing'. The details of data collection techniques in this study are; first, the researcher will collect all the data related to this theme, especially all the books that talk about literacy, its types, and so on. Especially for books written by scholars and also general non-Islamic literature around the word 'write'. Second, researchers will also collect secondary data from journals and websites (internet) that match the theme of this research. In order to be accountable and in accordance with the wishes of researchers. 13 After obtaining the data in question based on the steps that have been described, the next stage is the data processing method or technique. The data processing techniques used; a) determine the problem to be discussed, b) trace and compile the discussed problem by compiling verses of the Qur'an that explain it, c) study verse by verse which explains about 'write', d) understand the correlation of verses mentioned in their respective suras, and e) complete the explanation of the verse with reference sources that are relevant to the topic of this discussion. 14\n\nReview of Writing from Various Perspectives\nIn the current era of globalization, writing is one of the most important things for a person's survival to learn and develop into a good and educated human being. Without writing, it will be difficult for humans to know something that they do not know. So, they will be inclined to things that are of no benefit to them. Therefore, it is important to improve one's writing skills so as not to be left behind by people who are experts in their field. Writing skills will also lead someone to a lot of knowledge and experience. The more knowledge we find, the more extensive our horizons will become. So that it is not easily affected by bad things out there. 15 Writing is interpreted as an act of conveying advice by requiring written language such as a tool or device. Writing is the result of the product of the human mind by producing what is in his heart for humans and himself through writing instruments. Writing means symbols and illustrations of letters and meanings that users can observe and agree on, in this case writers and readers. With this there are four aspects in it, namely the person who writes as the messenger, the substance of the review, the tool for conveying the writing and the person reading is part of the recipient of the message. Tarigan argues that writing is defined as inscribing or illustrating graphic symbols which imply a meaning in language so that they can be understood by the reading public, in that way the reading public can understand the graphic. Writing is part of the form of representation of the unification of language expressions. 16 According to the Muslim sociologist, Ibn Khaldun, writing is one of the easiest ways for us to access information regarding both intellectual traditions and the history of earlier nations. By knowing the history of their nation, people know what must be developed for their country to become a more advanced and superior country. Thus, writing and writing are the pillars of science and knowledge, a means of guaranteeing the preservation of the stories of the ancients and their thoughts, becoming a means of transforming knowledge between nations and generations, so that the development and growth of knowledge can be known by many nations and their generations. That is why Islam really requires its people to be skilled and active in reading and writing. Moreover, when Islam was just revealed in Arabic, they were an ummi nation, that is, they lacked skills in reading and writing. Writing is a model of practicing or working for knowledge. So, there is a saying that a friend says: Whoever works for knowledge or practices his knowledge, Allah will give him knowledge of what he does not know. 17 Someone who writes has a goal in expressing his ideas or ideas through written language, the goal is for the author or dear readers. In general, writing has a purpose, namely as an effort to describe feelings, as a medium of notification, an effort to influence and provide entertainment to readers. The function of writing is to write or describe an incident or event, condition, situation, or other certain circumstances as an effort and hope so that there is no negligence or negligence, so that it is formed into a work of writing, because writing has specific things. 18 In other references, writing is a productive and expressive activity. As a writer, you have to be creative in using language and vocabulary. To have writing creativity, you must always practice and be followed by discipline and regular practice. Another definition according to Rusyana's opinion, writing is an attempt to develop language patterns in a piece of writing with the aim of describing an idea. Alwasilah's opinion is to interpret writing as an effort to increase productivity in language through psycholinguistics, which starts with ideas through semantic procedures, then performs syntactical data collection, and the last is described through written form. 19 Someone who is said to be a writer must have aspects of accuracy in using elements of language, preparing discussion in the framework of writing, suitability in using language, and processing words or sentences when writing. From the opinion of Saleh Abas explained that writing is part of processing thoughts that are interrelated, from the trial process to being able to review it again. So that writing has the meaning of activities to express ideas, feelings, thoughts and ideas on aspects of language. Being a legitimacy, human beings are actually gifted with talent by Allah. It's just that what needs to be done is to find and develop these talents. However, someone who has talent alone is not enough to fulfill writing. With talent, it is easier for someone to absorb writing theories. So, from here lies the importance of motivation to write. 20 There are things that can foster motivation to write, namely the urge to become a writer is part of God's commandment which becomes a value of worship, if this becomes a value of worship then the process of continuity of writing will continue until writing is a form of struggle. Realizing this, it will provide additional energy in writing in the hope that you will remain accustomed to writing so that writing becomes a habit in life. To hone writing skills can be started from reading. By reading, you will gain knowledge, get ideas and ideas, and have a high sense of confidence in yourself as a writer. Discuss with groups who have a high interest in reading, as an effort to gain insight into writing broadly, or from experiencing a life event so that it can be documented in written form. 21 Currently, there is an intellectual work born from the hands of leading Muslim scholars and scientists that can be found today. Various scientific disciplines covering the interpretation of the Qur'an, Hadith, Fiqih, Astronomy, Philosophy, Mathematics, Science, etc. With the various works of these scientists, they are able to explore and reflect on what prompted them to write a work which was then formed into a book. With books people are more flexible in carrying out logical-critical analysis of every idea put forward to the public. So that people can develop their thinking better.\n\nTerm of Writing in the Qur'an\nVerses regarding the term 'write' are found very frequently in the Qur'an. However, in this study only took three different verses in the mention of the word 'write'. This means that each verse mentions the word 'write' but in a different sentence form. The three verses are as follows: a. The term 'write' using the term 'kataba' contained in surah al-Furqan/25: 5 'And they said, \"(It's just) the tales of the ancients, who were asked to write down, then read those tales to him every morning and evening.\" 22 The name Surah al-Furqan has been known since the time of the Prophet. The naming of this sura with al-Furqan which means differentiator, is taken from the word al-furqan which is in the first verse. Surah al-Furqan is the 25th sura in the Qur'an which has a total of 77 verses and is a Makiyyah sura. While some scholars exclude three verses, namely 68-69 and 70. They consider it descended in Medina. However, this exception is rejected by the majority of scholars. According to Tabataba'i, this allegation arose because there was a description of the prohibition of adultery. But still according to Thabatab'i, this is not a valid reason because the prohibition of liquor and adultery has been carried out since the beginning of the presence of Islam. The opinion of the majority of scholars states that the prohibition of adultery is also carried out by the Qur'an in stages. 23 Overall, Sayyid Quthub considered that this sura al-Furqan looked as if it was sent down as entertainment for the Prophet, who could relieve the pain and fatigue of the Prophet with a loving touch. It also calmed his heart, poured faith and confidence and breathed into him breaths of care, affection and love. Because on the other hand this surah describes a harsh war with humans who are misguided, dissident and against Allah and Rasulullah. They argued loudly, refused with hatred, resisted with great rudeness, and disobeyed clear and blatant instructions. 24 Textually, this verse contains accusations against the Qur'an which was revealed by Allah to the Prophet Muhammad as the Naziir for mankind. However, they accuse and accuse that the contents of the Qur'an are just lies that were invented by the Prophet Muhammad and he was assisted by other people. For all their accusations and accusations, then Allah denied that they had actually committed a great injustice and lie. The accusers were the Makkah polytheists led by al-Nadhr ibn Harith and Jewish figures named 'Adas, Yasar and Jabr. 25 The accusations and propaganda levelled by the polytheists and Jewish leaders were, first, that the Qur'an was a fake book (man-made) because it was the written work of the Prophet Muhammad, second, that the Qur'an was a fairy tale told written and composed by someone to the Prophet. This is denied by Allah, that the accusation is a heinous and unjust accusation. This false accusation has absolutely no basis. Allah's denial of this accusation is very clear, because the Prophet Muhammad could neither read nor write nor had he ever read any book before nor had he copied from other sources. As explained in the following discussion. 26 b. The term 'write' using the term 'Khaṭ' contained in surah al-Ankabut/29: 48 Surah al-Ankabut which means spider is the 29th surah in the composition of the Qur'an and was revealed in Mecca. Surah al-Ankabut consists of 69 verses. The majority of scholars are of the opinion that all of his verses were revealed before the Prophet Muhammad. emigrated to Medina. There are also narrations which state that the entire verse was actually revealed after the migration. The third opinion states, some are Makkiyah and some are Madaniyah. Adherents of this opinion, among other things, stated that the first verse to the third verse was revealed after the Prophet migrated. There is another opinion stating that the beginning of this surah up to the eleventh verse are the verses that came down after the migration. Scholars state that this sura was revealed before the migration and admit that it is the last Makkiyah sura. 28 The name al-Ankabut is taken from the word contained in the 41st verse. The main theme of this surah is an explanation of the nature of faith, that faith is not just a saying with the tongue, but its essence is reflected in persistence in the face of waves of torture and persecution and temptation. This is because humans will not be allowed to say, 'We have believed', without being tested to find out the nature of the faith that is growing in their hearts. Almost all the verses of this sura revolve around this theme. 29 The beginning of the surah explicitly talks about the test of life and faith, while alluding to the attitudes of believers and hypocrites. This was followed by the stories of Nuh, Ibrahim 'Ad, Tsamud, Qarun, Fir'aun, and Haman are explained at a glance, but all of them also describe the various obstacles, trials, and persecution that lie on the path of da'wah to faith, throughout human generations, which are equipped with a description of the creed and falsehood of idol worship. Thus it can be concluded that the main goal is for Muslims to be steadfast in facing various obstacles and prove their words of faith by struggle and practice. 30 In verse 48, there is a message that Allah wanted to convey to mankind from that time until now, that Kafirs should not accuse the Prophet Muhammad of falsifying the Qur'an, through the help of other people's writings and essays. Even though the Prophet Muhammad never wrote or copied himself or with the help of others for any book. Such accusations would undermine the credibility of the Prophet Muhammad and the originality of the Qur'an. 31 c. The term 'write' using the term 'Saṭara' contained in surah al-Qalam/68: 1\n\nBy the pen and what they write.\" 32\nSurah al-Qalam is the 68th sura in the Qur'an and is a Makiyyah sura. Textually, this verse contains Allah's denial of the accusations of the Kafirs, namely their accusations against the person of the Prophet Muhammad. This verse contains and begins with the word qasam or oath. Theoretically, the redaction of a verse that begins with an oath serves as an affirmation and reinforcement of what will be explained after the oath, while the objects used as an oath are considered as something extraordinary. According to some commentators, this verse was revealed on the treatment or accusation of the Quraysh Kafirs against the Prophet Muhammad. 33 In this surah al-Qalam, the important thing and what Allah wants to confirm and strengthen is about the greatness of the personality qualities of the Prophet Muhammad, as a refutation of the accusations and harassment of the Kafirs against the Prophet Muhammad. As is known, Allah disproved the Kafirs by starting the surah by swearing through a qalam (pen) and things that humans can write with a pen. It can be understood that from this verse Allah wants to show the greatness of the pen and the things that result from writing with a pen. 34 Al-Qalam, according to the majority of mufassir, is a type of pen used as a writing instrument used by Allah, both the pen used to write His verses in the heavens and on earth. Writing humans produce written works, because from writing, expressions are realized. Allah swears by a pen and what is written with a pen by humans is in the form of knowledge and knowledge, as a form of mercy from Allah swt. Knowledge and knowledge from the qalam and the writings will prove that the Prophet Muhammad was not a madman as alleged by the Quraysh infidels, but a noble man with commendable morals. In addition, Allah's oath with a pen and what is produced by the pen also indicates the greatness of the favors of the pen and its writing products, as a complement to the blessings of speaking and language skills. 35 In Islam, Allah, through His words, strongly recommends every Muslim to maximize the function of the pen and writing activities, in order to produce many blessings for humans themselves. Pens and their products can be used to defend Allah, the Prophet and Islam. Pens and their products can be used to spread knowledge and knowledge to all nations, social groups and individuals. Therefore, pens and writing products from them can be used as a benchmark for the progress of a nation and its civilization. 36\n\nConcept of Writing in Qur'an as the Concept of the Importance of Writing in Life\nEvery religion always calls on its adherents to do good based on the norms that apply in the teachings of that religion. This is so that there is a balance between rights and obligations, both personally, publicly, humans and nature, fellow humans, or humans and God. So that all religious norms are packaged in a conception called worship. In Islam, the Qur'an has signaled the importance of writing, because indeed at the time the Qur'an was revealed, the Arab community that lived at that time was a society that did not know how to read and write. The Qur'an explains that knowledge is a privilege that makes humans superior to other creatures. It aims to carry out human duties as representatives on earth. So that humans will become useful people in the surrounding environment. 37 34 Shihab, Tafsir Al-Misbah. 35  Al-Ghazali once said that seeking knowledge is a virtue for humans. Because by studying knowledge, humans will reach Allah and become close to Him, so that humans will gain eternal happiness and eternal pleasure in their lives. Therefore, it is very important for humans to enrich themselves with all kinds of knowledge through writing. So that they can be critical of something new to them, also smarter in choosing which way is right and which way is wrong. Of course, this will not be realized if we as humans do not take advantage of existing knowledge by developing our writing skills. 38 This is in accordance with what Allah conveys in the Qur'an as follows: To whom belongs what is in the heavens and on the earth?\" Say, \"Belongs to Allah.\" He has fixed (the nature of) compassion on Himself. He will truly gather you on the Day of Judgment without a doubt. Those who harm themselves, they do not believe.\" 39 This verse uses the term kataba which means to determine. It means that in this verse Allah has established a characteristic in Allah, namely the nature of compassion. In this verse, Allah also explains that only Allah is the owner of the heavens and the earth, and all that is between them. Thus, this verse explains a hint about the various forms of God's grace given to humans. So that humans do not only write and read, but also should study every form of God's majesty. Do not be afraid to feel wrong in terms of learning, because people who are afraid of learning to try a science for themselves, will bring losses. For that, continue to hone yourself as a human being who is thirsty for knowledge. 40 If traced from the source of his teachings, then implicitly or explicitly, writing activities are very important in life, the reasons are: 41 First, writing is part of the culture of the Qur'an, the mention of the word 'write' is often repeated by Allah for its mention. Such things in the form of mentioning and repeating are not mere figures of speech but a concern and appreciation in the Qur'an for the importance of writing, therefore write more because the Qur'an n has given great appreciation. Second, the privilege of writing is found in the Qur'an, Allah expresses an oath in the Qur'an indicating that Allah is special to something that is sworn in, as in the example of Allah's oath to Himself, the heavens, the earth, the stars, mountains and so on. In other words, Allah makes muqsam bih an important and special thing. Allah also swears by the pen as an order for humans to write too. So that the connection between the two letters shows that reading and other sciences must all go through writing, all of that because if there is no writing, there will be nothing to read and if there is nothing to read then there will be no knowledge that can be practiced.\nThird, Allah and His Messenger recommended writing, based on a history sourced from Abdullah Ibn Amr Ibn Ash that the Prophet allowed Abdullah to write what was known from the Prophet. In addition, the Prophet also gave freedom to prisoners of war with the guarantee of teaching them to read and write to their companions. So, it is not uncommon for him to also ask friends who are good at reading and writing to write down the revelations that come down. This gives an indication that the Messenger of Allah also attaches great importance to writing activities. Fourth, writing can be a medium of knowledge and communication. Mustafa al-Maraghi's interpretation states that the function of writing is nothing but a means of communication. Besides that, writing can also help in preaching, as was done by Prophet Sulaiman who invited Queen Bilqis to worship Allah, through letters. After that, this tradition was continued by the Prophet Muhammad who invited the dignitaries of the people and kingdoms that existed at that time to embrace Islam. This da'wah was carried out by the Prophet Muhammad through letters. Apart from that, in his commentary Mustafa al-Maraghi also added that this verse has substance that can change a nation from a lowly, non-civilized nation to a civilized, noble nation. So, you can imagine how things would be now if there was no written order.\nFifth, writing as a binder of knowledge. In the development of science, writing is a very significant thing, because by writing, there will be writing that can become a legacy for later generations, so that knowledge that has been written before can be a lesson for generations. what follows can be easily studied and developed. As quoted by Buya Hamka, the opinion of Imam Syafi'i, \"Science is game animals and writing is the rope that binds the game, therefore tie the game with a strong rope\". The phrase that was issued is a sign that writing is a very significant thing. If science is likened to hunting animals, then there must be a strong rope that binds them, because as it is known that hunted animals are wild animals, so if they are not tied up, they will escape. This is the same as knowledge, if it is not bound by writing, then the knowledge will quickly disappear. The description here does not use reason as the bond of knowledge because human memory will reach a weak moment when it is old age, so Imam Syafi'i equates rope as a strong binder for wild game, and writing as a binder for extensive and extensive knowledge.\nTherefore, writing is one of the most important skills in everyday life. Without writing, humans cannot recognize knowledge. Also, without writing, humans will not be able to think well for the benefit of their lives. One attitude that can be applied so that humans are always beneficial in their lives is to change their mindset to always want to continue learning and learn anything they don't know, so that humans will be free from their laziness. Besides that, learning must be by studying with people who are experts and pious, don't just rely on selftaught or analyze yourself without in-depth knowledge. Because this is so that the knowledge we get can be known to other people where the source of this knowledge comes from, so that it becomes clear that the knowledge one gets is well preserved. Too obsessed with wanting to be smart instantly, will actually make someone look stupid, but if someone wants to learn gradually from scratch, then knowledge and blessings will be obtained. 42 Regarding the writing verses contained in the Qur'an, an Indonesian interpreter scholar, Quraish Shihab, argues that writing verses give a signal to humans about the importance of learning to write, because in this life everyone will have muamalah, which means they will carry out transactions that must be recorded, one concrete example is regarding loans or accounts payable. From this it is clear that the ability to write is very necessary in everyday life, not only when someone owes or borrows, but the meaning is quite broad if someone can apply it properly and perfectly, especially in the field of writing. 43 According to Buya Hamka, as quoted by Tarigan, argued that to be a good writer, they should write it as completely as possible., not taking sides with anyone, knowing what to write, and writing must be in accordance with what is conveyed by other people as a source of information. Thus, to be a good writer, a writer needs to master the ability to write according to predetermined rules. Because being a writer is not easy. Need good accuracy and know the idea that will be written later. So, if the writing is good and correct, then other people who read it will easily understand what is written. In order to write a scientific article or essay, the author should first look at and study sources whose validity is recognized by certain institutions. So that the writing becomes the real truth and can be trusted by others. Because a piece of writing whose words have been arranged on paper, is a reading that will be enjoyed by readers. Therefore, to achieve good and correct writing, writers should write with a great sense of responsibility and intend to practice science for Allah's sake. 44 When viewed from the side of a writer, the writing verses contained in the Qur'an can be a reference for writers to always provide the best and original work made by themselves. The fruit of a work must have a valid basis and its truth can be proven, not fabricated or fabricated. So, it requires in-depth research from the authors about the references or sources they want to cite. Just a little mistake in taking references can be fatal, people who have read or researched his work will feel distrustful of his writings and they will no longer want to enjoy the author's work. For this reason, writers are required to always work with a sense of responsibility and honesty, both before the readers and before Allah. 45 In addition, a writer should not crave a wage. The value of a small or large wage he gets will have no meaning at all if his work does not benefit many people. Therefore, this is also in line with the opinion of Buya Hamka that our nation is left behind from other nations, one of which is because of the rise of stupid people who like to falsify information in their writings, then other people just believe in these people.\n\nIndonesian People's Writing Awareness and Its Application in the Contemporary Era\nAs in the previous discussion, literacy is one of the keys to building world civilization. Without literacy, humans will not know and know all forms created by God, starting from those that lie on earth and those in the sky. 46 In addition, humans will also not be able to know an event that occurred in a country, if they do not have the enthusiasm to learn about it. 47 In Indonesia itself, there are still very few people who understand writing and few people know the importance of it. As is well known, the percentage of people who have the ability to write in their daily lives is still relatively small, besides that, the Indonesian state is lagging far behind other neighboring countries. Among the causes are due to facilities that are not supported by the government, technological developments that are increasingly rapid, as well as the enthusiasm of the people who are still very lacking or one might say they don't really care. For this reason, it is necessary to have cooperation between all levels of society from various related institutions or agencies, to develop and revive young people who are highly literate. So that the progress of Islamic civilization can be achieved properly. 48 So, to achieve this goal, a Muslim should always apply the values contained in the Qur'an correctly. What's more, the Qur'an itself also discusses a 45  lot about writing whose values can be put into practice in everyday life. It should be noted that the concept of writing offered by the Qur'an to be applied in today's life is very diverse. The first thing he said was about recommendations for a writer to first know the rules of good and right writing. Then the Qur'an gives advice that writing is not just words that are strung together on paper, but more than that, before a piece of writing must be prepared in advance an interesting theme or idea to be studied more deeply, so that later it will become a masterpiece. good in front of society.\nApart from that, the Qur'an not only requires writers, but also other people to write an agreement in terms of an agreement between the two parties, whatever the agreement that has been agreed upon must be written. The aim is so that neither party feels the most disadvantaged and also cheated. Furthermore, after knowing what theme to raise, the author then develops the theme by looking for reference sources from the main references and some of the original books. For this reason, in order for the writing to be good, a writer should look for valid reference sources, not fake and must be complete. It is intended that a written work can be trusted by the truth of the readers. 49 In addition, the Qur'an also provides a solution for writers, if their work has been disseminated to the public, then the authors before that are required to be ready to be responsible for the results of their own work. This is intended so that when later in their work there is a fatal error, which makes certain parties feel disadvantaged, the authors can later be held responsible for their work in whatever form they can do. Still the same as the process of writing, the Qur'an also requires writers to always prioritize moral values, one of which is not to work with expectations of the value of the royalties earned. Moreover, the royalties that the author gets come from a work that contains all lies. Therefore, big or small the value of a price obtained must be well received and sincere by the writers. 50 Thus, so that writers can master the steps of the solution above, a writer needs to rearrange his intentions in working to provide the best for society, and there needs to be a review of his learning how to become a true writer who works selflessly. Therefore, in the middle of the learning process it should be accompanied by exploring the values contained in the Al-Qur'an, because the Al-Qur'an is the word of Allah whose splendor is no doubt, because it has various solutions that can overcome various problems. problems in this country, especially in particular the problem of Indonesian literacy in the current era of the millennial generation.\n\nConclusion\nBased on the results of the discussion that has been described, it can be concluded that in the Qur'an there are three consonant words about writing, namely Kataba, Khaṭ, and Saṭara. From a study of the verses of the Qur'an about writing through the terms Kataba, Khaṭ, and Saṭara, it can be concluded that writing skills and activities are an important and vital way of developing knowledge. Besides that, the three consonant words have different aims and objectives, but have the same meaning between one another, so it is necessary to understand the rules for placing words in the verses of the Qur'an. In addition, these three words are interpreted in writing, but in essence they still have differences.\nEfforts to build a writing culture based on the Qur'an have actually existed in Islam since the first revelation was revealed to the Prophet Muhammad. With the revelation of verses that contain the meaning of writing as a starting point to lead humanity from ignorance to a literate society. Efforts to contextualize these revelations are necessary for the breadth of meaning and scope of writing activities, especially in this day and age. Writing is a pillar of science and knowledge. Writing is a transformation of knowledge between nations and generations. So, the religion of Islam demands that people diligently read and write. Because the command to read and write is contained in the Qur'an. With the activities of reading, writing and recording the knowledge obtained with a tool or pen, it will leave an impression on the human mind and heart."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-28"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 10796,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254951356"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "ILLUSOR BEING OF IDEOLOGY\n\nIn this study, the transition of philosophy into its own otherness, that is, the ideology proceeding from the non-identity of thinking and being, is considered. Just as Hegel’s logical philosophy reached the form of universality, through a direct denial of the diversity of its historical forms, so post-Hegelian thought was defined through its attitude to ideology as philosophy «overturned» into politics, again appeared in historical forms. On the example of the paradoxical legacy of famous thinkers of the twentieth century, such as K. Mannheim, M. Heidegger and others, the authors establish the extreme point of the fall of philosophy into ideology and the beginning of its release from the functions of political technology in order to return to lost universality.\n\nleaving as reality of thought, but in the reality of National Socialism, formulates two diametrically opposite statements. 1. National Socialism cannot become a new philosophical (!) Principle; and 2. On the contrary, it can become one. 3. Puts forward a condition in which the key is «cognizes, understands».\nSo, we have two drives of two defining Western thinking forms of being: from politics to philosophy, and from philosophy to politics. The point of their contact is the phenomenon of ideology. How could this happen? What is the cause of this phenomenon?\nIt should be taken into account that initially K. Marx and F. Engels, using the term «ideology», transferred his sharply negative meaning (distorted consciousness) not only to individual systems of philosophy. «They not only opposed a certain philosophical system, but, in the end, overcome and destroy philosophy in general through their scientific socialism,» emphasizes Karl Korsh. Here, contrary to the founders, he insists on the deep connection between Marxism and philosophy: «It is impossible to abolish philosophy without realizing it» [6]. And under this thesis the Marxist signed? There is some kind of game of meanings. As part of the same teaching, the thesis «shines» (scheinen) in the antithesis and vice versa.\nHere is a phrase from Hegel's logic that struck the ideologists of the beginning of the last century so much that it forced him to fully establish himself in the final revision of the alternative «philosophy -ideology». «Objectivity of Appearance» (Die Objektivität des Scheins). What is the meaning of this statement of the German philosopher? «Isn't it the same thought that is objective and illusory, because it contains one of the sides of the o b j e c t i v e world? Not only Wesen (essence), but also Schein (appearance) are objective».\nBut if someone demands something from someone, then they are clearly in a relationship, in a dialogue, in reflection. This attitude is determined by the very «appearance», behind which some ideologists are forced to recognize an objective, ontological status. But even eight years ago, at the time of the creation of Materialism and Empirio-Criticism (1908), they hardly agreed with such a statement. It turns out that none other than Hegel himself in his preface (and, according to many Hegelian experts, was written after the main body of the work had already been written), he laid the «bomb» of reflection into the finished building, a precisely adjusted ladder of the logic of transitions hermeneutic horizon, which so worried F. Schleimacher at that time.\nOn October 13, 1806, in the vicinity of Yen, two people commit acts that will go down in history and remain for a long time. They are practically peers. The name of one on the lips of half -world, behind the 100 thousandth army and tomorrow a brilliant victory over the Germans. His name is Napoleon. The name of the other is practically not known to anyone. He is German. Iena was taken by the French, and he rushes around the city in search of a refuge. In his pockets he has a manuscript of phenomenology. His name is Hegel.\nWhere does world history live, or, how did the German philosopher love to express himself, the World spirit? Not in the hat of Napoleon, his horse, and not even on the tip of the bayonets of his soldiers. And even more so, not in his body. We say: worldwide history lives in the actions of thousands of thousands of people acting at this time. What remains after a worldwide event? -Memories. Where does the spirit of the «phenomenology of the spirit» of Hegel live? Not in the letters, sheets and bindings of this book. He is in the meanings that are equal to actions that are born as a result of reading these letters, words, pages, etc. What remains after reading a philosophical text that affects ideologists, and those, in turn, on politicians, etc.? -Interpretation.\nBoth in reminiscences and in interpretations, the truth of world history, as it were, acquires a second birth. Glory -this is the reward that the participants in the actions of real participants in political deeds receive. The initial silence, nonrecognition and subsequent surprise at the «secret» of the text is the assessment of the participants in the philosophical cognition of reality. On October 13, 1806, the most practical practice -politics and the most theoretical theory -philosophy, apparently had no chance of an organic combination. But exactly one hundred years later, in the form of a strange «centaur» of ideology, they became friends «do not spill water». How could this happen?\nPaper main body. Hegel's system was a response to a very serious claim to philosophy, which was constantly expressed in its address almost from the very moment of its emergence. It sounded something like this: «Philosophy first deal with the contradiction of the diversity of its own forms, when every thinker declared himself a representative of absolute truth, and only then count on some kind of recognition.» Therefore, the appearance and disappearance of the great historical philosophical systems from Parmenides to Hegel was an «omen» in the ontological plan of world history, and Hegelian philosophy became a world-historical «event». In this sense, Hegel did not create any new philosophical idea. He voluntarily abandoned his application for the crown of genius, concentrating on solving this problem in Jena (1807). In exactly the same spirit, M. Heidegger defines his destiny in a letter to K. Jaspers (1932). «Since then, I have existed in the role of a gallery keeper, who, in particular, ensures that the curtains on the windows are properly parted or drawn, so that the few great works of the past are more or less well lit for random visitors. Without a picture, I teach and deal only with the history of philosophy (our italics), that is, I try, without regard to lecture time, to state what I consider important for the revival of philosophizing» [7].\nIn non-notifying a new sake of historical recognition, something in common between two thinkers is clearly traced. But in the management of the «curtains» through which the light that illuminates the great systems of predecessors penetrates, there is a categorical difference between them. Hegel is not episodically (like Heidegger) addresses one or another philosopher of the past, but methodically subordinates their «lighting» by the logic of the transition. Thanks to this, the initial goal -giving unity by diversity, was achieved. The historical era began to listen to the voice of philosophy. But the solution to one problem led to the emergence of a new one. The form proposed by Hegel, a little later, raised a lot of questions, especially as shown above, he himself participated in their occurrence.\nAs Damocles, a sword over Hegel's teaching, the problem of interpretation of the transition in the «System» of the absolute idea into nature hung. The transition of inorganic matter is lively, and, then-in a reasonable, formal reason was more or less able to put in the school formula «origin». The picture in his imagination was drawn simply: there was no living cage on Earth, billions of years passed -it appeared; There was no conscious activity on the planet, billions of years passedit arose, etc. Voltaire began to laugh at this reason with his understanding of the «transitions» back in the middle of the 18th century. «At first, our imagination enjoys an inconspicuous transition from gross matter to organized matter, from plants to zoophytes, from these zoophytes to animals, from them to man, from man to spirits, from these perfumes, clothed in a small air body, to intangible substances; and … to God himself …» [8]. But as the ideal passes (gives rise to) this reason into the material reason, remembering a visual example of the difference in conceivable and real hundred thalers (Kant), could not understand.\nF. Engels frankly admits that «in Hegel, the creation of the world often takes on an even more intricate and absurd form than in Christianity.» [9]. He is echoed by A. D. Vlasov: «Hegel slid, fell and, getting up, moved in a different direction. Immediately after the completion of this first \"system\" of philosophy, the process of its redevelopment and restructuring began, in our opinion, poorly thought out and justified» [10,[16][17]. Numerous supporters of this point of view can be understood. After all, indeed, if you follow the development of the categories in the «Logic of Being» in the form of a transition, the object is given to itself in immediacy. But for us it also exists, only its givenness is manifested in the appearance of reflexive categories. In addition, development itself is also given only for us. It turns out for an outside observer the subjectivity of this point of view is obvious. Hegel himself explicitly expresses this in paragraph 161 of the Minor Logic. But who said that the «Logic of transition» is a universal key to understanding what is happening. By developing the logic of reflection and the logic of development, does the German philosopher himself provoke a different view of the world?\nIt was here that Willy Moog suspected Hegel of a methodological «trick». «The non-logical idea as such develops further, since it represents the absolute as a whole, philosophically comprehended in a logical form. But this purely logical way of looking at the absolute is not the only possible way of looking at it, rather, in accordance with its own dialectic, it needs to be supplemented by a way of looking at the idea in its otherness» [11]. We will return to the end of this phrase «an idea in its otherness», but now it should be remembered that Hegel was initially well aware of both types of interpretation of reality: in the form of a transition (ladder), and in the form of reflection (conductor). Therefore, in Phenomenology, he forces all three forms: consciousness, self-consciousness and reason, either to behave directly (on the first steps), then reflexively (on the second step). At the same time, he fulfilled the disclosure of the certainty of the whole of Phenomenology and the Encyclopedia of Philosophical Sciences in the horizon of the advantage of the logic of transition. We have already pointed out the reason above: it is logically the first, without it, both objectively and subjectively, the logic of reflection is impossible.\nIn addition, his logical philosophy itself acted as the first, and, therefore, direct denial of the diversity of historical forms. But the main feature of any historical philosophy was that each of them, creating a new idea, carried it through all the moments of being. There was no question of the independent existence of the sciences of nature -natural sciences, and of the spirit -the humanities. Hegel, therefore, conducts research in his «Lectures» on the philosophy of his predecessors, dividing them sequentially into the corresponding sections: «Theory of knowledge plus logic», «Philosophy of nature» and «Philosophy of spirit». Naturally, here they themselves did not yet have the opportunity to «grab» (Hegel himself often emphasizes the similarity of the words «greifen -to seize» and «der Begriffconcept»), the elements of their own systems in their unity. In this sense, for example, E. Linkov characterizes the historical development of science as «an unconscious prerequisite for philosophical thinking» [12,9]. Here he uses the term «unconscious» exclusively in the context of the absence of the indicated moment «for oneself» of the historical forms of philosophy, which can only be overcome in the logic of the history of philosophy.\nFor Hegel, the main task was to work from beginning to end the form of transition logic. The total dominance of this category affects the entire array of its system. Only in «Great Logic» (1812) does he use the category of «transition» in the first volume -253, in the second -121 and in the third -139 times. This is several times more than the «reflection» and «development» categories with it. The same thing is repeated in his «philosophy of law» (1821).\nThe Hegelian version of the «transitions» in the Hegelian system turned out to be so convincing that a hundred years later there was a temptation to designate this whole era, thinking in the horizons of this category «as» modern \", contrasting it with another era -«postmodern». A whole direction is formed in the second half of the twentieth century. There is even a special name -«nomadology». The corresponding images were proposed: «Orchids and bees», «Tree and rhizomes (rhizomes)» (Deleuze and Guattari). And this is all in order to express and understand the «shifts» that occurred in historical thinking at the turn of the nineteenth and xx centuries. Revolutionary transformations in being are instantly reflected in the minds of this being, and, conversely, otherwise the thinking consciousness qualitatively transforms the world around him.\nHegel speaks clearly and definitely about the difference between the three logics: being, essence and concept, and therefore about the difference between the three ways of dialectical consideration, and therefore about the difference between the three ways of behavior of everything that exists. Here he emphasizes: in the first we are dealing with a «transition into another» (Übergehen in Anderes); in the second, with «appearance in another» (Scheinen in Anderes); and only in the third one can one speak of the «development» (Entwicklung) of the concept [13,343]. In these three horizons, opposites 'posit' themselves ('meet' or 'behave') in quite different ways.\nThe «transition» relationship is the most popular because of its simplicity. Here the dialectical movement is such that «first» is one, and «later» is its opposite. Day follows night. God's creations follow one after the other. From the category of quality arises the category of quantity, and so on.\n«Visibility in the other» is not like that. By the swaying of the trees we judge windy weather; on the fall of the body -about the gravitational force; on inflationa crisis in industry, on rampant crime -on corruption in law enforcement agencies, etc. Here the one, by subjecting itself to negation, posits its other, and vice versa, denying the independence of this other, posits itself. Here it is not the direct that dominates, but the mediated, that is, reflection. And if in the horizons of temporality in the sphere of being, succession is decisive, then here, in the sphere of essence, it is simultaneity. Here again we encounter the same simultaneity (Gleichzeitigkeit) that Hegel speaks of in connection with the relationship between historical philosophy and the epoch, and Gadamer speaks of images and reality.\nHegel even names the structural units of the two corresponding Logic: «Being» and «Essence», differently. In the first logic these are categories, in the second they are reflexive definitions. For example, when one follows the dialectic of quality and quantity, one can be considered without the other until it, having completely exhausted itself, passes into its own opposite. This is the dialectic of transition. But the dialectic of essence and phenomenon is completely different. As in the structure of ancient scales, an equal-armed lever (yoke) simultaneously connects any movement of one edge with another, here one reflexive definition (essence, identity, thing, etc.) develops only through its other (phenomenon, difference, property, etc.).\nV. Moog «requires additions to the way of considering the idea in its otherness». Until now, we have also considered this problem in the methodological plane. But what if we look at it as an ontological problem? I. Kant in his Foundations of the Metaphysics of Morals speaks of the «honesty» (Lauterkeit) of philosophy and precisely in the plane of its being, which is far ahead of the Hegelian formulation of the question of the relationship between philosophy and the corresponding historical epoch. «Unrecognized, philosophy cannot cling to anything in heaven, nor be supported by anything on earth. And here she must prove her honesty by observing her own laws, and not acting as the herald of those laws that her inspired feeling or, perhaps, tutelary nature whispers to her» [11].\nIt is noteworthy that this idea is reproduced by M. Heidegger in his treatise «On the Essence of Truth» (1930). Just at that time, K. Mannheim proposes the principle of «relationalism» in the analysis of total ideologies, and H. Plesner insists on the need for philosophy to «enter the dangerous sphere of political life». Yes, and Heidegger himself spoke unequivocally about «participation in historical action» as the duty of a philosopher» [13,308]. In his study, he cites the words of Kant at the very end, emphasizing the importance of freedom for the knowledge of truth. But there are two important points in them that are directly related to the revolutionary transformations of the twentieth century and even to the personal fate of M. Heidegger [14].\nI. Kant speaks about the philosophy of «Unerachten». Z. N. Zaitseva translates this word through «unrecognized», but you can -«not held in high esteem». Hegel in Phenomenology speaks of the «recognition movement» (die Bewegung des Anerkennens) [15]. His dialectic will help clarify what kind of non-recognition of philosophy Kant is talking about. First, for recognition to take place at all, not one, but two self-consciousnesses are needed; secondly, each of them must see itself in the other. This is the moment of identity. But what is even more important, thirdly, that the self-consciousness in which one self-consciousness sees itself must be different, different, moreover, completely different -opposite.\nThe «Foundations of the Metaphysics of Morality» was published in 1785. Why, according to Kant, is philosophy still unrecognized, not held in high esteem? Let us fix an additional condition for recognition, which is necessary for such a form of knowledge as philosophy, which, in addition, still leads a historical life. To recognize the ontological status of philosophy, another, at least equal in value, form of cognition is needed. Maybe it's art? But for aesthetic taste, the experience of knowing the world in terms free from sensibility is alien by definition. Maybe it's a religion? But faith is jealous, it itself claims to know the absolute truth and philosophy is a «deadly» competitor for it, a hundred times worse than other religions with which it fights for recognition by the people, the state and the historical era.\nTo answer this question, we need the second plot, which is contained in the above passage from Kant. Philosophy, he says, «must prove its integrity by itself keeping its own laws». Of course, a clarification suggests itself here: not only its own laws, but also the laws of the universe. These are the same laws! Hegel, «closing» one period of the existence of philosophy, the period of its definition through the attitude to its own historical forms of existence, reveals a new one: the definition of oneself through its own other. In its entirety, he explores the key features of this period in his «Logic of essence», demonstrating its power according to all the structural elements of his system, preparing an era when the law «attitude to its other» will fall on a philosophy itself, which has now been an ontological fact.\nThis form of the absolute other being of philosophy initially, in the root, in its very core, should be permeated with reflection and visibility -non -subservience of being. This is, as it were, a theory, but in fact it is not a theory, but ratherpractice. But, perhaps, and vice versa: this is not a practice, but a theory that first revolutionary remodels reality in thought, and only then carries out this in being. This is, as it were, the fruit of the activity of individual will, while in fact -the result of the activities of the masses. But, even rather, on the contrary, the activities of the masses under the guidance of the will of the individual leaders. Finally, this is a kind of philosophy, which in reality does not tolerate philosophy, or, conversely, not a philosophy, which with all his might wants to seem a real philosophy. This is ideology.\nIdeology is philosophy «overturned» into politics, and politics «overturned» into philosophy. There is no need to speak here about any sequence of ideas, when the emergence «removes» the limitations of the previous stage, bringing something new to itself. The Hegelian method of «transitions» to this period of the otherness of philosophy is either not possible here at all, or must be relegated to the background from the very beginning. Therefore, historians of «non-classical» philosophy most often limit themselves to the genesis of ideas within an individual thinker or school, direction and focus on the reflection of existentialism into positivism, phenomenology into ontology, neo-Kantianism into neo-Hegelianism, etc. And they do it right. This is how the object behaves, and they can only follow it. In the rhizome (tuber, rhizome) of social practice, three directions are ripening, each of which defends itself, denying its other. All their forces are spent not on creating something new, but on improving, as K. Mannheim writes, the method of criticism. This method «reduces to the destruction of the enemy's utopia by revealing its conditionality by being» [10]. If these words are clarified in the terminology of K. Mannheim himself, then such criticism demonstrates how each «total» ideology seeks to show that its competitor is in fact not a total, but a «partial» ideology, that is, a false, distorted consciousness [16].\nAccording to all the laws of the «phenomenological genre», a relationship of recognition developed between two self-consciousness: philosophy and ideology. Each sees its own in the other, and the other in itself. If necessary, philosophy easily changes the negative content of the concept of «ideology» (as false consciousness) into a positive one, receiving in return exactly the same dialectic (K. Mannheim).\nAll three major ideologies of the 20th century at the initial stage not only do not break off relations with philosophy, but, on the contrary, sometimes even curry favor with it. Until now, for example, the ideology of liberalism in Wikipedia is characterized as a «philosophical and socio-political movement.» In response, philosophy immediately declares its practical aspirations. Here is a statement, according to R. Safransky, the founder of philosophical anthropology H. Plesner: «Philosophical thinking «is never as wide as life, and at the same time it is always wider than life» [13]. And here R. Safransky confidently emphasizes that for Plesner «awareness of the radically understood historicity leads to the point of view that philosophy, not only because of the obligation imposed on it from the outside, but also because of the internal logic of its development (our italics) must enter the dangerous sphere of political life». As they say «two in one»: both the Hegelian movement in essence from nothing, through nothing, to nothing, and a statement of the fact of the «fall» of philosophy into politics (F. Lange).\nConclusions of the research. Philosophy, acquiring its otherness in the face of ideology and starting from its non-real (illusory) being, got the opportunity to develop its completely new, now post-historical, forms of existence. As a result, the relationship between philosophy and ideology has consistently unfolded in the historical reality of the last two centuries all three logical forms: the transition (philosophy into ideology), reflection (philosophy into ideology and vice versa), and, finally, the removal of ideology in philosophy."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 10870,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255039507"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The tractatus ladder from a wittgensteinian religious point of view\n\nI propose that, in its form, it is possible to make a religious reading of Tractatus Logico-Philosophicus along the lines of how Wittgenstein himself understood religion\n\n\nIntroduction\nWittgenstein's ideas about religion have long been a topic of scholarly debate.\nSome authors have tried to show how Wittgenstein understood both religious discourse and religion itself. Others have tried to apply his ideas about philosophy and his notes on religion to classical problems in the philosophy of religion. Both perspectives show how fruitful his work is for debates in this area. This article, however, neither intends to make an exegesis of Wittgenstein's notes on religious belief nor apply his ideas to classical problems in this discipline. Rather, this work aims to be an exercise in reading the Wittgensteinian idea that the Tractatus is a ladder that after being used must be thrown away. I propose that the ladder can be read from a Wittgensteinian religious perspective, and that this reading would help produce a better understanding of the final aphorisms of the Tractatus. Everyone who has studied Wittgenstein's work will be familiar with the conversation between Wittgenstein and Drury in which the author of the Tractatus makes the following statement: \"I am not a religious man but I cannot help seeing every problem from a religious point of view.\" (RHEES, 1984, p. 94). This statement has already been the subject of important discussions-perhaps the most important of which is Norman Malcolm's (1993)  Furthermore, I want to show that a specific religious point of view is compatible with some of Wittgenstein's ideas, that is, his own religious point of view-which is his perspective on how instruction in a religious faith works in conversion and the role of religious doctrine. Finally, it is important to emphasise that, unlike Malcolm who proposes four possible analogies between a religious point of view and Wittgenstein's second work, 1 I am interested only in two, as highlighted above: religious instruction for conversion and the issue of religious doctrine. In short, I want to show that the very Wittgensteinian idea of how instruction in a religious belief works can be seen in the way the Tractatus sets out the idea that such work is a ladder to see the language, the world and the life correctly. In turn, I emphasize the idea that Wittgenstein's ideas about religious doctrine can help us to understand the sense of the Tractatus as a whole.\nTo do this, I first intend to examine the Wittgensteinian idea that religious belief is a system of references. And to enter it, a very special kind of instruction is necessary-which is described in Culture and Value (1980, p. 64) as follows: Instruction in a religious faith, therefore, would have to take the form of a portrayal, a description, of that system of reference, while at the same time being an appeal to conscience. And this combination would have to result in the pupil himself, of his own accord, passionately taking hold of the system of reference. It would be as though someone were first to let me see the hopelessness of my situation and then show me the What interests me in this passage is the description of the process of becoming religious or entering a religious system of references. In this sense, I am not interested in focusing on the content of the process but on the steps that take one from not belonging to a religious system to adopting it. In the above passage, Wittgenstein enumerates three steps to instruction in a religious faith that leads to its adoption.\nSuch steps begin by 1) demonstrating the hopelessness of the believer's situation; passes through 2) showing him salvation: the religious reference system itself; and ends with 3) an awareness of the need for religious faith. In my view, these three steps for the conversion to or adoption of a religious faith can also be seen in the Tractatian ladder that the reader must first climb to understand the work, and then be able to throw the ladder away and become aware of a correct view of the world and of life.\nTo reiterate, in 6.54 of the Tractatus, Wittgenstein states that his book is to be taken as a ladder, which after being used must be thrown away so that the reader can see the world correctly. But what are the rungs of this ladder? In my view, for the reader to understand the world correctly after reading the work, it is necessary to climb up at least three rungs analogous to the steps of instruction in a religious faith.\nLike a good instructor who thinks about problems in a religious way, Wittgenstein, in the Tractatus, would then begin by 1) demonstrating the hopelessness of philosophy problems; 2) revealing the salvation for philosophy: a correct understanding of the logic of language; and then end 3) by making the reader, by himself, seize the way out: changing his life by beginning to see the world correctly. The instructor, Wittgenstein, sets out the first two steps in his own work. The third, despite being also shown in his work, needs an awareness induced by the correct understanding of the book.\nTherefore, I will defend the possibility of reading the Tractatian ladder as analogous to the process of instruction in a religious faith. I say analogous because while religious instruction leads to a correct view of the system of religious references and its importance for the life of the believer, the Tractatian ladder, if climbed correctly, leads to a correct view of the language, the world, and the life. This understanding of the Tractatus ladder as analogous to religious instruction helps us to better understand why the ladder needs to be thrown away, as the Tractatus proposes, and therefore helps us to understand the sense of the Tractatus sentences themselves.\nAs I will defend later, the sentences can be understood as analogous to the religious doctrine in that, despite not saying anything in the strict sense of the Tractatus, they play an important role in showing a correct vision of the world, the language, and the life. If my proposal for this reading of the book is plausible, the Tractatus, despite not being the work of a religious man, is the work of someone who, even in his youth, \"cannot help seeing every problem from a religious point of view\". 2 I will begin by elucidating each of the possible steps of the Tractatus, analogous to instruction in a religious faith, so that later I can show the effects of this for the interpretation of the work's sense.\n\nShowing the hopelessness of philosophy's situation\nOne of the central points in understanding the Tractatus is, without a doubt, understanding that the author of the work is a harsh critic of the preceding philosophy. Wittgenstein announces this in the preface to the work, stating that \"[t]he book deals with the problems of philosophy, and shows, I believe, that the reason why these problems are posed is that the logic of our language is misunderstood\" (TLP, p.3). This is even clearer in 4.003 of the Tractatus, when he states that: Most of the propositions and questions to be found in philosophical works are not false but nonsensical. Consequently, we cannot give any answer to questions of this kind, but can only point out that they are nonsensical. Most of the propositions and questions of philosophers arise from our failure to understand the logic of our language. (They belong to the same class as the question whether the good is more Rev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 68-84, out./dez. 2022 or less identical than the beautiful.) And it is not surprising that the deepest problems are in fact not problems at all.\nBasically, for Wittgenstein, philosophy has been concerned with unanswerable problems. She has tried to propose and discover the essence of the world and to answer questions about, for example, what defines being in itself, whether the beautiful and the good are the same, and indeed what is the sense of existence. The big problem is that these questions cannot be answered sense fully, because language with sense only contains propositions that figure the world, which are subject to truth and falsity, and which are composed by names denoting objects in the world. That is, language with sense belongs only to human investigations that are concerned with how the world is and with facts that are or are not the case. Thus, the great philosophical questions that have preoccupied great philosophers' minds are, in fact, the result of a misunderstanding of the logic of language-a misunderstanding of the limits of what can or cannot be said with sense.\nThe philosophy prior to the Tractatus is nonsense because it seeks to make theory and to provide answers with claims of truth or falsity-but there is no object that corresponds to a study of philosophy. For the Tractatus, a correct understanding of language forces this philosophy into silence. And it must be silent simply because it is unable to give meaning of the linguistic signs that it has always used.\nFor Wittgenstein, the despair that devastates the non-believer before religious conversion is the despair of not seeing a way out of his life and of not seeing a solution to his problems. An analogous despair seems to be present in the philosophy that Wittgenstein criticizes. It is always dealing with problems that have no solutions: pseudo-problems generated by its misunderstanding of the correct way of doing philosophy. These pseudo-problems entangle the philosopher in studies that generate confusion and logical errors, which make him stuck, and which produce an incorrect understanding of the life and of the world.\nThe first step in changing this is to become aware of the situation. In this sense, in the first step of the Tractatus, Wittgenstein shows to the reader the despair of his situation as a philosopher-his life as someone who is dealing with problems that are not really problems. He shows us the misunderstanding of the philosophical life that the philosopher intends to lead. In a way, Wittgenstein makes the philosopher aware of the hopeless situation of philosophy. Seeing this hopelessness is essential in order to climb to the next rung: the one where the philosopher can find the solution to his desperate situation.\n\nShowing to the reader the way out: a correct understanding of the logic of language\nJust as the believer, prior to conversion to the religious belief system, despairs at not understanding the correct path for his salvation, Wittgenstein shows that pre-Tractatus philosophy is on a completely wrong path. It is just a set of pseudo-problems.\nThe reader of the work, realizing he is involved in such an inconsequential task, then needs a salvation. And the Tractatus, analogously to good religious instruction, describes the new system of references, that is, a new philosophy which can make the reader climb up one more rung towards clarification of his role.\nThis new philosophy or system of references, unlike traditional philosophy, is not a set of doctrines that try to express absolute truths about the world and about life, but she is a \"critique of language\" (TLP, 4.0031). Her task is to tell us what can and cannot be said and what does and does not make sense. This proposal does not include a body of doctrines and treaties, nor is it a specific field of knowledge.\nPhilosophizing comes to be understood as an activity of conceptual clarification. As Hacker (2001, p. 324)  The correct method in philosophy would really be the following: to say nothing except what can be said, i.e. propositions of natural science-i.e. something that has nothing to do with philosophy-and then, whenever someone else wanted to say something metaphysical, to demonstrate to him that he had failed to give a meaning to certain signs in his propositions.\nAlthough it would not be satisfying to the other person-he would not have the feeling that we were teaching him philosophy-this method would be the only strictly correct one. (TLP, 6.53).\nSo, philosophy as a doctrine must disappear, and a philosophy as clarification  incapable of answering such questions because in trying to do so, it struggles against the limits of language; it does not understand such limits. By not understanding them, philosophy, as metaphysics, seeks to deal with things that are outside of the limits of language as if they were things with sense -as if they were possibilities susceptible of truth or falsity. This is how pseudo-ideas about the Self, God, Ethics, Aesthetics, the sense of the world, eternity, and so on are born.\n\nRunning to and grasping the way out: seeing the world correctly and changing your life\nOnce the hopelessness of philosophy as a traditional metaphysics is understood, and the path to a correct understanding of philosophy and language is shown, it is up to the reader of the Tractatus to access the last rung of the ladder: to become aware of the limits of language and to see the world and life properly. Or, to put it better, we become aware of the limits of language and its effects on philosophy, our understanding of the world, and of life.\nIn the preface to the Tractatus, Wittgenstein states that, perhaps, his book \"will be understood only by someone who has himself already had the thoughts that are expressed in it -or at least similar thoughts\" (TLP, p. 3). He is, in a way, saying that a correct understanding of the work involves the reader trying to take, by himself, the path that the author is taking. In this sense, it is necessary to climb the ladder while simultaneously thinking for oneself about its sense. If he does so, the reader of the Tractatus, as well as the religious person with his new system of references, will adopt the ideas of the Tractatus and the limits it shows him. He will see that the totality of what language manages to picture is the totality of the world, and beyond that, nothing with sense can be said. The reader will see that, like an eye in the visual field, he sees only as far as the limits of the world, but he cannot see beyond these limits. He will see that a world exists, but that we cannot speak about the foundations of its This awareness arises through a kind of astonishment at the fact that there is a world.\nThus, the world presents itself as sub specie aeterni.\nIn aphorism 6.45, Wittgenstein wrote that \"To view the world sub specie aeterni is to view it as a whole -a limited whole. Feeling the world as a limited whole -it is this that is mystical.\" Such an idea can only be understood within a total vision that permeates the work. 5 The Tractatus understands the world as the totality of facts that occur within a field of possibilities called logical space. All facts are of equal value and are contingent in the sense that they are or are not the case. It is necessary to remember that in the world everything is as it is and everything happens as it happens, there is no value in it (TLP, 6.41). All propositions representing this world have equal value, that is, no value. The world is a whole made up of facts limited by facts. In the world nothing is beyond facts, nothing but the space of possibilities given by the logical space. Everything can be otherwise. Black (1964), when commenting on the mystic of the Tractatus, argues that the vision of the world as a limited totality is the intuition that there is something beyond the factual world that cannot be expressed in words. This something is the sense of the world, which has real value. In the world there are only facts and there is nothing of value, everything that has value is outside the world.\nThus, when Wittgenstein says that the mystical feeling is the intuition of the world as a limited totality, he is not saying something metaphysical or extra-linguistic, but simply that to see the world in the form of eternity is to see the world as limited to facts. The subject who perceives the world from a timeless point of view is perceiving it outside space and time, as the totality of possible facts. It is the perception of the world in a space of possibilities where everything can be otherwise, but otherwise factual. To see the world in this way is to see it in the form of eternity, beyond time and space: it is to conceive of it in its entirety.\nBut this is not a fleeting feeling or something that is made inside us by the work of some being. It is a feeling that arises from the logical understanding that the Tractatian ladder provides: to see the world as limited is to see it through the general form of the proposition. Such intuition is mystical because it cannot be said. Nothing we say about this possible view of the world as a whole makes sense. No proposition can reach the totality of the world. Its wholeness can only be achieved outside it. It is necessary to be on the edge of the world to see it as a totality, and to see it as a totality is to detach it from a certain moment in a certain space and see it timelessly. The eternity of the sub specie aeterni is seen as timeless, independent of any connection with time. And this is shown by logically articulated language: the logic of language, through the idea of possibility, shows us how the world is.\nSeeing the world in the form of eternity provides us with a move away from the factual world towards a vision of the world as a whole. The subject realizes, through this understanding, that the world is composed of facts that can or cannot happen. To be like this is merely a fact that could be different. It is to see reality not as absolute, but as a space within possibilities. This vision is not attained by the psychological subject, who as part of the world, is in the time and space of a physical and factual life. Such a vision of the world is reached only by the volitional subject, who sees the limit of the world, which as a limit is outside the factuality of the world, despite being only part of this world. It is necessary to emphasize again that the vision of the world sub specie aeterni is only possible if we are equipped with the conceptions that the world is a limited whole and that language is also limited. Otherwise, we will always try to go beyond the limits of language and we will stop contemplating the world in the form of eternity. Therefore, the reader, as a last resort, will only succeed in understanding the Tractatus if he or she performs the exercise of running to it and grasping its ultimate sense. This is precisely to recognize that the propositions set out in the Tractatus are a contra-sense that only serve to show the limits of the language and of the world and  I have argued that it is possible to read the Tractatus from Wittgenstein's own religious perspective because he claims that he always sees problems from a religious perspective. In my proposed reading, I stated that the reader of the Tractatus is taken along a path that begins with an awareness of the despair of pre-Tractatus philosophy, then passes through an understanding of how to do philosophy correctly and ends with an awareness of the limits of language and the world that makes a correct understanding of the world and life possible. In this sense, the book's ultimate aim is practical; that is, it should lead to a profound change in philosophizing and in the way we see the world and life, and in the way we relate to the problems that arise in our world and in our lives. Thus, to the author of the Tractatus, it matters less that the reader knows the complete doctrine (sentences) of the work he writes and more the practical effects that the understanding of the work generates."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 11281,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255255480"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Seeing Croatian Islands as Something Other than Paradise\n\nSeeing Croatian Islands as Something Other than Paradise This article is a review of the study entitled Życie na wyspach. Chorwacka współczesna proza insularna [Life on Islands: Contemporary Croatian Insular Fiction], authored by Anna Boguska (Warszawa: Instytut Slawistyki PAN & Fundacja Slawistyczna, 2020, pp. 232).Inne spojrzenie na chorwacką wyspę jako rajRecenzja monografii naukowej Życie na wyspach. Chorwacka współczesna proza insularna, autorstwa Anny Boguskiej (Warszawa: Instytut Slawistyki PAN & Fundacja Slawistyczna, 2020, 232 ss.).\n\nby examining Croatian writers' own literary examinations of life and living on islands. In so doing, Boguska reveals a multifaceted landscape of diverging interpretations of islands that is at odds with the well-known stereotypes; she embarks on a journey into the often-hidden borderlands of European literature, uncovering jewels of Croatian writing unknown outside of Croatia and yet tied to well-known European philosophers and writers.\nThe 232-page book is structured in five distinct parts. As is typical of a doctoral thesis-turned-book, Boguska begins with an excursion into the theoretical framing of her study, but the clarity and brevity of the introduction is untypical of such books. In the first part, she draws on the comments of Croatian writers and publicists, such as Damir Miloš and Radovan Marčić, as well as the American historian John Gillis, to make two things clear in regard to the topic and methodology. First, despite any deeper objective authentic attributes, islands are functionalized as a place for the projection of human needs and dreams. Rather than depicting islands realistically, literature and culture describe them in order to meet humans needs and desires. Second, Boguska takes care to let Croatians' views on Croatian islands be heard while also reflecting general scholarship on insular literature in general, thus promising a balanced scholarly approach. This is also the crux of her methodology. In combining Paul Ricoeur's approach to symbols with close textual readings of the works of 20 th and 21 st century Croatian authors, she expands the meaning of islands beyond their local significance (although this is also part of her contribution) to include their broader meaning in universal categories. Thus, rather than an ethnographic discussion of life on Croatian islands, she examines human concepts of possible ways of being and how these concepts are projected onto the islands of the discussed works. She identifies four types of symbolic projections on islands: an island as a desert or emptiness; an island as a prison; an island as Theatrum Mundi (the Great Theater of the World); and, finally, an island as a garden. Each chapter is dedicated to one of these concepts and centers around the discussion of two key, thematically relevant works, although other pertinent examples appear in the margins.\nThe book's first chapter, \"Island-Desert,\" first elucidates the meaning of desert, recalling Michał Paweł Markowski's question \"What kind of 'thing' is a desert?\" (Boguska, 2020, p. 22) to set the frame for the examination of the image of an island as a desert by visiting Petar Šegedin's Mrtvo more [The Dead Sea] (1953) and Damir Miloš's Otok snova [The Island of Dreams] (1996). The study draws on Polish philosopher Barbara Skarga's understanding of the term 'desert', comparing and contrasting it with Jacques Derrida's essay on the Greek term khôra. Following Skarga's argument, Boguska sees a khôra, and subsequently an island, as both a place of complete solitude and aimless wanderings without a final destination, as well as a luring mirage of a potential horizon that, unlike its biblical use, ultimately disappoints. The title of Šegedin's novel, The Dead Sea, indicates a sense of hopelessness that Boguska fleshes out in readings in two sections: \"The desert as a void\" (Boguska, 2020, pp. 25-35) and \"The desert as a khôra\" (Boguska, 2020, pp. 35-41). With the help of the select quotes she has painstakingly translated, Boguska highlights the sense of enduring emptiness pervading Šegedin's Dead Sea, as is summarized in the quote \"There was no one, only emptiness\" (Šegedin, 1953, p. 7). The village of The Dead Sea is seemingly lifeless, immersed in quiet, save for the buzzing of flies -a lifeless place amidst the waves. The protagonist finds himself in a twofold emptiness: both alone in his thoughts and cut off from the world surrounding him. His mental battles with demons in solitude do not lead him to some biblical clarity for his future path but rather are an ever-repeating vicious circle, thus illustrating Skarga's first concept of the desert. The second finds its expression in the protagonist's attempts to reinvent himself and his reality through the act of writing in order to evade death and not forget the past; yet, as Boguska points out by drawing on Borge and others, the achievement of this goal is out of reach as the writer is constructing his world as he writes, not simply narrating it.\nIn the first chapter's second part, \"the Mirage in the Desert\" (Boguska, 2020, pp. 41-65), Boguska turns to Damir Miloš's Otok snova [The Island of Dreams]. Miloš portrays an island as a place of promise, a projection of the touristic expectations of islands as a return to a place of eternal youth, sensuality and sexuality during vacation and outside the confines of everyday mores and life. Rather than an Arcadian idyl, Boguska sees in the protagonists' actions and words a warning of increasing immaturity and infantilism, thus reflecting Francesca Cataluccio's views of islands. In Miloš's text, the island has been reduced to a commodity to be consumed and offers no form of deeper enlightenment associated with Utopia. Although one is inclined to agree with this assessment, the remarks would have benefited from a closer examination of how this is specifically typical for modern humans.\nBoguska continues to demonstrate how Miloš's text does not simply reject the tourist cliché but rather engages in the art of parodying a range of classical texts, its title referencing the Island of the Blessed, whose plot and characters prominently feature in the text. In this way, the author leaves no doubt as to Croatian literature's place in the longer Western classical literary tradition, even while it challenges older conventions. Like Šegedin, so too does Miloš do this in typical postmodern fashion by turning to the construction of the text itself and questioning the creation of meaning as well as the author's moral obligation to his characters. In the end, the meaning of the text is a mirage, not a dream, offering no unequivocal answers.\nReturning to the motif of an island utopia, Boguska thus concludes that both Croatian texts are anti-utopian, not merely dystopian, in the sense that they reject the utopia of an island as a new, positive non-place. Rather than a utopia in More's sense, Boguska reveals these first examples of insular literature to be treacherous mirages of a better way of being. Certainly, on the basis of the two texts analyzed here, one must agree with this conclusion.\nFollowing the first chapter's rejection of an island as a utopia, the second chapter, \"Island-Prison\", engages with the concept of an island as a prison in Croatian insular literature. In keeping with the previous chapter's structure, the author illustrates this symbol of an island with the help of two novels: Slobodan Novak's Mirisi, zlato i tamjan [Gold, Frankincense and Myrrh] and Renato Baretić's Osmi povjerenik [The Eighth Commissioner].\nIn the first part of the second chapter (Boguska, 2020, pp. 67-86), the author examines Slobodan Novak's novel against the backgrounds of Croatian history, world literature and philosophy. Expanding on Tatiana Jukić's assertion that the krugovaši -a group of Croatian writers who contributed to the journal Krugovi after World War Two and dedicated themselves to deconstructing previous black and white depictions of war and politics -Boguska summarizes the plight of the former comrade Mali as a prisoner on an island. Trapped at the bedside of a dying old woman, first questioning and then accepting his fate for causes unknown to him, the author compares the experience to Kafka's K. and views the isolation and lack of privacy as a variation on Jeremy Bentham's Panopticon and a reflection of Zygmunt Bauman's criticism of modernism and the automatization of the individual. Here, as elsewhere, the author takes care to place Croatian insular literature in a broader context in order to make its larger relevance clear, but the reader occasionally wishes for more information on the cultural specifics. A brief remark on the former prison island, Goli Otok, itself the subject of more than one fictional text, would have further anchored the author's arguments, accented the frequency of portraying an island as a prison, and acquainted readers with Croatian realia.\nThe longer second part of the second chapter (Boguska, 2020, pp. 86-104) is dedicated to Baretić's award-winning Osmi povjerenik [The Eighth Commissioner], a modern literary utopia. Boguska oscillates between drawing parallels to Jean-Jacque Rousseau's rejection of civilization and Thomas More's Utopia, pointing out topographical similarities to the latter as well as accenting the contrast with the protagonist's corrupt city of Zagreb and the idyllic vision of a fictional island, Trečić, as a juxtaposition typical of literary utopias. Although the island stay transforms the protagonist Šimiša from an egotistical politician to a compassionate, thoughtful islander, thus echoing the plot of classic utopias, Boguska argues that his return to the island of Trečić is also a prison and an illusion. Here, one may come to a different conclusion. Trečić certainly has its dark sides (one need only think of the fate of the seals) and it is not an image of perfection, as most modern utopias are not; however, it may be considered more positive than the trap (Boguska, 2020, p. 102) suggested here: if the world itself is not entirely transformed, the protagonist, at the very least, has been transformed and finds a better way of being.\n\nand Pavao Pavličić's Koraljna vrata [The Coral\nGate] (1990). Marinković's novel, which is set during the Second World War, depicts an island as a modern Ithaca to which the protagonist, Mateo Bartol Svilić, desires to return as an idealized place of his youth. As the title's homage to E. A. Poe suggests, Svilić learns there can be no return to the past, and Marinković engages in the art of intertextual references beyond Croatian literature, citing not only Poe but also incorporating characters from Tolstoj, Sienkiewicz and, of course, Homer. Relying on Bergson and Kristeva, the author reflects on the protagonist's view that everything experienced on an island is simply a farce and not authentic. The predominating melancholic humor may be read as the protagonist's reaction to a tragic loss. This hopelessness is also shared by the author, Marinković, who fell silent shortly after the book's publication.\nThe stage of Pavličić's novel is the island of Lastovo, on which the protagonist Krsto Brodnjak arrives. Lastovo is idealized as an ideal place, free from evil, awaiting \"resurrection\" both from its winter sleep and in a metaphorical sense (Krsto's name evokes Christ). Like Marinković, the writer and literary scholar Pavličić engages in postmodernist intertextual play with Boccaccio's Decameron as well as the myths of Pandora and literature's origins, echoing Macpherson's Ossian. He uses the protagonist's island experience in order to portray the search for differentiation between \"Sein und Schein\" and to explain how evil can exist in a world created by God. Boguska reads Pavličić against the backdrop of the \"paradox of perfection\" of the Polish philosopher Władysław Tatarkiewicz, Odo Marquard's Glück im Unglück [Happiness in Unhappiness] and Gottfried Leibniz's theodicy. In her analysis, she argues that Pavličić rejects the teleologization of evil, and she places his work in the tradition of a utopian hermeneutic of seeking a better way of being rather than literary utopias per se. The precise reading of key passages of Pavličić's work are a welcome addition to the ventures into philosophy, and one is inclined to wish for more of the same for the benefit of readers not familiar with Croatian literature, particularly for such works that have not been translated. Boguska concludes by stating that the novels can be summed up by a rule from Aeschylus's Agamemnon which states that \"men must learn by suffering\", making an island not an earthly paradise but a testing ground for ideas, inseparate from earthly suffering elsewhere.\nThe study's final chapter, \"Island-Garden\" (Boguska, 2020, pp. 145-195), returns to a more conventional, widespread motif tied to islands, recalling the earthly paradises presented in much European literature and art's depiction of the South Seas: an island as a garden. As in previous chapters, two works are visited to illustrate this symbolic use of islands: Slobodan Novak's Izgubljeni zavičaj [The Lost Homeland] (1954) and Senko Karuza's Vodič po otoku [A Guide to the Island] (2005).\nThe first two sections of the author's discussion of Novak's Lost Homeland accent two aspects of a closely related theme: the garden as an earthly paradise and as an Arcadian garden. Boguska differentiates between the two terms, arguing that the bliss of working and its rewards are foregrounded in representations of earthly paradise, while an Arcadian garden shares the primitivism of the genre of the idyll and pastoral and focuses on the harmony of humans and nature. Boguska does not accept Novak's visions at face value but argues that the text's final sections lend it an allegorical note, viewing the lost paradise as the protagonist's idealized childhood, to which it is impossible to return after breaking off all contact with his family home (Boguska, 2020, p. 161), leaving him forced to forget his former life (Boguska, 2020, p. 162) and to live in a world without his own place. In this way, the author argues that Novak's vision is an illusion, placing his work in the sequence of Croatian insular fiction that views the modern world as a desert and the condition of human existence as melancholic and depressive. In her summary, Boguska argues that such representations demystify the image of Croatia as a tourist paradise, a point that could be expanded upon in this subchapter and convincingly tied to the results of the first chapter. For while Novak rules out a return, tourism presents a temporary escape from the modern world, selling the illusion of escape to the illusion of an idyll, an idea developed in the discussion of the book's last work, Vodič po otoku [A Guide to the Island].\nBoguska argues that Senko Karuza's collection of short stories breaks with the negative appraisal of the island motif, giving special consideration to the short stories \"Selo\" [Village], \"Vinograd\" [Vineyard], \"Turisti\" [Tourists] and \"Magarac\" [Mule]. In the first, the author reflects on Karuza's depiction of the island of Vis as an unmasking of the touristic vision of an island as a product to be consumed, a vision performed by the inhabitants of Vis for visitors, challenging the island's symbolic meaning and echoing the treatment of islands as commodities that is found in the work of previous authors. The theme of authenticity, illusion and the typical visitor to a supposed utopia is fleshed out in the following discussion of the collection's other stories; then, an island as a philosophers' garden is examined in the sections dedicated to Gaston Bachelard's \"oneiric house\", Martin Heidegger's \"being-in-the-world\", Montaigne and Socrates. Boguska employs these authors' views to note an affinity between the existence of Karuza's characters and a mode of disclosing the truth, understood as aletheia, to argue that Karuza creates a new type of character and represents a break from modern and postmodern melancholic tendencies in Croatian insular fiction. Unlike previous texts, Boguska argues that Karuza combines a vision of paradise with a dynamically changing vision of one's home that differs from the static commodity that is found on tourist postcards. This chapter, unlike previous ones, makes an uncomfortable break from frequently citing the primary literary text in its philosophical discussion, thus giving readers unfamiliar with Karuza pause to question the assertions' validity.\nThe study concludes with a short overview of the presented materials, rephrasing the individual chapters' conclusions, before accenting the special place of Senko Karuza in escaping the postmodernist negation of meaning and immersion in play as offering a fresh, new approach to islands and discovering humankind's place in the world. These are all valid remarks; however, the curious reader would relish an outlook on Croatian insular literature's trajectory since the appearance of Karuza's collection, as well as a more detailed evaluation of islands as a utopia as either an irrelevant motif of the past or one that has further potential. An outlook on unstudied insular texts would help quench readers' thirst for more knowledge on this understudied subject and would indicate points of departure for future studies.\nThere can be no question that the book is an invaluable contribution to the study of Croatian insular literature that goes beyond the specialized interests of scholars of Croatian literature or utopian studies. By engaging with the thoughts of philosophers and writers in her analyses, Boguska goes to great lengths to contextualize the multifaceted aspects of the island motif as so much more than a touristic or earthly paradise. Her study thus shows how such ideas may open up new understandings of these texts, as well as how prominent thought has been drawn on by Croatian authors, thus demonstrating, once again, how Croatian literature is tied to well-known Western European thought and traditions. Enthusiasts of Croatian literature will welcome this survey's potential to make discussion of the tradition of Croatian insular literature available to a broader circle of readers. These readers will also cherish the bibliography, the footnotes and the hints for further reading, while those new to the subject will perhaps wish greater attention had been paid to the chosen texts' cultural specificity."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-27"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 13424,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254853687"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages\n\nThis paper investigates the problem of Named Entity Recognition (NER) for extreme low-resource languages with only a few hundred tagged data samples. NER is a fundamental task in Natural Language Processing (NLP). A critical driver accelerating NER systems' progress is the existence of large-scale language corpora that enable NER systems to achieve outstanding performance in languages such as English and French with abundant training data. However, NER for low-resource languages remains relatively unexplored. In this paper, we introduce Mask Augmented Named Entity Recognition (MANER), a new methodology that leverages the distributional hypothesis of pre-trained masked language models (MLMs) for NER. Thetoken in pre-trained MLMs encodes valuable semantic contextual information. MANER re-purposes thetoken for NER prediction. Specifically, we prepend thetoken to every word in a sentence for which we would like to predict the named entity tag. During training, we jointly fine-tune the MLM and a new NER prediction head attached to eachtoken. We demonstrate that MANER is well-suited for NER in low-resource languages; our experiments show that for 100 languages with as few as 100 training examples, it improves on state-of-the-art methods by up to 48% and by 12% on average on F1 score. We also perform detailed analyses and ablation studies to understand the scenarios that are best-suited to MANER.\n\n\nIntroduction\nNamed Entity Recognition (NER) is a fundamental problem in natural language processing (NLP) (Nadeau and Sekine, 2007). Given an unstructured text, NER aims to label the named entity of each word, be it a person, a location, an organization, and so on. NER is widely employed as an important first step in many NLP downstream applications, such as scientific information retrieval (Krallinger and Valencia, 2005;Krallinger et al., 2017), question answering (Mollá et al., 2006), document classification (Guo et al., 2009), and recommender systems (Jannach et al., 2022).\nRecent advances in NER have mainly been driven by deep learning based approaches, whose training relies heavily on large-scale datasets (Rosenfeld, 2021). As a result, the most significant progress in NER is for resource-rich languages such as English (Wang et al., 2021), French (Tedeschi et al., 2021), German (Schweter and Akbik, 2020), and Chinese (Zhu and Li, 2022). This reliance on large training datasets makes it challenging to apply deep learning-based NER approaches to low-resource languages where training data is scarce. To illustrate the ubiquity of low-resource languages, WikiANN (Rahimi et al., 2019), one of the largest NER datasets, has NER labeled data for 176 languages; however, 100 of those 176 languages have only 100 data samples.\nProviding NER for low-resource languages is critical to ensure the equitable, fair, and democratized availability of NLP technologies that is required to achieve the goal of making such technologies universally available for all (Magueresse et al., 2020;King, 2015). In addition to our team, several research teams are pushing the frontiers of NER for low-resource languages in two orthogonal and complementary directions. The first direction aims to obtain larger NER datasets to solve the data scarcity problem, via either direct data collection or augmentation (Malmasi et al., 2022;Al-Rfou et al., 2014;Meng et al., 2021a;Rahimi et al., 2019). The second direction aims to develop new model architectures and training algorithms to account for the data scarcity. For example, ideas from meta-learning (de Lichy et al., 2021), distance supervision (Meng et al., 2021b), and transfer learning (Lee et al., 2017) leverage the few-shot generalizability of language models for NER in data-scarce settings.\nContributions: In this work, we propose Mask Augmented Named Entity Recognition (MANER), a new approach for NER of low-resource languages that does not rely on additional data or modify existing model architectures. MANER exploits the semantic information encoded in a pre-trained masked language model (MLM). Specifically, we reformat the input to the MLM by prepending a mask token to every token in the text to be annotated with NER tags. This reformatted input is then used to fine-tune the MLM with a randomly initialized NER prediction head on top of the prepended mask tokens.\nExtensive experiments on 100 extremely lowresource languages (each with only 100 training examples) demonstrate that MANER improves over state-of-the-art approaches by up to 48% and by 12% on average on F1 score. Detailed ablation and analyses of MANER demonstrate the importance of using the encoded semantic information (<mask> token vs. new random token, languages seen during pre-training vs. languages seen only during fine-training), and performance with respect to the number of training examples).\n\nMethodology\nIn this section, we develop MANER , which repurposes the <mask> token for NER training. MANER takes as input a sentence and outputs a NER label for each word in the sentence.\nIn the following section, we first propose a key modification to the input sentence S that incorporates the <mask> token and second specify the design details of MANER.\nProposed modification to input sentence: We append a <mask> token to beginning of each word in sentence S. The new sentence S = {m, w 0 , m, w 1 , .., m, w n−1 } where m is the <mask> token. The modified labels L are {c 0 , ∅, c 1 , ∅, .., c n−1 , ∅}. The original NER label of each word in the sentence is assigned to the <mask> token to the immediate left of the word.\nWe hypothesize that in such a setting, MANER will be better able to use the <mask> token to weigh the relative relevance of the neighboring word vs. the rest of the context when determining the label to assign to the neighboring word. For instance, Paris is founder of a new AI startup called Banana. Appending a <mask> token before Paris captures additional contextual information that may be helpful to classify the NER label of Paris.\nMANER: We now describe MANER, which consists of a transformer model and an NER classifier. The transformer model takes a sentence as input and outputs embeddings for each token in the sentence. The NER classifier uses the token embeddings to output the most probable NER class for each token. Denote the MANER model by M. The transformer model is given by T , The NER classifier is modeled using a weight matrix M ∈ R D×|N | that takes the computed token embeddings as input. Using these token embeddings, the classifier output scores for all NER labels for each token in the sentence. Passing these scores through a softmax nonlinearity provides probabilities p i ∈ R |N | for all NER classes in N for a given token i in S: Summing up, we have The weights of M and T are learned/fine-tuned by minimizing cross-entropy loss. Note that the loss is not calculated for labels marked ∅ in the modified label set L . The NER label of the word is given by the NER label of the <mask> token preceding it. MANER inference: Similar to training, during inference, each token in the sentence is prepended with the <mask> token and the NER class of each word is the most probable NER class of the <mask> token prepended to it.\n\nExperiment\nThis section describes the NER dataset used for experiments and the quantitative results. Then, we investigate why MANER performs better for extreme low-resource languages using multiple qualitative measures.   Table 1.\n\nDataset, Models, and Baselines\nDataset: We use the WikiANN multilingual NER dataset (Pan et al., 2017;Rahimi et al., 2019), which provides named entities for Wikipedia articles across 176 languages. To the best of our knowledge, WikiANN is by far the most comprehensive dataset for multilingual NER. There are other multilingual datasets (e.g., CoNLL (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003)), but they cover only four popular languages: English, German, Dutch, and Spanish.\nOur main emphasis in this paper is on the 100 languages in WikiANN that each have only 100 samples for train and test splits. The NER labels in WikiANN are in IOB2 (Inside-outside-beginning) format (Ramshaw and Marcus, 1995)  WikiANN. XLM-RoBERTa model has been pretrained using the MLM objective on 2.5TB of filtered CommonCrawl data containing 100 languages (Conneau et al., 2020). We chose this model, since it is the largest publicly available multilingual transformer based model.\nFirst Baseline: Similar to MANER design, current NER systems built upon transformer models also simply add a NER classifier to top of a transformer model. The classifier predicts the NER class of each token of an unmodified sentence S: where M base is an NER model built on a trans-former model T using classifier weight matrix M. This baseline method remains the de-facto method for training NER models for most languages (especially low-resource languages) to the best of our knowledge, though specialized models have been built for popular languages like English.\nInference: Similar to training during inference, the NER class of each word in the sentence is the most probable NER tag assigned to the classified word embedding.\nSecond Baseline: Our MANER methodology in Section 2 is one way to change the input phrase using the mask token. In this second baseline, we introduce yet another way to repurpose the <mask> token for NER that is inspired by the masked language modeling (MLM) framework that is used for pre-training transformer models. In MLM, a word is predicted using the words surrounding it in the sentence. Since the NER category of a word is also a semantic property of the word, we use the philosophy of MLM for NER fine-tuning.\nIn MLM pre-training, the dataset is prepared by masking random words in a sentence with a <mask> token with a fixed probability p mlm . Then, the masked words are predicted using the context information.\nAnalogous to MLM pre-training, for NER finetuning, we randomly replace words in sentence S with the <mask> token with the fixed probability p ner . However, instead of predicting the missing words, as with MLM, we predict the NER labels L for each word w in S irrespective of whether the word was replaced by a <mask> token or not. In the case the word was replaced with the <mask> token, the transformer outputs the <mask> token embedding for that word.\n\nThus the modified input to the transformer is\nwith p i a random number between 0 and 1 generated for w i . Then, we use the first baseline NER model design M base for training, but now it is is fine-tuned on S and L (note we predict the label of the <mask> tokens as well). Inference with this model remains same as the first baseline model.\n\nQuantitative results and comparison of different strategies to repurpose <mask>\nWe train an XLM-RoBERTa-large model for 100 languages for 30 epochs with a learning rate of 5e −6 and the loss optimized using Adam (Loshchilov and Hutter, 2019). We report the average of F1 score for 100 languages in WikiANN that have 100 training samples each in Table 1.\nOur interpretation for why the second baseline does not improve over MANER (even though both methods use the <mask> token), is primarily because of two reasons, First, there is a difference between the train and inference setting in the second baseline. In the inference setting of second baseline, the <mask> token is not used to classify the token, unlike in MANER . In MANER , the model can learn to give more importance to the context in the case of out-of-distribution test labels using the <mask> token during inference and vice versa, explaining its significant 12% improvement as compared to both baselines.\nThe second reason is because the <mask> token introduces noise in the context during training of the second baseline model. Hence, it loses critical information that makes the NER task more difficult for the model. However, in MANER , the <mask> token is used in conjunction with the word to be labelled.\nIn Figure 2, we plot the F1 score of all of the 100 low-resource languages comparing the first baseline against MANER . One can see that there are only a few languages (12 out of 100) in which the first baseline outperforms MANER.\n\nImportance of the <mask> token\nWe now conduct experiments to demonstrate the importance of using the <mask> token in MANER. Intuitively the <mask> token can be helpful because it encodes the semantics of the context and, Figure 3: Measure effect of training samples to performance in MANER. MANER can give a boost in performance till 400 samples and then both MANER and first baseline NER model perform similarly. This demonstrates that MANER is best suited for extreme low resource languages and rapid prototyping since it is easy and costeffective to obtain very few human annotations to achieve large performance improvements (just 100 annotations are required). thereby, the word (by distributional hypothesis) that needs to be tagged.\nControl token: In the first experiment to gauge the importance of the <mask> token, we replace the token with a control/random token (<rand>) in MANER . Note that the <rand> token is not learned during the XLM-RoBERTa model pretraining; thus it will not encode any contextual information. As we see in Table 2, if we replace the <mask> token with <rand> in MANER, we achieve only a 6% improvement in F1 performance over the first baseline.  Why does <rand> token provide a 6% gain in F1? We believe that MANER provides a significant 6% boost in F1 score even without using the <mask> token because the model can use the <rand> token to predict how much weight to assign the context and the word immediately adjacent to it depending on the test sample.\nXLM-RoBERTa pre-training languages: In the second experiment to gauge the importance of the <mask> token, we report the F1 score in Table 3 averaging only on those languages that the XLM-RoBERTa model was pre-trained on with at least 0.5GB of training data per language. The rationale behind this experiment design is that the <mask> token will encode the context semantics of a language only if the language was seen during the pre-training stage of XLM-RoBERTa model.\nAs we see in Table 3, in this case, MANER provides a whooping 18% improvement in F1 score (as compared to the 12% gain in Table 1) if the language was seen in the pre-training stage. This experiment again highlights the importance of using <mask> token in MANER.\n\nEffect of training samples\nIn the previous subsections, we showed, by exploiting the context semantics encoded in the <mask> token, MANER can achieve significant gains over the baseline NER methodology for low-resource languages. In this section, we measure the effectiveness of MANER in situations where more training data is available. For this purpose, we select 4 languages from WikiANN dataset that have 1000 training data samples each. From Figure 3, we see that MANER boosts F1 performance over the the first baseline until about 400 samples and then both methods perform similarly. This demonstrates that MANER is best suited for extreme low resource languages and rapid prototyping since it is easy and cost-effective to obtain very few human annotations to achieve large performance improvements (just 100 annotations are required). The catastrophic forgetting (Kirkpatrick et al., 2017) phenomenon that masked language models undergo during any kind of fine-tuning is one of the reasons we think MANER does not provide gains when more training data is available (of course more training data also implies less reliance on specialized techniques like ours). Catastrophic forgetting causes the loss of useful context semantics encoded in the <mask> token during the fine-tuning stage that MANER heavily relies on. Adding an additional masked language modeling loss to the NER loss during fine-tuning can help to circumvent catastrophic forgetting, but its investigation is left for future work.\n\nConclusions\nIn this paper, we have proposed Mask Augmented Named Entity Recognition (MANER) for NER in extreme low-resource language settings. MANER exploits the information encoded in pre-trained masked language models (inside <mask> token specifically) and outperforms existing approaches for extreme low-resource languages with as few as only 100 training examples by up to 48% and by 12% on average on F1 score. Analyses and ablation studies show that use of semantics encoded in <mask> token is integral to MANER. Future work will target using MANER for rapid prototyping in an active learning setup since it offers a costeffective way to supplement human annotators."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 15350,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254434316"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The Contribution and Philosophical Development of the Reformational Philosopher Herman Dooyeweerd and His Conversation with Dirk Vollenhoven\n\n\nThis article builds on my previous article on Dirk H. Th. Vollenhoven (Ive 2015) and provides an overview of the development of the systematic philosophy of Herman Dooyeweerd. This article seeks to provide an overview of the key developments in the thinking of Dooyeweerd, both in the convergences arising from the conversation of the two brothers-in-law and long-term colleagues and in their divergences. Dooyeweerd and Vollenhoven worked within the tradition of Abraham Kuyper, the father of Reformational philosophy. The development of their respective philosophical systematics has tended to be addressed separately. However, I argue that the close cooperation between the two philosophers was critical in the formulation of the philosophy of the cosmonomic Idea. The philosophical conversation between the two persisted, despite critical divergences, largely aired privately. Their respective insights remained a source of mutual stimulus and complementarity foundational for Reformational philosophy."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-06"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 16487,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254670141"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Beyond the Greening of Faith: Contemplative Practice in the Anthropocene\n\nPart of a special section on Creation Care, this essay argues that Christian responses to the ecological crisis ought to move beyond a conversation organized around the demands of prevailing environmental philosophies before which religious tradition seeks to justify itself, and towards a more dialogical, theoretically rigorous, heuristic, and contemplatively transformative exploration of the way Christian communities might deploy their spiritual and intellectual traditions in order to participate in the continuing effort to construct an integral ecological theory, practice, and politics. Drawing on the contemporary ecological criticism of writers such as Amitav Ghosh, Jan Zwicky, and Robert Bringhurst, the essay proposes that the Christian contemplative practice of reading the book of nature (theoria physike) provides a powerful example of what such Christian contemplative formation might look like in the Anthropocene."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 17522,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254432449"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Exploring the Metaphysics of Hegel's Racism: The Teleology of the ‘Concept’ and the Taxonomy of Races\n\n\n This article interprets Hegel's hierarchical theory of race as an application of his general views about the metaphysics of classification and explanation. We begin by offering a reconstruction of Hegel's hierarchical theory of race based on the critical edition of relevant lecture transcripts: we argue that Hegel's position on race is appropriately classified as racist, that it postulates innate mental deficits of some races, and that it turns racism from an anthropological into a metaphysical doctrine by claiming that the division of humankind into races (at least in the Old World) is not a brute fact, but follows a ‘higher necessity’. We then summarize our interpretation of the relevant metaphysical background to this theory. On our reading, Hegel postulates an essentialist form of explanation that explains given kinds as stages in a teleological, non-temporal process through which the nature of a superordinate kind is realized. We argue that Hegel's views about a hierarchical and necessary division of humankind into races are an application of this model to the case of human diversity, motivated by explanatory considerations and subject to confirmation bias. By way of conclusion, we address two possible attempts to ‘save’ Hegelian philosophy from its racist baggage."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 18140,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254391444"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Teleology and Basic Actions: A reading of the chapter on Teleology in Hegel's Subjective Logic in the terms of action theory\n\n\n In this paper I argue that there is textual evidence that the chapter on Teleology in Hegel's Science of Logic, read under certain premises, also discusses something that in contemporary analytic philosophy is called a ‘basic action’. The three moments of Teleology—(a) ‘The Subjective Purpose’, (b) ‘The Means’ and (c) ‘The Realized Purpose’—can be interpreted as (a) a certain intentional content in the mind of a subject, which can be expressed in the form of an imperative, (b) the immediate taking in possession of the body, which can be described as a basic action, and (c) the description of the relation of the event brought about by the basic action with other events in the world, which can be described in the terms of event-causality. This reading reveals an astonishing parallel to Donald Davidson's distinction between proper basic actions and their different descriptions in the form of events. In this way we can make Hegel's, at first glance, confusing identification of subjective purpose (intention), means (basic action) and realized purpose (event) comprehensible. Through that, the actual aim is to show that what I call basic actions are in fact an example of a more general thought that Hegel calls a teleological relation."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-06"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 23282,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255335797"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Scripture and Philosophy in the Writings of Arius of Alexandria: A Brief Note\n\n\n This brief note seeks to cast light on the sources that Arius used in his writings. The first part focuses on Scripture and the second part on philosophical texts. Two passages will be analysed, the first part of the creedal letter to bishop Alexander of Alexandria, and one section of the hymnic Thalia."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-12"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 23795,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "253869015"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Sein und Kunst: Zum epistemischen Wert der Kunst bei Heidegger\n\nIn this essay, Heidegger's theses on art, as he develops them in the text \"On the Origin of the Work of Art,\" are reconstructed, interpreted, and critically evalua- ted. In doing so, we pursue a threefold goal. First, his theses on art are put in relation to the main theme of his philosophy:\n the question of being. Second, the different ways in which Heidegger takes art to be epistemically valuable are dif- ferentiated and reconstructed in detail. Third, Heidegger's theses are related to the contemporary debate on the epistemic value of art and critically discussed."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-15"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 28965,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254913063"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Response to Roundtable Discussion\n\n\nIn this text I will deal with three worries that were articulated by my three responders in astounding unanimity: firstly, I will defend the nation-based framework of my history of the idea of recognition on basis of the well-established hypothesis that there is path-dependency of later aesthetic or theoretical productions (1); secondly, I hope to be able to demonstrate that the various theories of subjectivity I investigate can in fact all be subsumed under the concept of “recognition” (2); and thirdly, I will try to clarify the motives I had for synthetizing the three different concepts of recognition in one single approach (3). In an extra step at the end of my article I explain why I don’t see myself making the mistake to assign philosophy the function of something amounting to a leading discipline in the history of ideas (4)."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 43875,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254716288"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Is Aesthetic Good? A Study on the Aesthetic and Vitality Judgment of Pictorial Representations of the Dead, Saints and Non-Saints\n\nIn the history of the Western world, there has always been an association between good and beautiful. Starting from a brief history of beauty, two questions arise: is beauty linked to good even in art? How important are people’s religious beliefs in aesthetic and vitality judgments? The psychology of art could answer these questions by studying people’s reactions to the images of Saints as testimonials of goodness. Moreover, the study of Saints’ paintings would allow us to investigate vitality, understood as one’s perception of a living being. The research aimed to investigate the aesthetic and vitality judgments of faces representing the dead, Saints and non-Saints. More than a hundred participants were asked to evaluate the aesthetics and vitality of these paintings; moreover, two tests assessing spirituality and religiosity were administered. Overall, these data suggest Saints were judged more beautiful than non-Saints, and non-Saints were judged more vital than Saints. This might suggest a relationship between ethics and aesthetics, also in the perception of art, and offers reflections on the theme of vitality. The religion and spirituality of participants are not correlated to aesthetic or vitality judgments; this fact could support that these judgments are linked to the basic bottom-up reactions to images.\n\n\nIntroduction\nThe stereotype \"what is beautiful is good\" has been advocated in different societies and eras in the Western world. Already in the Iliad, the gods and heroes are characterized by having a positive epithet. For example, Achilles is depicted as \"with beautiful hair\"; conversely, the anti-heroes are characterized by physical distortions, for example, Thersites (the anti-hero par excellence) is described, among other things, as lame in one foot [1]. These are probably primordial traces of kalokagathìa, an expression that would be born in the 5th century BC in Athens, which indicates an ideal of unity between visible physical beauty (kalòs) and (kai), and moral quality (agathòs) [2,3]. The concept of kalokagathia has spread in different fields, from philosophy, with the aesthetics of Plato, to art, with the use of the golden section in painting and sculpture. The Romans received the cultural heritage of the Greeks and this idea of harmony between mind and body continues to influence artistic production [2]. For example, the Roman statues, while differing from the Greek statues because of their greater realism, are still built using the golden section. Starting from late antiquity, the Greco-Roman culture was enslaved to Christianism: physical beauty, although not despised, is temporary and therefore is considered inferior to moral beauty; however, moral beauty is reflected through the body, and therefore beauty is still associated with virtue, and ugliness with vice [4,5]. For this reason, in the Middle Ages Behav. Sci. 2022, 12, 507 2 of 10 and the Renaissance, it is possible to find depictions of martyrs and Saints (for example Sant Irene) who are represented as virtuous and beautiful [2,6]. From the Enlightenment onwards, the theme of beauty is disconnected from the theme of religion and is regarded differently according to the field of reference; there is no longer a universal beauty, but there is a subjective beauty that depends on the observer and on the field from which it is investigated [2,4]. Thus, since the Enlightenment, we have passed through epochs exalting the idea of beauty linked to art, then to nature, and then to subjectivity; with the industrial revolution and the advent of the reproducibility of artworks, the theme of beauty is set aside [2,7]. In the Contemporary era, it seems that the two currents, that of the Greek-Roman tradition and that of the Enlightenment tradition, collide, and therefore these two questions arise: is beauty linked to good even in art? Equally, how important are people's beliefs in aesthetic attribution?\nPsychology, and especially social psychology, has investigated the relationship between beauty and good through numerous studies showing that people derive, on the one hand, moral inferences from physical beauty and, on the other hand, information relating to morality's influence upon aesthetic judgments [8][9][10]. However, of all this research, the psychology of art has so far provided no significant additions, although it could contribute to the debate with interesting studies. Research into Saints' paintings could offer fascinating insights. From the Middle Ages onwards, the paintings of the Saints were proposed to offer devotees models of beautiful and virtuous men [2,4], as can be seen in the representations of St. Sebastian (e.g., San Sebastiano of Botticelli) [11]. However, are Saints perceived as beautiful? There is still no research that has evaluated the aesthetics of the Saints, and yet, knowing how Saints are perceived from an aesthetic point of view would allow us to answer both the question of whether moral beauty is linked to physical beauty, and whether people's beliefs influence aesthetic evaluation. In brief, if Saints were perceived as aesthetically beautiful, especially by religious people, this fact, on the one hand, would support the relationship between beauty and good, and on the other hand, would confirm that religious beliefs influence aesthetic perception in religious art. The study of Saints' paintings would also allow us to investigate another little-explored theme in the field of the psychology of art: vitality. There is no precise definition of what vitality is in images, so it is very complex to deal with this theme; however, this concept could be understood as a force that resides within the image that allows the viewer to see the images as if they were alive, and that would depend on both the image itself and on the viewer. It is not a question of factuality, but it is about the reactions to the image and its perception. In the Western world, and particularly in the Catholic world, which encouraged visual art for religious purposes [12], there are various accounts of \"vital\" artworks, such as statues or paintings that cry or move their eyes [13,14]. In the \"Dialogus magnus visionum et miraculorum\" by Cesario di Heisterbach, it is possible to find hagiographic stories that also involve art and images. For example, there is a story of a portrait of St. Nicholas, a protector of pregnant women, who, placed in front of a woman in childbirth, turns towards the wall to avoid looking at the birth [13,15]. Interestingly, according to Freedberg [13,16], it is precisely the features of the face (especially the eyes) that give vitality to an image. According to psychology, the face provides fundamental information for interactions between humans [17], and also between humans and robots [18]. However, if it is true that the face is fundamental in everyday interactions, it also seems to be so in images, according to studies on iconoclasm [19][20][21][22]. Iconoclastic movements have often attacked the face, as if the image to be scarred were a real person, as happened, for example, to the painting \"Seven Works of Mercy\", by Master of Alkmaar, where the eyes were intentionally targeted. This artwork was damaged during the iconoclastic movements of 1566 when Protestants vandalized Catholic churches [21,22]. Therefore, the vitality of the images is expressed through the face, but from here at least three questions arise. Firstly, is there a difference between religious and secular images? The living images reported by the tales often refer to paintings depicting Saints [13], and therefore it would be interesting to verify if religious images have greater vitality than other types of images. Secondly, does vitality fail if the subjects are represented as dead and with their eyes closed? Practically, it would be interesting to understand whether the attribution of vitality to images could persist even in the absence of the strongest elements of vitality, the eyes [13]. Finally, what relationship, if any, exists between aesthetics, vitality, and beliefs? With these questions in mind, we aimed to investigate whether an image perceived as alive is also seen as beautiful (and vice versa, an image perceived as not very lively is seen as unattractive) [17], and if this relationship can be influenced by religiosity and/or spirituality. The present research aims to investigate the aesthetic evaluation and judgement of vitality in images of faces representing dead Saints and non-Saints. Based on the theoretical background introduced above, it was hypothesized that participants would attribute a higher aesthetic evaluation and a higher vitality judgment to images of dead Saints than dead non-Saints. It was also hypothesized that religious status and spirituality would have some influence on the aesthetic evaluation and the vitality judgment of the image; namely, Catholics and highly spiritual people would express a greater aesthetics and vitality judgment compared to non-Catholics and poor spiritual people. Our hypotheses have been only partially confirmed: the Saints are more beautiful but no more vital than the non-Saints, and there is no correlation between aesthetic and vitality judgments, and the religious and spiritual dimensions.\n\nParticipants\nThe study involved 114 Italian participants, aged between 18 and 52 years (M = 24.2; SD = 5.40), almost half females (54.4%). About 58% of participants had a high school diploma, and many (27.2%) had a bachelor's degree. Most of the participants (92.9%) did not possess specific artistic skills or knowledge; moreover, 60.5% declared that they visited museums or galleries twice a year and 21% did not frequent these places at all. Additionally, most of the participants reported being non-religious (42.1%), whereas 30.7% were religious, and 27.2% were uncertain about their faith; in addition, they were asked to indicate the religious context that they recognize as a cultural reference. By cross-referencing this information, a dichotomous variable was created, religious status. On the one hand, Catholics, that is, religious and uncertain people who use Catholicism as a reference, and, on the other hand, the Others, that is, non-religious, uncertain, and religious people who refer to a religion other than the Catholicism. Concerning religious status, participants were divided into two categories: Catholics (52.6%) and Others (47.4%), that is, people who did not recognize themselves in any religion or recognize a religion other than Catholicism.\n\nDesign and Procedure\nThrough the psychology department, using snowball sampling, an email was sent with the invitation and link to access the research. Participation in the study, which took approximately 15-20 min, was voluntary and participants gave their written consent. Demographic data, artistic competence, and religious status were collected first. Then, the participants were asked to assign an aesthetic and vitality evaluation to the images presented in a randomized order. They were subsequently asked to indicate whether the faces were of Saints or non-Saints, and to indicate how familiar they were with the images before the study. Finally, they were administered two tests: the Spirituality Assessment Scale [23] in the Italian version [24], and the Utrecht-Management of Identity Commitments Scale (U-MICS; [25]) to assess spirituality and religiosity, respectively.\n\nAesthetics and Vitality\nThe selection of the stimuli was carried out through online art galleries (Web Gallery of Art, Freeart-online art museum, Hermitage Museum-online collection). The images were included in the study if they met the following criteria: (a) the subjects are represented as dead; (b) the subjects show no signs that could immediately identify them as dead (e.g., wounds or dull skin); (c) the subjects have their eyes closed; (d) the artworks considered are dated between the early 1300s and early 1700s. From the initial selection that met these criteria (about 130 images), after discarding those images that were impossible to manipulate without noticeable distortions (e.g., removal of a large portion of the face to hide the nimbus) or impossible to isolate (e.g., individual portrayed in a crowd), the 8 images with the best sharpness and resolution were selected (4 of Saints and 4 of non-Saints). The final number of images presented was based on previous research [17], showing that a comparison of eight images divided into two categories was sufficient to detect any significant variations in beauty and vitality. The faces were cut out of the original painting using GIMP so that all contextual elements were removed, and the stimuli were made homogeneous in terms of resolution (300 × 300 px) ( Table 1).\n\nAesthetics and Vitality\nThe selection of the stimuli was carried out through online art galleries (Web Gallery of Art, Freeart-online art museum, Hermitage Museum-online collection). The images were included in the study if they met the following criteria: (a) the subjects are represented as dead; (b) the subjects show no signs that could immediately identify them as dead (e.g., wounds or dull skin); (c) the subjects have their eyes closed; (d) the artworks considered are dated between the early 1300s and early 1700s. From the initial selection that met these criteria (about 130 images), after discarding those images that were impossible to manipulate without noticeable distortions (e.g., removal of a large portion of the face to hide the nimbus) or impossible to isolate (e.g., individual portrayed in a crowd), the 8 images with the best sharpness and resolution were selected (4 of Saints and 4 of non-Saints). The final number of images presented was based on previous research [17], showing that a comparison of eight images divided into two categories was sufficient to detect any significant variations in beauty and vitality. The faces were cut out of the original painting using GIMP so that all contextual elements were removed, and the stimuli were made homogeneous in terms of resolution (300 × 300 px) ( Table 1).\n\nAesthetics and Vitality\nThe selection of the stimuli was carried out through online art galleries (Web Gallery of Art, Freeart-online art museum, Hermitage Museum-online collection). The images were included in the study if they met the following criteria: (a) the subjects are represented as dead; (b) the subjects show no signs that could immediately identify them as dead (e.g., wounds or dull skin); (c) the subjects have their eyes closed; (d) the artworks considered are dated between the early 1300s and early 1700s. From the initial selection that met these criteria (about 130 images), after discarding those images that were impossible to manipulate without noticeable distortions (e.g., removal of a large portion of the face to hide the nimbus) or impossible to isolate (e.g., individual portrayed in a crowd), the 8 images with the best sharpness and resolution were selected (4 of Saints and 4 of non-Saints). The final number of images presented was based on previous research [17], showing that a comparison of eight images divided into two categories was sufficient to detect any significant variations in beauty and vitality. The faces were cut out of the original painting using GIMP so that all contextual elements were removed, and the stimuli were made homogeneous in terms of resolution (300 × 300 px) ( Table 1). The selection of the stimuli was carried out through online art galleries (Web Gallery of Art, Freeart-online art museum, Hermitage Museum-online collection). The images were included in the study if they met the following criteria: (a) the subjects are represented as dead; (b) the subjects show no signs that could immediately identify them as dead (e.g., wounds or dull skin); (c) the subjects have their eyes closed; (d) the artworks considered are dated between the early 1300s and early 1700s. From the initial selection that met these criteria (about 130 images), after discarding those images that were impossible to manipulate without noticeable distortions (e.g., removal of a large portion of the face to hide the nimbus) or impossible to isolate (e.g., individual portrayed in a crowd), the 8 images with the best sharpness and resolution were selected (4 of Saints and 4 of non-Saints). The final number of images presented was based on previous research [17], showing that a comparison of eight images divided into two categories was sufficient to detect any significant variations in beauty and vitality. The faces were cut out of the original painting using GIMP so that all contextual elements were removed, and the stimuli were made homogeneous in terms of resolution (300 × 300 px) ( Table 1). The selection of the stimuli was carried out through online art galleries (Web Gallery of Art, Freeart-online art museum, Hermitage Museum-online collection). The images were included in the study if they met the following criteria: (a) the subjects are represented as dead; (b) the subjects show no signs that could immediately identify them as dead (e.g., wounds or dull skin); (c) the subjects have their eyes closed; (d) the artworks considered are dated between the early 1300s and early 1700s. From the initial selection that met these criteria (about 130 images), after discarding those images that were impossible to manipulate without noticeable distortions (e.g., removal of a large portion of the face to hide the nimbus) or impossible to isolate (e.g., individual portrayed in a crowd), the 8 images with the best sharpness and resolution were selected (4 of Saints and 4 of non-Saints). The final number of images presented was based on previous research [17], showing that a comparison of eight images divided into two categories was sufficient to detect any significant variations in beauty and vitality. The faces were cut out of the original painting using GIMP so that all contextual elements were removed, and the stimuli were made homogeneous in terms of resolution (300 × 300 px) ( Table 1). For each image, a pair of adjectives (ugly-beautiful; dead-alive) was presented and the participants were asked to indicate in each pair the position that best described their opinion on a 7-point scale to measure the aesthetic evaluation (1 = ugly; 7 = beautiful) and the judgment of vitality (1 = dead; 7 = alive).\n\nFamiliarity\nSeveral studies show that being familiar with a stimulus makes it more attractive, and this also happens for paintings and faces; if already known, they are perceived as more beautiful [26,27]. For this reason, the participants were re-proposed the same images and asked to indicate, on a 5-point scale (1 = not at all; 5 = a lot), how familiar they were with the artworks before the study.\n\nRecognition\nParticipants were asked to indicate whether the image presented was the face of a Saint or non-Saint. This information potentially makes it possible to link correct or incorrect recognition with religious status.\n\nSpirituality\nSpirituality can be defined as \"the human desire for transcendence, introspection, interconnectedness, and the quest for meaning in life\" [28] (p. 3). Spirituality was measured through the Italian version [24] of the Spirituality Assessment Scale [23]. This scale has 28 items, rated on a 6-point scale (1 = strongly disagree; 6 = strongly agree), and is For each image, a pair of adjectives (ugly-beautiful; dead-alive) was presented and the participants were asked to indicate in each pair the position that best described their opinion on a 7-point scale to measure the aesthetic evaluation (1 = ugly; 7 = beautiful) and the judgment of vitality (1 = dead; 7 = alive).\n\nFamiliarity\nSeveral studies show that being familiar with a stimulus makes it more attractive, and this also happens for paintings and faces; if already known, they are perceived as more beautiful [26,27]. For this reason, the participants were re-proposed the same images and asked to indicate, on a 5-point scale (1 = not at all; 5 = a lot), how familiar they were with the artworks before the study.\n\nRecognition\nParticipants were asked to indicate whether the image presented was the face of a Saint or non-Saint. This information potentially makes it possible to link correct or incorrect recognition with religious status.\n\nSpirituality\nSpirituality can be defined as \"the human desire for transcendence, introspection, interconnectedness, and the quest for meaning in life\" [28] (p. 3). Spirituality was measured through the Italian version [24] of the Spirituality Assessment Scale [23]. This scale has 28 items, rated on a 6-point scale (1 = strongly disagree; 6 = strongly agree), and is For each image, a pair of adjectives (ugly-beautiful; dead-alive) was presented and the participants were asked to indicate in each pair the position that best described their opinion on a 7-point scale to measure the aesthetic evaluation (1 = ugly; 7 = beautiful) and the judgment of vitality (1 = dead; 7 = alive).\n\nFamiliarity\nSeveral studies show that being familiar with a stimulus makes it more attractive, and this also happens for paintings and faces; if already known, they are perceived as more beautiful [26,27]. For this reason, the participants were re-proposed the same images and asked to indicate, on a 5-point scale (1 = not at all; 5 = a lot), how familiar they were with the artworks before the study.\n\nRecognition\nParticipants were asked to indicate whether the image presented was the face of a Saint or non-Saint. This information potentially makes it possible to link correct or incorrect recognition with religious status.\n\nSpirituality\nSpirituality can be defined as \"the human desire for transcendence, introspection, interconnectedness, and the quest for meaning in life\" [28] (p. 3). Spirituality was measured through the Italian version [24] of the Spirituality Assessment Scale [23]. This scale has 28 items, rated on a 6-point scale (1 = strongly disagree; 6 = strongly agree), and is For each image, a pair of adjectives (ugly-beautiful; dead-alive) was presented and the participants were asked to indicate in each pair the position that best described their opinion on a 7-point scale to measure the aesthetic evaluation (1 = ugly; 7 = beautiful) and the judgment of vitality (1 = dead; 7 = alive).\n\nFamiliarity\nSeveral studies show that being familiar with a stimulus makes it more attractive, and this also happens for paintings and faces; if already known, they are perceived as more beautiful [26,27]. For this reason, the participants were re-proposed the same images and asked to indicate, on a 5-point scale (1 = not at all; 5 = a lot), how familiar they were with the artworks before the study.\n\nRecognition\nParticipants were asked to indicate whether the image presented was the face of a Saint or non-Saint. This information potentially makes it possible to link correct or incorrect recognition with religious status.\n\nSpirituality\nSpirituality can be defined as \"the human desire for transcendence, introspection, interconnectedness, and the quest for meaning in life\" [28] (p. 3). Spirituality was measured through the Italian version [24] of the Spirituality Assessment Scale [23]. This scale has 28 items, rated on a 6-point scale (1 = strongly disagree; 6 = strongly agree), and is divided into four subscales that measure the different dimensions of spirituality: Scope (4 items), Interiority (9 items), Interconnection (9 items), and Transcendence (6 items).\n\nReligiosity\nReligiosity was investigated through the Italian-validated version of the Utrecht-Management of Identity Commitments Scale (U-MICS) [25] which measures three dimen-sions of religious identity: Commitment (5 items), In-depth exploration (5 items), and Reconsideration of the commitment (3 items). People make commitments related to their religious identity by adhering, for example, to a specific vision of reality, and then they can decide to strengthen their commitment or reduce it and explore other beliefs [24][25][26][27][28]. Items were measured on a 5-point scale (1 = completely false; 5 = completely true). This scale was only administered to participants who had defined themselves as religious or uncertain, since non-religious people could not answer questions related to their religion.\n\nExploratory Correlation Analyses\nPearson's correlation analyses were carried out to evaluate the relationship between the aesthetic and vitality judgment of Saints and non-Saints, and the participants' age, gender, level of education, attendance of museums or galleries, religious status, and selfreports of familiarity with the stimuli. The analysis showed a significant relationship (Bonferroni corrected for multiple comparisons, p < 0.01) between familiarity and aesthetic judgment, r = 0.25, and familiarity and vitality judgment, r = 0.27, for the dead Saints only. This effect was plausibly influenced by religious status which correlated with greater familiarity with the images of Saints, r = 0.23\n\nRecognition\nIn line with the results above, when asked to recognize the images by indicating if they portrayed a Saint or non-Saint, recognition indicated that participants recognized most images-except for two (one Saint and one non-Saint) above chance level, p < 0.05. The percentage of correct recognition is reported in Table 2. Contrary to the results above, this was independent of the participant's religious status.\n\nAesthetic and Vitality Judgment\nTo evaluate the differences in the aesthetic and vitality judgments between images of Saints and non-Saints, a GLM repeated measures analysis was carried out with two levels of task (aesthetic, vitality), and two levels of image category (saints, non-saints) as within-subjects factors. As familiarity positively correlated with the aesthetic and vitality judgments for Saints' portraits, familiarity was included in the model as a covariate. Post hoc comparisons were Bonferroni corrected. The results revealed a significant interaction between task and category (Figure 1), F(1, 112) =16.31, p < 0.001, partial-η2 = 0.13, δ = 0.98, indicating that Saints were judged more beautiful than non-Saints, Mdiff = 0.55; SE = 0.12, p < 0.001, and that non-Saints were judged more vital than Saints, Mdiff = 1.04; SE = 0.13, p < 0.01. No significant interactions were found between any of the factors and the covariate familiarity, p > 0.05.\nindicating that Saints were judged more beautiful than non-Saints, Mdiff = 0.55; SE= 0.12, p < 0.001, and that non-Saints were judged more vital than Saints, Mdiff = 1.04; SE = 0.13, p < 0.01. No significant interactions were found between any of the factors and the covariate familiarity, p > 0.05.\n\nSpirituality and Religiosity\nWe carried out Pearson's correlation analyses (Bonferroni corrected, p < 0.01) to evaluate if the religious and/or spiritual dimensions were associated with aesthetic and vitality judgments of the Saint and non-Saint images. As religious people may be quite familiar with religious art, thus affecting the aesthetic judgment of images of Saints, correlations were further carried out between familiarity with the stimulus and religiosity. No associations were found between the religious and/or the spiritual dimensions and the aesthetic and vitality judgments, p > 0.05. Similarly, no significant associations were found between religiosity and familiarity for both the Saints and non-Saints, p > 0.05.\n\nDiscussion\nThis study aimed to evaluate differences in the aesthetic and vitality judgments for images showing pictorial representations of Saint and non-Saint dead people. The effect of religiosity and spirituality on such judgments was also explored. The results showed that Saints were judged more beautiful than non-Saints, and that non-Saints were judged more vital than Saints, thus only partially supporting our hypothesis. These differences appear not to be related to either the participants' religious and/or spiritual inclinations or the contextual dimension (i.e., catholic vs. other religious or atheists). These results open several reflections.\nFirstly, the Saints were perceived as more beautiful than non-Saints. This result is not related to the individual's religiosity or spirituality, given the absence of a correlation between the religious/spiritual sphere and the aesthetic judgment of Saints. Furthermore, this result cannot be justified in terms of greater familiarity with the images, because in the analysis of the differences between the images of Saints and non-Saints, we took into consideration familiarity and no significant correlations emerged; moreover, no correlation emerged even between familiarity and religious dimension or context. What, then, could be a possible explanation for the aesthetic preference ascribed to Saints? Focusing on the intrinsic value of the religious image, a possible interpretation can be traced back\n\nSpirituality and Religiosity\nWe carried out Pearson's correlation analyses (Bonferroni corrected, p < 0.01) to evaluate if the religious and/or spiritual dimensions were associated with aesthetic and vitality judgments of the Saint and non-Saint images. As religious people may be quite familiar with religious art, thus affecting the aesthetic judgment of images of Saints, correlations were further carried out between familiarity with the stimulus and religiosity. No associations were found between the religious and/or the spiritual dimensions and the aesthetic and vitality judgments, p > 0.05. Similarly, no significant associations were found between religiosity and familiarity for both the Saints and non-Saints, p > 0.05.\n\nDiscussion\nThis study aimed to evaluate differences in the aesthetic and vitality judgments for images showing pictorial representations of Saint and non-Saint dead people. The effect of religiosity and spirituality on such judgments was also explored. The results showed that Saints were judged more beautiful than non-Saints, and that non-Saints were judged more vital than Saints, thus only partially supporting our hypothesis. These differences appear not to be related to either the participants' religious and/or spiritual inclinations or the contextual dimension (i.e., catholic vs. other religious or atheists). These results open several reflections.\nFirstly, the Saints were perceived as more beautiful than non-Saints. This result is not related to the individual's religiosity or spirituality, given the absence of a correlation between the religious/spiritual sphere and the aesthetic judgment of Saints. Furthermore, this result cannot be justified in terms of greater familiarity with the images, because in the analysis of the differences between the images of Saints and non-Saints, we took into consideration familiarity and no significant correlations emerged; moreover, no correlation emerged even between familiarity and religious dimension or context. What, then, could be a possible explanation for the aesthetic preference ascribed to Saints? Focusing on the intrinsic value of the religious image, a possible interpretation can be traced back to the connection between the beautiful and good, as described in the introduction: Saints are perceived as attractive due to a vague halo effect [29,30]. The halo effect is a cognitive bias in which the perception of a trait is influenced by the perception of one or more different characteristics of the person [31]. For example, physical attractiveness is linked to positive inferences about personality, and therefore a beautiful person can be perceived as good and a good person can be perceived as beautiful [32]. This is what happens in our case: Saints are recognized as such, that is, as virtuous people, and hence they are also considered beautiful.\nAdditionally, opposite to our initial predictions, the non-Saints were regarded as more vital than the Saints. A possible explanation could be based on intrinsic pictorial differences between the two groups of stimuli (Saints and non-Saints). However, we would tend to exclude this hypothesis because, if we consider the styles, the historical period of creation, and the painting definition that made clear the signs of death vitality (e.g., closed eyes, yellowish skin, etc.), the two groups of stimuli tend to balance. However, it cannot be ruled out that vitality depends on the intentionality of the painters. In an interpretative key, therefore, turning the perspective, we can say that Saints are less vital than non-Saints. We could argue that the Saints are perceived as more spiritual and distant from everyday life, and therefore less vital if one conceives vitality as a corporeal and concrete quality. The problem here lies precisely in the lack of a shared definition of vitality, and a solution must be found to continue the studies of this concept from a psychological point of view. To refine a definition, it might be interesting to do qualitative and quantitative research and, for example, ask people what makes a painting vital to them and if there is any correspondence between vitality and the reactions and perceptions of the paintings. Moreover, it should be investigated whether this form of vitality is different from the vitality investigated by neuroscience [33].\nAnother significant result is that the religious status and the spirituality of the participants do not correlate with either the judgment of aesthetics and vitality, or the correct recognition of the images. This fact could support the idea that there is always a basic level of reaction to images that is independent of the cultural context [12,13], and therefore the judgments of vitality and aesthetics are linked to the basic bottom-up reactions to images and are not significantly influenced by top-down socio-cultural elements, such as religiosity and spirituality.\n\nConclusions\nThe results of this study show a relationship between the beautiful and good in the perception of religious art images. As reported in the scientific literature, e.g. [26,32], not only do aesthetics influence moral perception, that is, whoever is beautiful is also seen as good, but the opposite is also true, that whoever is perceived as an ethically correct person is also seen as more beautiful. Moreover, it is interesting to note that spirituality and religiosity, which we assumed before the study to be two relevant variables in aesthetic and vitality judgments, actually have no bearing in this regard. Therefore, it would be interesting to carry out this research in countries where the Catholic religion is more strongly felt, to investigate whether these variables are irrelevant. In addition, it would be interesting to have two groups, one Catholic and one Protestant, for example, to see if there is any change in the perception of religious images, given that these two religious currents have very strong differences in the acceptance and the use of religious images [12]; in this case, it would be possible to better understand the respective weight of the top-down and bottom-up variables. A possible future extension of this area of research could involve the application of this research design to the study of imagery in Byzantine art, which could be particularly interesting because it involves not only the topics of vitality and holiness, but also the theme of iconoclasm [13,20]. Moreover, it could be very useful to have a comparison between art experts and non-art experts to highlight possible socio-cultural influences on the perception of these images. Lastly, for reruns or future extensions of this research, the inclination of the head in the paintings should be controlled to understand if this variable could have any influence on the recognition of the Saints. In fact, in religious figures, including the Virgin and Saints, the canting of the head is a sign of submission and devotion. For example, nobles in images have slightly less inclination than the Saints [34].\nIn conclusion, this work offers an important initial starting point for investigating, outside of the religious domain, the intrinsic nature between the good and the beautiful, and therefore between ethics and aesthetics; this is a theme that has already been extensively dealt with over the centuries in the philosophical sphere, but that could also be further explored in the psychological field. Finally, this study is vital and pioneering, in that it opens up several research questions that will require adequate and in-depth studies. Institutional Review Board Statement: The study was conducted following the Declaration of Helsinki and approved by the Ethics Committee of Università Cattolica del Sacro Cuore di Milano (2020).\n\nInformed Consent Statement:\nInformed consent was obtained from all subjects involved in the study.\nData Availability Statement: All data needed to evaluate the conclusions in the article are present in the article.\n\nConflicts of Interest:\nThe authors declare no conflict of interest."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 44356,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255185291"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Pathologizing Ugliness: A Conceptual Analysis of the Naturalist and Normativist Claims in “Aesthetic Pathology”\n\nAbstract Pathologizing ugliness refers to the use of disease language and medical processes to foster and support the claim that undesirable features are pathological conditions requiring medical or surgical intervention. Primarily situated in cosmetic surgery, the practice appeals to the concept of “aesthetic pathology”, which is a medical designation for features that deviate from some designated aesthetic norms. This article offers a two-pronged conceptual analysis of aesthetic pathology. First, I argue that three sets of claims, derived from normativist and naturalistic accounts of disease, inform the framing of ugliness as a disease. These claims concern: (1) aesthetic harms, (2) aesthetic dysfunction, and (3) aesthetic deviation. Second, I introduce the notion of a hybridization loop in medicine, which merges the naturalist and normative understanding of the disease that potentially enables pathologizing practices. In the context of cosmetic surgery, the loop simultaneously promotes the framing of beauty ideals as normal biological attributes and the framing of normal appearance as an aesthetic ideal to legitimize the need for cosmetic interventions. The article thus offers an original discussion of the conceptual problems arising from a specific practice in cosmetic surgery that depicts ugliness as the disease.\n\n\nI. INTRODUCTION\nScholarly work in the philosophy of medicine and bioethics shows the theoretical and practical value of defining the boundaries of health and disease. Defining health concepts assists in determining the scope of medicine, identifying legitimate medical practices (e.g., mainstream versus alternative medicine), or determining the just distribution of medical care (Kingma, 2017). In various clinical practices, for example, definitions of disease guide the decision-making of clinicians, from the use of diagnostic tests to the application of appropriate medical interventions. Furthermore, a philosophical project of defining conceptual boundaries in medicine facilitates clarification of the experience of what some scholars refer to as \"illness\" or \"sickness\", which denotes social and individual perceptions of a pathological condition (Carel and Cooper, 2014).\nDefining boundaries of medical concepts remains problematic, as demonstrated by the ongoing disputes about the ways two define the disease. The current debate on the meaning of disease generally involves two competing positions referred to as naturalism and normativism, which vary depending on the view of whether health concepts are based on facts about human biology or on evaluative judgments about well-being, respectively (Ereshefsky, 2009;Kingma, 2014). Other authors defend the view that health concepts are neither wholly naturalistic nor wholly normativistic (Wakefield, 1992;Cooper, 2002). For example, Wakefield (1992) argues that disease is a combination of naturalist notions of dysfunction and normative notions of harm or deprivation.\nAnother challenge to the seemingly fixed notion of disease is the phenomenon of medical expansion (Carel and Cooper, 2014). Sociologists of medicine, such as Conrad (1992Conrad ( , 2013, refer to the notion of medicalization, which can be understood as bringing conditions previously considered non-medical within the purview of medicine. One way of medicalizing a condition is by defining it as pathological and thus requiring medical treatment. The practice of pathologizing ugliness 1 is one example of a specific type of medicalization that blurs the boundaries of health and disease. Pathologizing ugliness refers to the use of disease language and medical processes to foster and support the claim that undesirable features are pathological conditions requiring medical or surgical intervention. Primarily situated in cosmetic surgery, the practice appeals to the concept of \"aesthetic pathology\", which is a medical designation for features that deviate from some designated aesthetic norms (Dolezal, 2010). 2 Despite the potential conceptual challenges raised by aesthetic pathology, discussions in philosophy of medicine have largely relegated ugliness concerns outside the realm of health.\nOne possible reason for the neglect of analyzing ugliness is that philosophers of medicine generally regard concern with beauty to be outside the realm of health (Boorse, 1976;Hamilton, 2010;Hofmann, 2019). Some authors appear to assert that the debate is settled, describing aesthetic concerns about the body as \"aspects of human life which have significant impacts upon its quality but which no sensible theory would call diseases\" (Hamilton, 2010, 324). Consequently, interventions for aesthetic concerns are generally considered as enhancements and not therapy. For other authors, medical and surgical enhancements of beauty raise conceptual (and ethical) issues that are the exception rather than the rule in medicine. Boorse (2014); for example, argues that cosmetic surgerywherein aesthetic concerns are primarily managed-remains a peripheral, albeit generally accepted, medical practice. An implication of this exceptionalist view is that conceptual problems arising from cosmetic surgery are thought to be inconsequential to general debates regarding accounts of health and disease. However, as I demonstrate in this article, the issues raised by pathologizing ugliness merit serious examination. Specifically, I offer a conceptual analysis of \"aesthetic pathology\", which I argue is the central concept in cosmetic surgery's practice of pathologizing ugliness.\nThe two-pronged conceptual analysis aims to capture the underlying naturalist and normative claims that underpin aesthetic pathology, as well as its significance to the conceptual boundaries of disease in general. Traditionally, normative accounts, not naturalist accounts, are subject to the charge that aesthetic concerns muddle disease definitions. In Section II, I elucidate on the passive and active manners in which the practice of pathologizing ugliness occurs in cosmetic surgery. Next, the first part of the conceptual analysis in Sections III-V describes three sets of claims that inform aesthetic pathology, namely, (1) aesthetic harms, (2) aesthetic dysfunction, and (3) aesthetic deviation, respectively. Finally, the second part of the conceptual analysis in Section VI introduces the phenomenon of the hybridization loop in medicine that offers a fertile ground for the reification of aesthetic pathology as a medical concept. The loop involves the twin processes of naturalizing aesthetic ideals and normativising statistically determined appearance norms. The loop helps to explain the ways in which aesthetic pathology complicates the distinction between the ideal and the normal, as well as the normal and the pathological.\n\nII. THE PR ACTICE OF PATHOLOGISING UGLINESS\nIn some ways, the pathologization of ugliness is a passive process that results from the ongoing conflation of health and beauty norms in medical specialties that manage appearance concerns (e.g., plastic surgery). In this section, I demonstrate the ways in which the conflation of looking healthy and looking beautiful inform the practice of pathologizing ugliness in the cosmetic surgery. By appealing to physical appearance as a reflection of health, there is a tendency for ideas about health and beauty to overlap. If we consider the lay interpretation that a healthy state is the absence of disease, then a healthy appearance can be understood as the absence of physical signs of disease, disorder, or deformity. There is a crude intuition that signs of disease are viewed in terms of physical flaws or imperfections, which deviate from accepted or prevalent norms of appearance, such as jaundice or pallor. In turn, a beautiful appearance is generally associated with physical features that are thought to be signs of good health, such as glowing skin. Admittedly, there are numerous cases where a healthyor disease-free-appearance is deemed by society as neither beautiful nor ugly. Davis (1997), for one, argues that ordinary, not necessarily beautiful, appearance can be associated with well-being. Conversely, there are cases where beauty ideals conflict with physical and mental health. For example, a wide range of research has explored thinness among women as an unhealthy beauty ideal that encourages extreme starvation, leading to both mental and physical disorders (Sengupta, 2006). Here, I focus on the cases that perpetuate the conflation of health and beauty, particularly those that take apparent signs of disease as undesirable imperfections and, in turn, beautiful appearance as devoid of signs of disease.\nThe notion that a healthy appearance overlaps with a beautiful one is illustrated by the way people deal with skin problems. Several studies claim that skin characteristics, such as color distribution and texture, are cues for \"attractiveness, healthiness, and age\" (Fink et al., 2008, 155). In particular, Fink and colleagues claim that skin color distribution which is even (that is, blemish-free) is considered healthy and beautiful, postulating that this skin characteristic can to some extent signal reproductive quality (2008,156). The notion that \"healthy skin is beautiful skin\" is pervasive not only in soap commercials but also in the marketing materials of cosmetic surgery clinics (Ringel, 1998, 428). Moreover, Ringel argues that marketing strategies that depict healthy skin as beautiful tends to imply that the converse is true, that skin imperfections are viewed simultaneously in terms of \"disease and ugliness\" (1998,428). Thus, visible skin characteristics are an example of a physical feature that signifies overlapping judgments regarding health and beauty.\nThe pathologization of ugliness becomes an active practice when cosmetic surgery explicitly reframes aesthetic concerns as a type of disease or deformity. 3 In her book, The Beauty Myth, Naomi Wolf argues that pathological framing of aesthetic concerns legitimized the market for cosmetic \"remedies\" offered by surgeons (1991,227). According to Wolf, the American dermatologist Dr. Arthur K. Balin suggested in 1978 that for marketing purposes, \"it would benefit physicians to look upon ugliness not as a cosmetic issue but a disease\" (1991,227). A specific strategy in the pathologization of ugliness is to appropriate definitions from the social model of disability by framing perceived ugliness as a handicap because unattractive features are psychologically disabling, or are associated with decreased socio-economic opportunities (Elliott, 2003b;Edmonds, 2007). Such framing echoes the language of limitations that is current in definitions of disability. For example, the Disabled People's International (DPI) defines disability as the loss or limitation of opportunities to take part in the normal life of the community on an equal level with others due to physical and social barriers (Oliver, 2017). On this account, the practice of pathologizing frames unattractive features as a form of disability. Scholars argue that the practice of treating ugliness as a disease is morally dubious, as the problem lies not with the individual but with suspect, albeit socially sanctioned, norms of beauty (Little, 1998;Elliott, 2011). Other scholars, such as Minerva (2017), assert that psychological distress and social stigma are reasonable justifications for regarding some unattractive features as pathological.\nIn his ethnographic study, Edmonds (2010) offers the cosmetic surgery industry in Brazil as an example of the ways in which unattractive features are pathologized. He found that local cosmetic surgeons enable the symptomatization of aesthetic problems by allowing \"the aged, the abandoned, the unemployed, the nonwhite, the unloved, to name their aesthetic complaint as 'aesthetic defect'\" (Edmonds, 2010, 114; emphasis in the original).\nThe pathologization of ugliness appears to be validated by state policies that allow government funding of cosmetic procedures. In the United Kingdom, for example, a woman can undergo government-funded breast augmentation as long as a qualified psychologist deems the surgery necessary for her well-being (Blum, 2003). In the Netherlands, various cosmetic procedures can be covered by the national health insurance based on a broad understanding of necessity (Davis, 2003). According to Kathy Davis, a sagging abdomen that makes a woman \"look pregnant\" qualifies as a necessary indication for abdominal surgery (\"tummy tuck\") that can be covered by the national health insurance (2003,63).\nIn the clinical context, the practice of pathologizing ugliness is exemplified by the term \"aesthetic pathology\", a medical designation for features that deviate from some aesthetic norms (Dolezal, 2010, 368). In general, pathologization in clinical practice involves the use of disease-sounding terminology 738 • Yves Saint James Aquino and medical jargon to refer to undesired states. Medical jargon refers to established medical terms associated with existing pathological conditions. The use of this language fosters an image that these conditions involve impairments based on the scientific knowledge and medical expertise, and that these conditions are subject to clinical management. Aesthetic pathology, in particular, has been used by surgeons as a term distinct from functional pathology (Trenité, 2007). While the latter refers to deformities that result in and/or from physiological dysfunction (such as difficulties with vision, hearing, sight, mastication, breathing, or speech), the former refers to physical features that are free from any type of physiological dysfunction but are deemed undesirable. For example, Mastering Revision Rhinoplasty, a plastic surgery textbook, contains sections on surgical anatomy, aesthetics, and aesthetic pathology covering every part of the nose (Sachs, 2006). Critics claim, however, that aesthetic pathology is a medical designation based solely on social symptoms, \"but which, doctors argue, causes psychological stress and distress that can be 'cured' through the use of cosmetic surgery …\" (Dolezal, 2010, 368). Hence, aesthetic pathology treads the line between an allegedly legitimate clinical concept that is used in the surgical practices, and a concept deemed hollow by critics.\nIn the following three sections, I explore the three sets of claims that underpin the practice of pathologizing ugliness embodied in the concept of aesthetic pathology, namely, normativist claims of aesthetic harm, as well as naturalist claims of aesthetic dysfunction and aesthetic deviation.\n\nIII. UGLINESS A S AESTHETIC HAR M\nOne of the most common claims regarding the practice of pathologizing ugliness invokes normative or evaluative notions of well-being and psychosocial harm to underpin the concept of aesthetic pathology. I refer to these claims as aesthetic harms, which are commonly expressed in terms of psycho-social suffering or disadvantages experienced by individuals with features deemed unattractive.\nTheorists such as Wakefield (1992) and Cooper (2002) begin with a naturalist notion of function or dysfunction before adding an evaluative element to their definitions of health. Wakefield's (1992) \"harmful dysfunction analysis (hda)\" states that the presence of dysfunction alone is not sufficient to claim that a condition or state is a disorder (his term for disease), and that an additional value judgment about harmful effects to the individual is necessary for defining disease. For Wakefield, disease refers to dysfunctions that cause \"some harm or deprivation of benefit to the person as judged by the standards of the person's culture\" (1992,384). More recently, Wakefield (2014) claims that harm is understood as a negative impact on an individual's life expectancy or reproductive success (which is not dissimilar to Boorse's definition of dysfunction). Goosens (1980) and Cooper (2005) assert that the concept of harm refers to consequences that pose a threat to well-being. In her account of disease, Cooper argues that one of the criteria in defining a condition as a disease is that the condition must be \"a bad thing to have\" (2005,41), which she explains in terms of being harmful to both the person and the society. She argues that harm can be based on an individual's self-assessment, on judgments by the community, or on objective criteria determined by medical authorities. Cooper claims that pathological conditions are judged as such because they often cause pain and suffering, and are disfiguring or disabling. Some authors define harmful consequences (hence, disease-making features) of dysfunctions in terms of death, pain, disability, loss of freedom, or loss of pleasure. Assessments of the harmfulness of these consequences (i.e., disvaluations) are based to a greater or lesser extent on social considerations (Clouser, Culver, and Gert, 1997). Increasingly, such notions of harm are seeping into aesthetic evaluations of appearance in medicine.\nBased on the current literature on beauty, social norms promote valuation, and pursuit of the beauty ideal by referring to phenomena such as the \"halo effect\" and \"beauty premium\". According to the social psychologists, the halo effect of beauty is a cognitive bias that ascribes several positive behavioral and intellectual attributes to individuals who possess physical beauty (Gupta, Etcoff, and Jaeger, 2016). The halo effect echoes the old trope that what is beautiful is good and therefore holy, with the holiness often symbolized by the presence of a halo. Empirical studies show that an attractive person is assumed to be good in the sense that they are deemed more confident, more honest, and more socially skilled than a less attractive person (Gross and Crofton, 1977;Gupta, Etcoff, and Jaeger, 2016). For example, a study by Mobius and Rosenblat (2006) evaluated the tendency of employers and employees to respond to physical attractiveness in their work environment. Their findings show that visual stereotypes (concerning attractiveness) raise both workers' and employers' productivity estimates without necessarily reflecting the actual productivity of the person deemed attractive. Based on the appearance alone, attractive workers are thought to be confident and trustworthy and more able than unattractive workers.\nSeveral authors argue that the halo effect tends to become translated into a beauty premium, which refers to the material benefits enjoyed by physically attractive individuals. Attractive individuals have better educational outcomes, higher-status jobs, and higher wages and are more likely to marry (Benzeval, Green, and Macintyre, 2013). In Mobius and Rosenblat's (2006) study, the positive perception that attractive employees tend to be more capable than unattractive employees had an impact on the employees' salaries and opportunities for promotion. Other authors investigated the long-term impact of physical attractiveness on an individual's economic success. One study found that people assessed as more physically attractive at the age of 15 had greater socioeconomic positions at the age of 36, in the terms of their employment status, housing tenure, and income (Benzeval, Green, and Macintyre, 2013). In addition, the study showed that the beauty premium contributes to the increased wages for attractive workers.\nThe consequences of the halo effect and beauty premium appear to give credence to the claim that beauty impacts a person's well-being. \"One would have to assume that attractive people are happier than other people. Afforded so many social and economic advantages, they must be happier,\" argued Gupta andcolleagues (2016, 1314). The potential impact of beauty on a person's happiness helps to explain the view that beauty, like health, is a component of well-being. This is evidenced by the national surveys in the United States, which suggest that, over time, socially valued interactions arising from physical attractiveness tend to promote not only socio-economic mobility but also well-being for attractive individuals (Umberson and Hughes, 1987). Gupta and colleagues (2016) claim that based on the empirical studies, physical attractiveness is associated with a statistically significant influence on self-reported well-being and depression or distress. The researchers measured well-being based on the self-reported factors that include personal growth, self-acceptance, and positive relationships with others. These studies demonstrate that beauty confers advantages that impact a person's well-being, as well as other normativist goals such as happiness and human flourishing. Consequently, features that fall short of the beauty ideal are associated with failure to maintain well-being or achieve other normativist goals, to the extent that such features are framed as pathological.\nEarlier efforts of some surgeons to pathologize ugliness in the first half of the twentieth century relied on the psychological concept of \"inferiority complex\" (Haiken, 1997;Elliott, 2003a;Heyes and Jones, 2009). Popularized by Austrian psychologist Alfred Adler in the 1910s, the inferiority complex was thought of as a psychopathological syndrome that allegedly involves a persistent sense of insecurity or low self-esteem (Heyes and Jones, 2009). Haiken (1997) argues that the inferiority complex provided a medical basis for surgeons to demonstrate the legitimacy of providing surgical interventions for aesthetic concerns. For example, throughout the 1930s, \"prominent nasal tips\", \"pendulous breasts\", \"prominent ears\", and \"lines and wrinkles about the eyes, jowls and neck\" were considered deformities that could cause an inferiority complex, and thus required surgery (Haiken 1997, 122). More recently, justifications for weight-loss surgery are increasingly framed in terms of relief from psychological suffering caused by being overweight. According to Schermer (2008), surgeries that relieve suffering tend to be framed as treatments rather than enhancements. In the absence of abnormalities apart from elevated body mass index, being overweight can be considered as another example of a pathologized aesthetic concern that can be modified through the bariatric surgery.\nIn addition, the aesthetic harms that underpin the practice of pathologizing ugliness structure psychosocial disadvantages as akin to disabilities. As I discussed in Section II, cosmetic surgeons describe ugliness in terms of disadvantages that result in decreased opportunities or limited access to socio-economic benefits (Edmonds, 2007), echoing the language of decreased opportunities and limited benefits that occurs in definitions of disability. Furthermore, the features deemed unattractive are increasingly becoming subject to modification in the same way that visible markers of disability have long been subject to medical or surgical interventions. Thus, the practice of pathologizing ugliness exploits the notion of aesthetic harm to demonstrate that unattractiveness can be classified as a disease in a normativist account of disease.\n\nIV. UGLINESS A S AESTHETIC DYSFUNCTION\nIn tandem with a normativist approach described earlier, cosmetic surgery's practice of pathologizing ugliness has increasingly appealed to some naturalist components of disease. These naturalist components, albeit presented unsystematically in marketing materials and empirical studies that investigate interactions between surgeons and clients (see Kaw, 1993;Blum, 2003;Edmonds, 2007Edmonds, , 2010Aquino, 2017), appear to involve two general claims involving what can be referred to as \"aesthetic dysfunction\" and \"aesthetic deviation\". In this section, I focus on the ways in which the naturalist concept of dysfunction facilitates attempts to justify cosmetic interventions as therapeutic in nature.\nNaturalist accounts of disease (and some hybrid accounts) revolve around the core concept of dysfunction, which naturalists claim is value-free. Boorse's notion of normal function appeals to conform to \"species design\" (1977,555). Boorse describes species design as the typical hierarchy of interlocking functional systems that contribute to the life of organisms. In Boorse's view, the biological description of normal function simply refers to the function's contribution to the goal of biological fitness (Ereshefsky, 2009). The Boorsean understanding of function is uncontroversial in the most cases of human traits, such as in the explanation of the circulatory function of the heart. However, the biological accounts of beauty complicate matters by framing aesthetic appearance as a human trait that serves a biological function by facilitating either survival or reproduction. In turn, these accounts suggest that physical appearance that fails to facilitate survival or reproduction can be viewed as dysfunctional.\nIn the context of cosmetic surgery, beauty and ugliness are increasingly described in terms of aesthetic function and dysfunction akin to other biological traits. In the cosmetic surgery literature, to state the aesthetic function of a physical feature is to describe its role in presenting \"a normal … appearance\", which can be interpreted as disease-free appearance (Bluestone, 2003, 974). In other cases, surgeons describe the aesthetic function of a feature by identifying its role in enhancing other parts of the body. For example, the alleged aesthetic function of the mandible is simply to define the projection of the lower third of the face (Chim et al., 2010). In another case, Sydney-based plastic surgeon Dr. Warwick Nettle claims that \"... the aesthetic function of the nose is actually to enhance the eyes\", and consequently noses \"that are twisted or noticeably disfigured\" are aesthetically dysfunctional (Australian Cosmetic Surgery Center, 2011, 80). 4 One can concede that the definitional issue of the Boorsean function becomes muddled if we consider human beings as social animals for whom the biological goals of survival and reproduction are complex phenomena (Kingma, 2014). Unlike many other animals, survival and reproduction in the human beings cannot be reduced to simpler concepts of adaptability to environment, access to food and shelter, defense against predators, or presence of the potential mates. As social beings, human survival and reproduction are influenced by various social, economic, political, and cultural factors. For instance, achieving the goal of reproduction in human beings depends on more than having functional reproductive organs. Human reproduction largely relies on the ability of an individual to attract potential and consenting sexual partner/s-an activity that generally requires being involved in favorable social interactions. 5 Therefore, human skills and attributes involved in social interactions can be postulated as functions or traits necessary to achieve the goal of reproduction. Admittedly, not all the types of social interactions are motivated by the goal of reproducing. However, the notion that social interaction is necessary for human reproduction implies that any human trait, such as having a beautiful appearance, might be viewed as a biological function. Ironically, the Boorsean definition of biological function as species-typical contribution to survival or reproduction thus broadens the scope of what it means for a function to be (exclusively) biological. For human beings, having an attractive physical appearance is more likely to facilitate the goal of reproduction, because that goal is socially mediated through processes that admit more rather than less attractiveness. In turn, unattractiveness can be considered an impairment in one of the functions of appearance, in that it reduces the likelihood of favorable social interaction leading to reproduction.\nMoreover, several studies show that the role of attractive appearance described in human reproduction is also relevant regarding human survival (Borah and Rankin, 2010). Social interaction is a necessary component of various aforementioned socio-economic factors that play a part in an individual's survival, as demonstrated by the phenomena of beauty premium discussed earlier. Moreover, the vagueness of the notion of \"survival\" implies that the Boorsean notion of goal-directedness does not effectively restrict functions to the purely biological. Goal-directedness can be used to identify aesthetic aspects of human appearance that either contribute to or impair an individual's survival. Boorse (1977) claims that disease, as impairment of health, does not require the impairment to be fatal. Impairment only needs to make an individual marginally less likely to survive or reproduce. Even a case like vitiligo, which is a non-life-threatening dermatological condition, is considered a disease because it marginally impairs a person's survival by increasing their chance of skin cancer. Based on the Boorse's account, unattractiveness can be understood as appearance that impairs a person's ability to survive or reproduce. In the same way that health depends on normal functioning that operates on average frequency and contributes to the goal of survival or reproduction, attractiveness can be described in the terms of functional contribution to the same goals. Subsequently, as, on the naturalist accounts, disease is subnormal performance of function that impairs health, it is possible to construe ugliness as subnormal performance of the function of attraction, which impairs a person's ability to survive or procreate. The complexity of human \"survival\" gives room for attractiveness to be considered as a biological function. Thus, cosmetic surgery's practice of pathologizing ugliness rehashes naturalist notions of function and dysfunction to demonstrate that unattractive features can be framed as pathological.\n\nV. UGLINESS A S STATISTICAL AESTHETIC DEVIATION\nThere is a second way in which naturalist accounts are exploited in the practice of pathologizing ugliness, which is through the concept of aesthetic deviation. Boorse's (1977) naturalist account in particular relies on the notion of normality based on the statistical frequency relative to an individual's sex and age. For Boorse, normality is statistically derived and relies on a notion that efficient or healthy performance is that which is common or frequent. In defining health as the statistically typical performance of a function that contributes to the aforementioned biological goals, Boorse's account relies on the notion that the function of the biological parts and process can be measured against the relevant statistical average, and thus be evaluated as healthy or not. Appeals to statistical average are fairly common for evaluation of appearance without reference to beauty or ugliness. Reconstructive surgeries of deformities caused by injuries or diseases aim to restore appearance based on statistically determined measurements of the features in question. Unsurprisingly, the same measurements are applied in the cosmetic surgeries that aim for aesthetic improvement of appearance.\nIn the cosmetic surgery, aesthetic deviation is commonly understood as a naturalistic, and allegedly objective, way of describing aesthetic pathology as a disease condition. This approach reduces physical unattractiveness \"to measurable characteristics, such as blood pressure, weight and body mass index...\" (Sailors, Teetzel, and Weaving, 2016, 62). Use of naturalistic language in describing features that deviate from accepted beauty standards provides a veneer of scientific objectivity (Merianos, Vidourek, and King, 2013). Aesthetic deviation as a naturalist dimension of aesthetic pathology is demonstrated in the quantitative descriptions of facial features that depart from ideal appearance as judged by surgeons. For example, the surgeons refer to the neo-classical facial canon as basis for the aesthetic norms that guide surgical modification of facial features (Leong and White, 2006).\nThe facial canon is allegedly based on the objective measurements drawn from proportions espoused by classical artistic traditions during the Italian Renaissance. In particular, surgeons allude to Leonardo da Vinci, who created standards for measuring ideal human bodily proportions in the figure of the Vitruvian man (Naini, Moss, and Gill, 2006). Da Vinci intended for such measurements to serve as guides for the artistic works, such as painting and sculpture. These measurements partly inspired the use of bodily, specifically facial, proportions outside the artistic realm in the sixteenth century practice of physiognomy that attempted to use objective measurements to define facial beauty (Ghigi, 2009). In the latter part of the eighteenth century, Swiss thinker Kaspar Lavater made further claims about the objective measurement of beauty by dividing the face into units (Ghigi, 2009). These historical examples of constructing beauty through numerical values inform cosmetic surgery's ongoing attempts to frame unattractiveness as deviation from allegedly objective aesthetic norms.\n\n• Yves Saint James Aquino\nThe modern facial canon offers a specific range of numerical values when describing facial features. For example, the naso-frontal angle should be typically within 115 to 130 degrees, while the nasal width-length ratio, calculated as the alar width divided by the length of the nose, is 0.7 (Ghigi, 2009). In cosmetic surgery, these measurements are used as reference when modifying facial features. Another version of the classical facial canon is best exemplified by the \"phi mask\", a diagnostic tool developed by American surgeon Dr. Stephen Marquardt as a \"method for measuring facial attractiveness in an objective manner\" (Bashour, 2006, 757). The phi mask allegedly reflects the golden ratio, which is the \"division of a line such that the ratio of the smaller section to the larger section is the same as that of the larger section to the whole\" (Rupesh et al., 2014, 22). As a diagnostic tool, Marquardt's phi mask is superimposed over a photo of a face to \"assess aesthetic problems\" ( Jefferson, 2004, 16). Jefferson claims that the farther the facial structures or landmarks are from the divine proportion, the more unattractive the face is. According to Rupesh and colleagues (2014), Marquardt claims that regardless of race or age, a beautiful face conforms to the proportions in his phi mask.\nSeveral authors have criticized Marquardt's beauty mask. Holland (2008) contends that using the golden ratio to describe beauty does not make the attempt objective. According to the author, Marquardt relies heavily on female fashion models as a reference standard of beauty. Other authors argue that the mask is oriented toward Western or Caucasian features, and the mask does not generally fit Asian features (Veerala et al., 2016;Aquino, 2017). Prendergast (2012) further argues that while golden proportion may be a recurrent theme in aesthetics, it should not be embraced as an immutable method of measuring human beauty at the expense of other factors. Despite the questionable notion that beauty can be reduced to numerical and seemingly objective values, the practice of pathologizing ugliness fuels the notion that surgeons and patients can conform to some allegedly scientific aesthetic norms.\nOn this account, normativist and naturalist notions of aesthetic harm, aesthetic dysfunction and aesthetic deviation underpin the practice of pathologizing ugliness in the cosmetic surgery. Next, I discuss the ways in which the practice of pathologizing ugliness further complicates the ongoing debate about accounts of disease in philosophy of medicine.\n\nVI. HYBRIDIZ ATION LOOP AND AESTHETIC PATHOLOGY\nIn this last section, I further demonstrate the relevance of aesthetic pathology-invoked in the practice of pathologizing ugliness-to ongoing debates in philosophy of medicine. In particular, I introduce the phenomenon of hybridization loop 6 in medicine (see fig. 1), which involves the twin processes of (1) naturalizing aesthetic ideals and (2) normativising statistically determined biological norms. By focusing on the practice of pathologizing ugliness, the loop offers a way of explaining the role of medicine-its practitioners and interventions-in blurring the line between naturalistic and normativistic understanding of disease and collapsing the distinction between the normal and the pathological. Before I discuss the hybridization loop, it is important to note that aesthetic pathology further complicates the already muddled distinction between cosmetic and reconstructive procedures. According to a 2017 report by the Nuffield Council on Bioethics (hereafter Nuffield), there is difficulty in drawing a sharp and consistent line between reconstructive and cosmetic procedures. The same procedure that employs the same tools and methods may be undertaken both for restorative and for the cosmetic purposes. The distinction then appears to be drawn in relation to the motivation, rather than the nature of the procedure itself (Nuffield, 2017). Currently, there are at least two ways to differentiate reconstructive from cosmetic procedures according to standard distinctions (Devereaux, 2009). One distinction lies with the type of appearance being modified. Reconstructive procedures modify pathological appearances resulting from disease, congenital deformity, or physical trauma; while cosmetic procedures modify normal, at least average-looking, features to achieve an enhanced appearance in the absence of specific disease or deformity. A second distinction refers to the type of appearance aimed for by the procedure. Reconstructive procedures tend to aim for the restoration of normal appearance, while cosmetic surgery aims for a more beautiful appearance than the patient's naturally occurring features. The practice of pathologizing ugliness makes it clear that even the relatively tenuous distinctions between cosmetic and reconstructive surgery are not going to hold up, given the hybridization loop in medicine.\nOne part of the hybridization loop leads to the naturalization of normativistic ideals. Naturalization in this case refers to a process of transforming socially determined aesthetic ideals into seemingly biological and statistically derived \"normal\" traits. In pathologizing ugliness, naturalization typically involves framing unattractive traits in terms of aesthetic dysfunction and/or the statistical deviation. Interventions that naturalize the ideal are exemplified by dental practices in the United States, where straight and pearly white teeth have become the norm. In other parts of the world, people view normal teeth as including variations in shade and alignment. In the United States, advertisements of the widely available dental treatments together with the willingness of Americans to undergo such treatments have generated the non-naturally occurring but the widespread norm of straight, white teeth (Khalid and Quiñonez, 2015). Consequently, having straight, white teeth get transformed from a normativistic aesthetic ideal into an instantiation of a statistically typical feature of normality.\nAnother example of this part of the hybridization loop is the popularity of facial surgeries that modify the non-Caucasian features, referred to as ethnic cosmetic surgery, in the Western countries. In the United States during the late twentieth century, facial features associated with African-Americans were subject to surgical modification as they were deemed to be deviating from the Caucasian-centric facial norms (Ghigi, 2009). More recently, Asian-Americans in the United States are choosing to undergo eye surgery to create double eyelids, a feature that is typical and naturally occurring among Caucasians (Kaw, 1993;Heyes, 2009;Aquino, 2017). Cosmetic surgeons naturalize Caucasian-centric aesthetic ideals by reducing the \"cause\" of the aesthetic concern from a socially determined preference into a genetically caused deformity that deviates from a statistically determined norm, given that the majority of the population in the United States is Caucasian (Kaw, 1993). Moreover, naturalization appears to be legitimized by formulating allegedly scientific measurements that establish a normal range from which ethnic features are deemed to deviate. For example, the surgical recommendations on Asian upper lid surgery state that the upper lid fold is typically located ~6.5 to 7.5 mm from the upper eyelashes in Asians versus 9 to 11 mm in Caucasians (Pai-Dei Chen and Park, 2013). Other facial features, such as the nose, jawline, and cheekbones, are similarly described in statistically derived numerical values. These examples demonstrate the ways in which medical and surgical interventions translate beauty ideals into biological norms that can be achieved through modification. Hence, medical interventions can create or perpetuate ideals of beauty that become the norm to the extent that such features appear to qualify as both statistically and functionally naturalistic in a Boorsean sense.\nThe other part of the hybridization loop involves medicine's tendency to idealize naturalist and biologically typical norms. In the practice of pathologizing ugliness, this process occurs when statistically typical features are idealized, in that medical and surgical interventions are offered to \"correct\" features that deviate from the statistical norm. While idealization of the naturalist norms may be bizarre in the context of beauty and ugliness, there are several examples in other areas of medicine. The dominant medical model, in particular, depicts bodies with visible signs of deformity and disability as lacking and in the need of repair (Garland- Thompson, 2001;Scully, 2004;McLaughlin 744 • Yves Saint James Aquino and Coleman-Fountain, 2014). This depiction underpins the tendency to employ surgical interventions on visible signs of deviation from accepted (and statistically determined) norms, at the times regardless of the extent of dysfunction associated with such visible deviation. For example, the typical feature of having ten fingers and ten toes is normativised when medicine determines that deviation from the standard number and configuration is a deformity subject to intervention. Polydactyly, an atypical feature that involves having a supernumerary digit in the hand or foot, is considered a deformity that can be treated by surgical excision of the extra digit (Kubat and Antičević, 2017). The example of surgical intervention for polydactyly shows the ways in which the naturalistic and typical set of fingers and toes becomes a socially normativistic ideal. On this account, medical and surgical interventions, such as the removal of a supernumerary digit, drive the slippage from the naturalistic notion of statistically normal into the normativist notion of a socially valued ideal.\nThe designation of aesthetic pathology similarly demonstrates the way in which cosmetic surgery concretizes fictional bodily norms. As in the most cases of pathologization, aesthetic pathology appears to command an obligation for bearers of undesirable features to seek intervention and for medical professionals to provide such intervention. Bordo (2009) argues that it is one thing to receive messages promoting aesthetic ideals from magazines and movies, but it is another to get such ideals as clinical judgments from the cosmetic surgeons. The author adds that framing ugliness as disease converts a subjective perception into a judgment mandated by medicine's expertise and professional authority. Consequently, medicine's professional authority establishes the understanding that medical procedures that modify disease-free but undesirable features are curative treatments. While the distinction of cosmetic and reconstructive surgery is tenuous, some cases remain to be generally classified under one category and not the other. To illustrate this distinction, one can compare the management of a broken jaw because of injury and a jaw deemed \"too prominent\". The former is considered reconstructive in that the aim is to correct a physical deformity caused by injury, while the latter is cosmetic because it aims to enhance the appearance of an unattractive feature that is free from disease or injury. However, reframing prominent jaws as an example of aesthetic pathology entails that the surgical intervention is no longer merely cosmetic but allegedly curative-and, thus, reconstructive. My conceptual analysis of aesthetic pathology shows that cosmetic surgeons appear to have moved towards eliminating any form of distinction between cosmetic and reconstructive surgery. 7 The practice of pathologizing ugliness thus demonstrates the tendency of medical processes to involve a hybridization loop, where normative values are transformed into naturalistic norms and vice versa. The twin processes of naturalizing social ideals and normativising biological norms involved in the loop help to illustrate the role of medicine in the expansion of the boundaries of disease. First, the practice of pathologizing ugliness translates socially valued aesthetic ideals into the biological norms that can be achieved through surgical interventions. Second, the continued use of surgical interventions for undesirable features that deviate from the statistically normal appears to idealize naturalist and biologically typical norms. Consequently, the practice of pathologizing ugliness tends not only to collapse the normal and the pathological but also blur the line between naturalistic and normativistic understandings of disease.\n\nVII. CONCLUSION\nIn this article, I offered a conceptual analysis of aesthetic pathology by drawing on the longstanding debates in philosophy of medicine. Aesthetic pathology illustrates cosmetic surgery's practice of explicitly reframing normal but unattractive features as a type of disease or deformity, thus warranting treatment. The first part of my two-pronged conceptual analysis described three sets of pathologizing claims that range from naturalistic to normative, namely, aesthetic harm, aesthetic dysfunction, and aesthetic deviation. The second part introduced the phenomenon of the hybridization loop, which showed that cosmetic interventions simultaneously involve naturalizing beauty ideals and normativising statistically determined norms of appearance. The loop illustrated the manners in which cosmetic surgical interventions blur the line between naturalistic and normativistic understandings of disease and facilitate the pathologization of ugliness. Traditionally, normative accounts, not naturalist accounts, are subject to the charge that aesthetic concerns muddle disease definitions.\nThe narrow coverage of my conceptual exploration of aesthetic pathology invites critical analysis of the legitimacy of pathologizing ugliness. One possible position is to establish that aesthetic harm, dysfunction, or deviation (or a combination thereof) is sufficient in labeling ugliness as pathological, as Francesca Minerva (2017) argues. In contrast, one can revitalize the difficult distinction between cosmetic and reconstructive surgery, where appearance concerns that are free from disease or injury should be viewed as normal. A third middle-ground approach may espouse a selective pathologization, with aesthetic concerns associated with social harms being considered pathological, while those based on deviation alone remain non-pathological. In any case, further critical analysis needs to establish the extent to which aesthetic pathology can measure up to the widely regarded as objective and scientific methods that inform norms in clinical practice, such as establishing standardized diagnostic criteria, promoting evidence-based practice guidelines, and defining clear indications for treatment, among others (Aquino, 2020).\nMoreover, my discussion potentially shows the value of investigating the ways in which conceptual and ethical analyses of pathologizing ugliness can inform each other. It is possible that the answer to the question of the legitimacy of aesthetic pathology is a matter of social justice rather than of conceptual boundaries of disease (see, Little, 1998;Dolezal, 2010). Future research can draw on homosexuality and disability as illustrative examples of the ways in which conceptual, ethical, and socio-political factors may warrant moving certain states away from the disease model.\nFinally, my conceptual analysis of aesthetic pathology invites further analysis of the broader impact of the hybridization loop on the conceptual boundaries of disease. The hybridization loop helps to illustrate the role of the medical practices in muddling our conceptions of disease by pathologizing (and proposing interventions for) any condition that departs from socially valued ideals or deviates from statistically derived normal traits. The hybridization loop is thus potentially relevant to a range of interrelated phenomena in medicine that risks expanding the boundaries of disease, including disease mongering, overdiagnosis, and overtreatment, among others. Disease mongering, for example, involves pathologization of symptoms or risk factors, or exaggerating the seriousness of mild or non-harmful conditions (Moynihan, Heath, and Henry, 2002). Common examples of disease mongering include baldness, shyness, and erectile dysfunction, as well as pre-disease states that refer to categories with abnormalities that remain below the threshold for diagnosis (e.g., pre-diabetes). The hybridization loop can help to examine the role of medicine in some varieties of disease mongering, where medical interventions and practices transform socially disvalued states, such as shyness (Krämer et al., 2012) and sadness (Horwitz and Wakefield, 2007), into seemingly naturalistic pathologies with underlying biological dysfunctions. In turn, the ensuing pathologization of such states typically involves the use of medical interventions that further reify such states as departing from both socially valued ideals and statistically determined biological norms-completing the hybridization loop discussed in pathologizing ugliness.\nIn conclusion, the conceptual analysis of aesthetic pathology demonstrates that cosmetic surgery's practice of pathologizing ugliness should be of interest to philosophers of medicine. Aesthetic pathology, which relies on both the naturalistic and normative claims, offers an interesting challenge to the untidy division between the naturalist and normative accounts of disease. Moreover, the hybridization loop in pathologizing ugliness offers a concrete example of the way medical and surgical interventions complicate the distinction between the ideal, the normal, and the pathological, showing the aesthetic pathology's significance to the conceptual boundaries of disease.\n\n• Yves Saint James Aquino\nNOTES 1 I apologize for having to use the terms \"ugliness\" and \"ugly\" in the course of this article. I admit that my use of these terms may further confer currency onto their oppressiveness. However, part of my task as a philosopher is to critically analyze these concepts, and the meanings and assumptions-including detestable ones-that they embody. 2 Psychological conditions with strong aesthetic fixations (e.g., imagined ugliness), such as body dysmorphic disorders (BDD), are an interesting topic of investigation but is not covered in this research. BDD is characterized by persistent and sometimes debilitating preoccupations with one's physical appearance, which can manifest as intense dislike of a part of one's body. 3 Historians of cosmetic surgery, such as Haiken (1997) and Gilman (1999), claim that pathologizing ugliness has always existed as part of the development of cosmetic surgery as a specialty that is separate from reconstructive surgery. 4 I briefly note here that the application of the terms \"function\" or \"dysfunction\" in aesthetic pathology appears to misrepresent the way these terms are understood in the naturalist account of disease. One can compare the manner in which function is understood in a physiological versus aesthetic sense. The physiological function of a heart to pump blood is both intuitively and logically acceptable in a medically scientific sense. In comparison, the concept of aesthetic function remains scientifically dubious since the standards of beauty that underpin definitions of aesthetic function are tenuous and arbitrary. 5 Assisted reproductive technology further complicates the issue because it allows artificial means of reproduction. The technology allows, for example, single women to have children, eliminating the need for sexual partners. 6 I am inspired by the looping effect from Ian Hacking's (1995) discussion of human kinds, which resembles some of the discussion here regarding norms. According to Hacking, the looping effect results from the interplay of (scientific) facts and (social) values, whereby facts modify our values and at the same time values modify how we appreciate facts. 7 To further advance the legitimizing aim of pathologizing ugliness, cosmetic surgeons have started appropriating the concept of prevention for aesthetic purposes. Prevention in mainstream medicine means either averting the occurrence or slowing the progress of a disease (Starfield et al., 2008). Cosmetic surgeons in the United States currently advertise \"early interventions\" that improve the chances of cosmetic modifications lasting longer (Blum, 2003, 72). The surgeons claim that since younger skin is more elastic, early interventions require less extensive and less frequent surgeries than interventions performed on bodies at older ages. Blum argues that the consequence, which may be bad for the patient but good for the surgeon, is that early intervention will \"require maintenance\" (2003,72). Such cosmetic procedures are now being performed in younger individuals with the aim of delaying the appearance of visible signs of aging. The example illustrates the manner by which cosmetic surgery manipulates our understanding of health and disease by appropriating established therapeutic norms, such as prevention, from mainstream medicine."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 44525,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255021537"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The Riddle does not exist ” . On proposition 6.5 of the Tractatus logico-philosophicus .\n\nThe essay has as its main theme proposition 6.5 of Wittgenstein’s Tractatus logico-philosophicus and, in particular, its second paragraph \" The riddle does not exist\". In the first section, the notion of riddle is compared with that of problem, emphasising, among other things, the great relevance that, for Wittgenstein, the distinction between problems of philosophy and problems of natural science has; in the second, an attempt is made to clarify the meaning that Wittgenstein assigns to the word “riddle”: a riddle would be a question “in the void”, without any “direction”, that is, so to speak, a question that does not know what it is asking. This means that, even if it presents itself as a question, the riddle is a non-question. The third section highlights how the denial that there is the riddle is complementary to the denial, so characteristic of the Tractatus , that there are a priori true thoughts or propositions\n\nlies outside space and time\" (TLP: 2 6.4312a)? (d) 3 What, if anything, can we learn about this proposition 6.5 from the four direct comments accompanying it (TLP: 6.51, 6.52, 6.53 and 6.54) 4 and from its being one of the main comments on proposition 6? 5 What is a problem? The problems of philosophy and the problems of natural science The term \"riddle\" immediately recalls another term, the term \"problem\" (Problem), which, as we can easily verify, occupies an important place in the Tractatus.\nMoreover, \"problem\" is on at least one occasion used by Wittgenstein as a synonym or in the sense of \"riddle\". It is, in fact, evident that \"the problem of life\" spoken of in the first paragraph of proposition 6.521, i.e. that problem whose solution \"is seen in the vanishing of the problem\", is none other than that \"riddle of life\" evoked in the first paragraph of proposition 6.432: \"The solution of the riddle of life in space and time lies outside space and time\".\n\"Problem\", in the singular or plural, is, as has just been mentioned, a term that recurs frequently in the Tractatus. We immediately note that, in this regard, Wittgenstein's 2 For the abbreviation used in quotations from the Tractatus, see the final bibliographical note. 3 On point (d) I will limit myself to a few considerations that are certainly not exhaustive. 4 \"Scepticism is not irrefutable, but obviously nonsensical, when it tries to raise doubts where no questions can be asked. / For doubt can exist only where a question exists, a question only where an answer exists, and an answer only where something can be said\" (TLP: 6.51); \"We feel that even when all possible scientific questions have been answered, the problems of life remain completely untouched. Of course there are then no questions left, and this itself is the answer\" (TLP: 6.52); \"The correct method of philosophy would really be the following: to say nothing except what can be said, i.e. propositions of natural science -i.e. something that has nothing to do with philosophy -and then, whenever someone else wanted to say something metaphysical, to demonstrate to him that he failed to give a meaning to certain signs in his propositions.\nAlthough it would not be satisfying to the other person -he would not have feeling that we are teaching him philosophy -this method would be the only strictly correct one\" (TLP: 6.53); \"My propositions serve as elucidations in the following way: anyone who understands me eventually recognises them as nonsensical, when he has used them -as steps -to climb up beyond them.\n(He must, so to speak, throw away the ladder after he has climbed up it.). / He must transcend these propositions, and then he will see the world aright\" (TLP: 6.54). It cannot be avoided here to point out how two of the Tractatus most well-known and controversial propositions, propp. 6.53 and 6.54, are part of the comments to proposition 6.5 on the non-existence of the riddle. reason why these problems are posed is that the logic of our language is misunderstood\" (TLP: p. 3) and that, precisely for this reason, they do not belong to that type of problems, the problems of natural science, which testify that our knowledge of the world (or of reality) or of some part or sphere of it is inadequate, partial or even completely absent; that is why, as we shall see, the solution to the problems of philosophy can never depend on a better or more extensive knowledge of the world (of reality); 6 (c) that for these problems he believes he has found, in the Tractatus, \"on all essential points, the final solution\" (TLP: p. 4), although he declares himself, in the end, convinced that one of the two values of his book consists, precisely, in having shown \"how little is achieved [e.g. in the perspective of the man of science or in the eyes of the philosopher who considers philosophy a science] when these problems are solved\" (TLP: p. 4).\nWhat is stated in the Preface could perhaps be clarified by saying that solving the problems of philosophy means or implies that one recognises and accepts that the problems of philosophy are not, strictly speaking, problems, i.e. that, in the proper sense, problems are only and exclusively the problems of natural science, i.e. problems such as those expressed by questions such as \"How far is the planet Mercury from the Sun?\"; \"When was the last ice age?\"; \"What kind of gas is helium?\"; \"What is inflation due to?\", etc. As these examples suggest, a problem of natural science is one that one can try to solve by making this or that conjecture or hypothesis (cf. TLP: 4.1121) about \"how things are in the world\" (TLP: 6.44) and ascertaining whether they are, in fact or really, so. After one has solved one of these problems, one would say, one knows more about the world and one can, eventually, use this new knowledge to refine our techniques, to try to satisfy, thanks to them, our various needs, desires, etc. For example, a correct answer to the question \"What is the cause of inflation?\" can enable us to devise effective economic policies that are useful in counteracting inflation or controlling it or, when appropriate, promoting it. Seeking an answer to the questions of science means, in short, aspiring to a better or more extensive knowledge of the world, whether this is done, let us say, for the sheer sake of knowledge or in view of the practical effects and technical acquisitions that can be obtained from it or for any other reason (personal affirmation, enrichment, etc.).\nOf course, it is one thing to be interested in glaciations, quite another to be interested in gases or inflation or the solar system, just as it is yet another thing to be interested, as the science that is psychology is, in the various states or mental processes (human or animal). But there is something that unites all these interests in so many different ways, and that is that what the geologist like the psychologist or the chemist, the astronomer, etc. has to deal with are, precisely, problems. And this means that their eventual resolution depends, as we said, on ascertaining \"how things are in the world\" (TLP: 6.44). Let us assume that the answer to the question \"When did the last ice age occur?\" is, as in fact it is, \"The last ice age began 110,00 years ago and ended 11,700 years ago\". Well, for this proposition one must say what, according to the Tractatus, applies to every proposition, namely that in order to tell whether it \"is true or false we must compare it with reality\" (TLP: 2.223), it being impossible to tell from the proposition alone \"whether it is true or false\" (TLP: 2.224). Like any other proposition, \"The last ice age began 110,00 years ago and ended 11,700 years ago\" cannot be \"true a priori\" (TLP: 2.225), i.e. before or independently of its comparison with reality.\nOf course, the manner or methods of this comparison or contrast may be Note that Wittgenstein's intent when he states that psychology is just one of several natural sciences is not to claim the natural (material or physical) character of its object. It is not to claim, for instance, that the real object of psychology as a science is the brain and not the mind or, still less, the soul. As the third paragraph of proposition 5.641 shows very well, psychology is and remains, according to Wittgenstein, one of the natural sciences, whatever the object assigned to it: \"the human being\", as non-dualist psychologists may believe, or \"the human body\", as materialists or behaviourists may think, but also \"the human soul\", as Cartesian dualists may believe.\nSuppose, for example, that the object of psychology is considered to be the human soul, as distinct from the body or, more specifically, the brain. Well, what Wittgenstein wants us to understand is that this does not mean that psychology would cease to be a natural science; like any natural science, it would continue to treat its object as a part of the world (cf. TLP: 5.641c), i.e. as something that, even when it is exactly as it is described, \"could be other than it is\" (TLP: 5.634c). Even for a possible science of the soul what applies to any other science would apply, namely that none of its propositions would be true a priori.\nIt should then come as no surprise that, in the Tractatus, Wittgenstein would go so far as to consider the problem of \"the temporal immortality of the human soul, that is to say of its eternal survival after death\" (TLP: 6.4312a), 7 as a problem of natural Returning, on the basis of the preceding considerations, to the first paragraph of prop. 6.4312, we can begin by noting how evident it is for Wittgenstein that the length of our life has little or, rather, nothing to do with its sense, and thus with the solution of the \"riddle\" or \"problem of life\" (TLP: 6.521a). The life of a centenarian has no more or less sense than the life of someone who died long before that venerable age. He who has lived longer has simply lived longer. \"Did he die a centenarian,\" we might also say, is in no way an answer to the questions, \"Did he solve the problem of life?\" or \"Did he find the sense of life?\" That he died a centenarian is a fact and, as we have seen and it is What is a riddle? How is a riddle resolved?\nAs we have seen, a scientific problem is one that requires, in order to be solved, and not by looking in the back cover for an indication of its price. As Wittgenstein wrote in the Philosophical Remarks, \"[w]hen I tell someone that tomorrow will be fine weather, he attests to his own understanding, not by seeking now to verify my statement\" (Wittgenstein 1980: §27g).\nObviously, it can happen that it becomes impossible to establish how many pages the book in question has. For example, it may be that you have never happened to check the number of pages and that the book is stolen before you can do so and that the publishing house that published it has long since destroyed all copies and that no copies of the book exist any more, etc. However, it remains established that \"That book has 125 pages\" is the true or correct answer if and only if that book has 125 pages and that it is in the book, and not elsewhere, that one must look for the answer. As Wittgenstein writes, \" [i]f a question can be framed\", then it must also be \"possible to answer it\" (TLP: 6.5c), even when it is, in fact, i.e. given these or those circumstances, impossible to do so. And this is because understanding a question means knowing where the answer is to be sought. The question \"How many pages does the book you are reading have?\" can only be answered by looking in the pages of the book and establishing, if it is in fact possible, how many they are. Any other answer would not be an answer. Put somewhat crudely, \"The book costs 15 euros\" can never be an answer to the question \"How many pages does that book have?\", although knowing that it costs 15 euros may be more important to me than knowing that it has 125 pages. A few years after the Tractatus, exactly in 1930, Wittgenstein will effectively explain by once again raising the question \"What is a question?\": What is a question? It is a request to look for something. A question introduces a movement of thought, as it were, at the end of which the answer is to be found. The direction of that movement is determined by the logical place of the answer. If no answer exists, then there is no direction in which you can look for anything; hence there is no movement of thought, and that means that there is no question (Waismann 1979: p. 245).\nBut there is also something else that must be emphasised here, namely that what applies to every other proposition also applies to \"This book has 125 pages\": in order to tell whether it \"is true or false we must compare it with reality\" (TLP: 2.223). Of course, it can certainly happen that the book has exactly the number of pages I claim it has, i.e.\nthat it has 125 pages. But everything that happens to the book (or of the book) belongs to the world, i.e. to the \"sphere of what happens and is the case\", i.e. to the sphere of the accidental. Indeed, as Wittgenstein writes, \"all that happens and is the case is accidental\" (TLP: 6.41b). Certainly, our propositions interrogate reality and force it, when they are really such, 9 to give an answer. As Wittgenstein writes, with what seems almost a reference to the Gospel, \"[a] proposition must restrict reality to two alternatives: yes or no\" (TLP: 4.023a). What a proposition can never do, however, is to answer \"yes or no\" instead of reality. It is also in this sense, or especially in this sense, that it is impossible to tell from the proposition alone \"whether it is true or false\" (TLP: 2.224).\nBut we will return to this in the last section of this essay.\nWe finally come to the question of the riddle. What does Wittgenstein mean by \"riddle\"? The answer is, at least in part, provided to us by the first and third paragraphs of prop. 6.5, which frame, as it were, the second paragraph and its peremptory statement: not know what it is asking; a question, as we might also say, that knows nothing about the answer; a question without \"movement\" and \"direction\" (Waismann 1979: p. 245).\nBut, as we know, such a question, a question that does not move towards the answer, is not a question. In a \"verificationist\" spirit, he will go so far as to write in the Philosophical Remarks that \"[t]he meaning of a question is the method of answering it\" (Wittgenstein 1980: §27a), whereas in the Tractatus, in the proposition devoted to scepticism, which, as we know from the decimal numbering, is a comment on our proposition 6.5, we can read, in terms less compromised with verificationism, that a question exists \"only where an answer exists\" (TLP: 6.51a).\nLet us then consider what Wittgenstein writes in the first paragraph of prop. 6.5: \"When the answer cannot be put into words, neither can the question be put into words\". Here Wittgenstein is referring polemically to all those who think that, just as one can ask what the capital of Norway is, one can also ask, for example, what the sense of life is (cf. 6.521b). For them, there are no limits or constraints or obstacles to the questions we can express or formulate. While it may be more or less difficult to answer, one can always ask. There are, in short, no questions that cannot be \"put into words\".\nSo, like the question about the capital of Norway, the question about the meaning of life can also be \"put into words\", but unlike the former, this second question is one that not only does not yet have an answer, but perhaps never can. Perhaps the answer to the question about the sense of life can never be \"put into words\". Once again, we have a question without \"movement\" and \"direction\" (Waismann 1979: p. 245), thus a non-question.\nWhat Wittgenstein seems to be suggesting is that it is precisely these questions that philosophy has always considered the most precious profound (cf. TLP: .003c), precisely because they are the hardest and most difficult. Harder and more difficult than the hardest and most difficult questions of science. Unlike those of science, in fact, the questions of philosophy are riddle. It is certainly no coincidence that many philosophers seem to have thought that the task of philosophy was not so much to unravel these riddles as to preserve and pass them on. How many times have we not heard it repeated that in philosophy, unlike in science, it is the questions, and not the answers, that count? years later in the first lines of the Blue Book, regarding questions of the form \"What is...?\", for example questions such as \"What is lenght?\", \"What is meaning?\" or \"What is the number one?\". These questions, he says, \"produce in us a mental cramp. We feel that we can't point to anything in reply to them and yet ought to point to something. (We are up against one of the great sources of philosophical bewilderment: a substantive makes us look for a thing that corresponds to it.)\" (Wittgenstein 1975: p. 1).\nThe preceding remarks can serve as a background to prop. 6.52, one of the most quoted in the Tractatus, which constitutes the second comment to prop. 6.5. Here Wittgenstein refers to a feeling that, by using the pronoun \"we\", he wishes to share with the reader he evokes at the beginning of the Preface. 12 \"We feel -we read, in fact, in the first of the two statements that form it -that even when all possible scientific questions have been answered, the problems of life remain completely untouched\" (TLP: 6.52).\nWe might immediately note that here the adjective \"scientific\" is redundant. As we know, there are no questions that are not scientific. A question, if it is such, is a scientific question; which does not mean that it belongs to this or that science (physics, biology, etc.), but that it, like its possible answer, belongs to \"what can be said\" (TLP: 6.53); to what can be \"put into words\" (TLP: 6.5a). The same clarification applies to the identification found in the prop. 6.53 between \"what can be said\"' and \"propositions of natural science\" (TLP: 6.53). According to Wittgenstein, what he elsewhere calls \"the propositions of our everyday language\" and of which he states that they are, \"just as they stand, [...] in perfect logical order\" (TLP: 5.5563a) are, like the propositions of physics, biology, etc., \"propositions of natural science\" (TLP: 6.53).\nBut perhaps it is not accidental that prop. 6.52 speaks of \"scientific questions\".\nWittgenstein is probably referring here to that \"modern conception of the world\" (TLP: 6.371) or to that \"modern system\" (TLP: 6.372b) which is characterised by the conviction that, with its research, discoveries and inventions, science can provide, if not now, at least in the future, an answer to \"the problems of life\" (TLP: 6.52). For the modern conception of the world, in short, it is only science that can solve \"the problems 12 \"Perhaps this book will only be understood by someone who has already had the thoughts that are expressed in it -or at least similar thoughts. -So it is not a textbook. -Its purpose would be achieved if it gave pleasure to one person who read and understood it' (TLP: p. 3). Proposition 6.5 (with its comments) is one of the places in the Tractatus that confirm its author's idea that \"[i]t is not a textbook\". gown of the scientist. But neither is he suggesting to us some flight into the irrational.\nIn fact, what, in perfect harmony with prop. 6.52, Wittgenstein is telling us is that \"to say nothing except what can be said, i.e. propositions of natural science\" (TLP: 6.53) is the most correct way, indeed the only correct way, for philosophers 16 to give an answer to the problems of life. 17 Wittgenstein's idea is, to put it a little too directly, that one can do science without being subject to \"the modern conception of the world\", which \"tries to make it look as if [by science] everything were explained\" (TLP: 6.372).\n\nNeither riddle nor a priori truth\nAs we have seen in the previous sections, a riddle would be a question \"in the void\" (Wittgenstein 1980 Here Wittgenstein observes, that a thought \"correct a priori\" would be \"a thought whose possibility ensured its truth\" (TLP: 3.04). This would mean that its truth would be \"recognisable from the thought itself (without anything to compare it with.)\" (TLP: 3.05). If we recall what we said about the riddle, we might then conclude that an a priori truth is, as we said, the exact opposite of the riddle: a question that, as it were, contains within itself the truth of its answer. Asking and answering would be the same and identical thing; one and the same movement. Well, if against the riddle the Tractatus asserts that \" [i]f a question can be framed at all, it is also possible to answer it\" (TLP: 6.5c), against rationalism he argues that the truth of the answer is never contained in the question. The question is directed towards the answer, but it is not the answer."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 47489,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254410328"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Awareness: An empirical model\n\nIn this work, we face the time-honored problem of the contraposition/integration of analytical and intuitive knowledge, and the impact of such interconnection on the onset of awareness resulting from human decision-making processes. Borrowing the definitions of concepts like intuition, tacit knowledge, uncertainty, metacognition, and emotions from the philosophical, psychological, decision theory, and economic points of view, we propose a skeletonized mathematical model grounded on Markov Decision Processes of these multifaceted concepts. Behavioral patterns that emerged from the solutions of the model enabled us to understand some relevant properties of the interaction between explicit (mainly analytical) and implicit (mainly holistic) knowledge. The impact of the roles played by the same factors for both styles of reasoning and different stages of the decision process has been evaluated. We have found that awareness emerges as a dynamic process allowing the decision-maker to switch from habitual to optimal behavior, resulting from a feedback mechanism of self-observation. Furthermore, emotions are embedded in the model as inner factors, possibly fostering the onset of awareness.\n\n\nIntroduction\nAside from the classical analytical perspective, this work mainly focuses on the weight of the impact and relevance of other facets that belong to the decision-making processes, such as tacit knowledge, intuition, emotions, awareness, and self-awareness. These factors, once considered largely irrelevant (if not a nuisance to be eliminated) in the decision-making process, are now being taken into account more seriously in cognitive studies, and their multi-faceted effects are intensely analyzed. Even though all of them have been thoroughly studied and described from a theoretical point of view in different fields of investigation, a formalization from a modeling point of view is still missing. This is the gap the present work aspires to begin filling by proposing a possible modeling formulation that is both broad enough to consider all these different aspects and sufficiently simple to be clearly understandable, and thus, interpretable according to the different perspectives of the practitioner. Although limited and imperfect by nature, also due to the difficulties in modeling complex phenomena-as human decisions arethis study could contribute to introducing new aspects which can expand research in the field of decision-making. This research has a multidisciplinary intent, involving different disciplines ranging from behavioral and cognitive sciences, economy, mathematics, philosophy, and psychology, and attempts to incorporate all the different perspectives involved in the process. We tried to be as faithful as possible to the different concepts exposed according to specialist literature while maintaining a constant interest and focus on the modeling perspective. Indeed, this work does not intend to give unique and definitive modeling recipes; on the contrary is aimed at fostering general interest in the explicit inclusion of crucial aspects of decision-making into quantitative models of awareness. Specifically, our aim is to propose a mathematical model incorporating all processes fostering the emergence of awareness. On one hand by identifying the main characteristics behind decision-making (including analytic and intuitive factors), their relationship with emotions and other drivers, and on the other hand by highlighting novel logical and philosophical aspects such as the importance of tacit knowledge, the correlation between optimal decisions and uncertainty, and the awareness dynamics. The mathematical formulation of the model is grounded on Decision Markov Processes, which embed most characteristics of human decision-making, indeed they integrate control actions, uncertainties, and temporal dynamics. Specifically, at each time step the decision-maker's specific choice provokes a change in the system's state according to the control actions he/she chooses. Additionally, this change is affected by external noisy stimuli. The evolutionary dynamics of such a system can be evaluated for very long-time processes (up to the limit of infinite time). For finite time-horizon processes, such as the ones reasonably considered in this paper, the trajectory of the system depends on the reward functions, the transition probabilities, and, at the final time, the terminal reward value. In any case, the convergence of the trajectory will be evaluated on average due to the stochastic nature of the process.\nThe formulation of mathematical models enables us to perform extensive simulations to understand the multifaceted nature of this process.\nThe paper is organized as follows. Section Decision-making processes: an overview exposes general concepts related to decision-making processes and awareness, as the individual styles and approaches, the role of emotions, tacit knowledge, and the relationship between decisions and information. Furthermore, we will offer a panoramic view on how scientific literature deals with the concept of awareness and its impact on decisions. Section Developing a mathematical model of awareness reports the mathematical formulation of the proposed model. It is first simply and generically described, and later possible developments and extensions are introduced. Section Numerical results presents some numerical results of the simulations carried out by applying the proposed model and their discussion.\n\nDecision-making processes: An overview\nRationality, intuition, tacit knowledge, and emotions\n\nRationality and intuition\nUntil very recently, scholars and practitioners agreed that effective decision-making occurs only under the most rational conditions. Since Descartes (2008) cognition has had a stronghold as being the only legitimized contributor to reasoning in that decisions must come exclusively from rational, cognitive, and logical processes, while emotions, intuition, and other subjective aspects are not considered as having a significant role in the process. This conventional teaching has been that \"the more objective and rigorous our thinking processes are, the better our decisions will be.\" A totally different perspective (much more similar to contemporaneous views) on cognition was outlined in the same years by another French scientist and philosopher, Blaise Pascal (2012). It is worth reporting a small extract of his considerations on cognition processes, which are at the very basis of our proposal (emphasis added).\n\"THE DIFFERENCE between the mathematical and the intuitive mind.-In the one, the principles are palpable, but REMOVED FROM ORDINARY USE; so that for want of habit it is difficult to turn one's mind in that direction: but if one turns it thither ever so little, one sees the principles fully, and one must have a quite inaccurate mind who reasons wrongly from principles so plain that it is almost impossible they should escape notice [. . . ] But in the intuitive mind, THE PRINCIPLES ARE FOUND IN COMMON USE and are before the eyes of everybody. One has only to look, and no effort is necessary; it is only a question of GOOD EYESIGHT, but it must be good, for the principles are so subtle and so numerous, that it is almost impossible but that some escape notice [. . . ] These principles are so fine and so numerous that a very delicate and very clear sense is needed to perceive them and to judge rightly and justly when they are perceived, WITHOUT  Based on the above statements, it is clear that Pascal considered \"intuition\" (\"esprit de finesse\" in French, as opposed to the \"esprit de géométrie\") a proper form of knowledge and not \"irrational\" and \"purely emotive\" nuisances to the proper way of reasoning. In another part of his essay, he clearly states Frontiers in Psychology frontiersin.org . /fpsyg. . that a real scientist (philosopher) must adopt both attitudes to have a fruitful approach to science. In real life we have never had problems accepting Pascal's view, and it is shared common sense to appreciate both the \"holistic-intuitive\" and \"logical-analytic\" capacity of decision-makers (whether they are managers, scientists, physicians, etc.). However, shifting from real-life to academic formalism, things abruptly change.\nThe Cartesian way of thinking, much more primitive with respect to Pascal, has been reinforced in more recent times with the ascendancy of empiricism and positivism (Tayor, 2004). Given the gaps in the rational theories, an alternative perspective proposed by theorists calls for a richer conception of the decision-maker accounting for the assumption that decisions are also driven by emotion, intuition, imagination, experience, and memories, and thus implicitly reviving Pascal's statements. There is now a consistent body of research delving into the nature of decision-making, particularly into the role of cognition, intuition, and emotion in human decisions (Soosalu et al., 2019). Notably, intuition and cognition deal with two different ways to process information, which we call intuitive and analytical (Adinolfi and Loia, 2021). Although dual-process theories come in several forms, they reflect the generic fundamental distinction between the two processes. The first is related to intuition, which is often considered relatively undemanding in terms of cognitive resources, and is associative, tacit, intuitive, and holistic; hence, at odds with Pascal's position as he considers intuitive and analytical knowledge to be of equal importance. On the contrary, the second involves conscious, analytical, deliberate, cognitive, logical, linear, and reason-oriented thinking, making certain demands on \"cognitive\" resources (Hodgkinson et al., 2008;Kahneman, 2017). In our opinion, this consideration holds true if (and only if) we consider such resources in terms of computational cost and time, but not in terms of depth and subtlety. It is no accident that the English translation of \"esprit de finesse\" as \"intuition\" conveys a somewhat different meaning. The etymological roots of the term intuition stem from the Latin word in-tuir, which can be translated as \"looking, regarding or knowing from within\". Intuition encompasses a complex set of interrelated cognitive, affective, and somatic processes in which there is no apparent intrusion of deliberate, rational thought. Moreover, the outcome of this process (an intuition) occurs almost instantaneously and can be difficult to articulate. The outcomes of intuition are perceived as a holistic \"hunch\", a sense of calling or overpowering certainty, and an awareness of knowledge that is on the threshold of conscious perception (Bechara and Damasio, 2005). In their comprehensive review of literature on intuition within the field of management, Dane and Pratt defined intuition as \"affectively charged judgment that arises through rapid, nonconscious, and holistic associations\" (Dane, 2007;Adinolfi and Loia, 2021). We can say that Blaise Pascal's cognition theory has obtained its deserved consideration after nearly 350 years.\n\nTacit knowledge\nThe roles of intuition and tacit knowledge were formally incorporated into the theory (Brockmann and Anthony, 1998) as factors that lead to better decisions compared to those relying solely on the rational or analytical approach. This is particularly evident in those areas involving creativity such as innovating, visioning, and planning. The concept of tacit knowledge (sometimes also called implicit knowledge) is mainly attributed to Michael Polanyi, who introduced it for the first time in 1958 in his work Personal Knowledge (Polanyi, 2009). This term indicates a kind of knowledge that is opposed to formal and codified explicit knowledge; it is difficult to express or extract and thus even more difficult to transfer to others through writing or speech (Polanyi, 2009). It represents a content that is neither part of one's normal consciousness nor open to introspection. When applied, tacit knowledge is helpful but not externally expressed or declared; rarely do we recognize when we are using tacit knowledge. According to Polanyi, people cannot describe their use of tacit knowledge; \"we simply know more than we can tell\" is his typical expression to explain this concept. Implicit learning and implicit knowledge contribute to the knowledge structures upon which individuals draw when making intuitive judgments (see the visual representation reported in Figure 1). However, although they may underpin the non-conscious cognitive, affective, and somatic processes that lead to an intuitive judgment, they are not equivalent to intuition (Reber, 1993;Hodgkinson et al., 2008). In our opinion, tacit knowledge is the closest relative to Pascal's \"esprit de finesse\" which, in its original definition, is devoid of any \"emotional\" aspect.\n\nEmotions\nAs Damasio points out, an important aspect of the purely rational position is that to obtain the best results we must keep emotions out (Damasio, 1994). For a long time, emotion has been largely banished from the predominant philosophies and theories regarding decision, reason, and management. However, emotions have a considerable impact on an individual's decisions and must be carefully considered, particularly the immediate emotions. Immediate emotions are those experienced at the moment of decision, in contrast to the ones expected to be experienced in the future, like regret and disappointment (Loewenstein, 2000).\nWe most certainly always mix emotion and reasoning, analytic attitude and intuition, this mixture being something neuroscientists consider essential (Damasio, 2012). Rational decision-making skills are required to clearly and logically process available information, and thus allow for accurate . /fpsyg. .\n\nFIGURE\nA schematic representation of Knowledge. Tacit knowledge is involved in both intuitive and analytical reasoning, yet it is more heavily used in the former while in the latter it is only slightly adopted. An example of the application of tacit knowledge in analytical reasoning could come from the field of data analysis, where we can recognize the action of tacit knowledge in the choice of the model to use to analyze collected data. In both cases it is a fast process: tacit knowledge is immediately available and instantaneously applied by the individual. On the other hand, both intuitive and analytical approaches, thanks to the experience they bring to the individual, contribute to the slow process of knowledge sedimentation that creates an individual's unique implicit knowledge. These aspects are represented in the image by links: the fast (slow) characteristic is figuratively represented by waves with a high (low) frequency. Their amplitude represents the weight of the influence between tacit knowledge and analytical/intuitive reasoning and, on the contrary, the influence of analytical/intuitive approaches on tacit knowledge. Moreover, the characteristic of tacit knowledge that can be neither explicitly declared nor open to introspection is represented by the haziness of its area. The last arrow, between analytical and intuitive, stands for the continuously evolving relationship between these two individual modalities, which has a speed of change entirely depending on individual characteristics.\nperception and interpretation of events. In addition to this type of knowledge it is essential to consider that people make decisions based on tacit knowledge grounded in experience, and may use intuitive decision strategies almost exclusively, particularly under high-stress conditions (Sayegh et al., 2004). All these aspects are, to some extent, included in the model proposed in this article, which integrates the analytical, logical, and cognitive abilities of the decision-maker but also subjective aspects like intuition, tacit knowledge, and emotions. These components are always present in any decision-making process and must be collectively considered to reach more aware choices, which can, in turn, lead to an individual's all-encompassing wellbeing.\n\nInformation and overfitting\nA crucial point in the decision-making process is how a single individual approaches the incoming information. We could notice how decisions become faster and faster and are made in a constantly changing environment. The amount of data grows exponentially, but despite its abundance it could be inaccurate, incomplete, or confusing. The consequent increasing spread of misinformation is more serious in some sectors with respect to others: a paradigmatic case is the increasing interest in the field of Infodemiology, the study of the determinants and distribution of health information and misinformation (Eysenbach, 2002). This phenomenon represents the importance in exploring the correlation between data, reasoning propensity, and decisions. When we think about thinking it is easy to assume that \"more is better\"; we may assume that having more data about the decision context can lead to a more accurate prediction of the future consequences of our choices, and thus to a better result. However, this cannot be always true. The question of how hard to think and how many factors to consider is at the heart of a thorny problem that statisticians and machine-learning researchers call Overfitting (Christian and Griffiths, 2016). If we were to have extensive, completely mistake-free data drawn from a perfectly representative sample, and a precise definition of exactly what needed to be evaluated, then the best approach would be using the most complex model available. In this ideal situation the only problem should be the presence of correlations among the different pieces of information used for building the predictive model. It is a well-known problem in statistics where it is defined as the collinearity of the regressors (Dormann et al., 2013). In the paradigmatic case of a dependent variable Y to be predicted by a set of independent variables Xs (regressors), the existence of mutual correlations between Xs creates a fundamental indeterminacy of the resulting model (Krishnan et al., 2007), causing unpredictable errors in subsequent applications. In any case, we can imagine that in an ideal case these problems can be faced by a preliminary factorization of the data set into mutually independent components (Xie and Kalivas, 1997), or by any other technique of prioritizing variable orders (Alhamzawi and Ali, 2018). Nevertheless, in a real situation that is far from being ideal, if we try to fit a given model to the actual data, a certain risk of incurring overfitting exists. In other words, overfitting poses a danger any time we are dealing with mismeasurement or environments that are vague, ambiguous, and ill-defined, as is commonly the case in a working setting and in everyday life due to the complex surroundings we live in. It is true that including more factors in a model will always, by definition, produce a better fit for our data. However, a better fit for the available data does not necessarily imply improved ability to generalize and thus better predict future cases that, by definition, are not available at the time the predictive model itself is constructed (Diaconis and Mazur, 2003;Christian and Griffiths, 2016).\nConsequently, because any decision has to do with some kind of prediction of future outcomes, a better fit on actual data does not necessarily lead to a better decision. A model that is too simple can fail to capture essential aspects of the phenomenon studied; on the other hand, a model too complicated can become oversensitive to the particular adopted sample. Since the model is finely tuned to a specific data set, the resulting oversensitivity ends up intermingling a mixture of both general and idiosyncratic information relative to the specific sample, generating highly variable and consequently poor solutions. In this respect, it is worth noting that while the usual practice of splitting the whole data set into a \"training\" (from where the model is built) and a \"test\" set can be very wise, it only partially solves the overfitting problem. Successful models in science stem from the clear division of information into \"sloppy\" and \"stiff \" parts (in the jargon of data analysis). By focusing on solid information (usually consisting of very few control parameters) and leaving out the rest, it is possible to predict the behavior of very complex systems with good, and sometimes excellent, approximation (Transtrum et al., 2015;Giuliani, 2018;Ho et al., 2020). The inclusion of \"sloppy\" details marginally improves the adaptation of the model to the experimental data on which it is built, but at the expense of its generalization capacity.\nIt is possible to see the analogy with individual reasoning: an individual who excessively relies on an analytical approach, collecting as much data as possible and analyzing all details for their decision, will spend lots of energy in this effort and not necessarily select the best possible action. Being too analytical could lead to focusing specifically on details and losing the ability to consider the general purpose of the problem.\nIn other words, it is indeed true that knowledge originates from both deduction (move from the general to the specific), and induction (move from the specific, eventually many different specifics, to the general), but also strongly relies on abduction (Figueiredo, 2021). Abduction is the process of reasoning that goes from \"the particular\" to the general, but \"the particular\" is not a huge collection of data prepared to be analyzed by statistics, algorithms, and models. In abduction, \"this particular\" is a small set of specific and significant data that anchors our reasoning. It is the process used by doctors when they diagnose (Bird, 2010), by detectives when they try to resolve a criminal case, and by experts when they work. A paradigmatic example of the power of abductive reasoning in contrast to fully quantitative methods grounded in machine-learning is presented by Beaulieu-Jones, who shows the evidence of clinically driven decisions, based on heuristics approaches emerging from the personal expertise of the clinicians (Beaulieu-Jones et al., 2021).\nFrom our point of view, the decision-maker who relies too much on the actual data can incur the dilemma of deciding based only on a specific sample of a much wider \"population\", losing generalization power and possibly leading to a worse prediction and thus worse decisions, or even to inaction (Diaconis and Mazur, 2003). We can imagine a kind of threshold beyond which the logical and analytical approach of collecting and analyzing data becomes counterproductive. This happens because we stop considering properties \"common\" to a certain class of problems and start to model the singularities of the particular reference set that have no equivalent outside the narrow scope from which the data derives (Transtrum et al., 2015).\nBeing aware of this phenomenon could change our approach to the decision in some way: mitigation of the previously described drawback could derive from the adoption of a behavior that not only relies on analytical and logical reasoning built on collected information, but also considers other subjective components. Although these aspects are difficult to express, extract or demonstrate with objective data, they could, in some way, be formalizable which is one of the novelties introduced by this work.\n\nFoundational elements for a mathematical model of awareness\nThe development of a mathematical model of awareness can be addressed by starting from different perspectives and considerations (Friston, 2010). In this study we focused on philosophical, logical, cognitive and behavioral aspects; we aim to formally identify information flows and learning mechanisms that can allow individuals to initiate a process of increasing awareness instead of focusing on measuring conscious states and processes at the neural level (Modica and Rustichini, 1994;Heifetz et al., 2013;Karni and Vierø, 2017;Halpern and Piermont, 2020).\nThe first interesting contribution to the definition of a model of awareness can be found in philosophy and logic. According to Modica and Rustichini, a subject is certain of something when they know if it is true or false and uncertain when they know not its truth-value, and the subject's awareness of this not knowing is \"conscious\" uncertainty. On the other hand, a subject is unaware of something when they know not its truth value and is incognizant of their not knowing-they cannot perceive the object of knowledge, they are unable to mentally grasp it (Modica and Rustichini, 1999). The authors define awareness as the opposite of unawareness. Awareness includes both certainty and uncertainty, claiming that the concept of unawareness is a source of \"ignorance\" and distinct from uncertainty. They then propose the axiom of symmetry requirement for awareness: an individual can be aware of a proposition ϕ if and only if they are aware of its negation (not-ϕ). In other words, ϕ and ¬ϕ (not-ϕ) are either perceived together or not at all. A logical definition of awareness is given in terms of knowledge: assuming Aϕ means \"the individual is aware of the propositional ϕ\" and Kϕ \"the individual knows ϕ\", the logical formulation of awareness is: which is PL-equivalent to: where PL stands for propositional logic. A subject who is aware of fewer things than another is not at all necessarily less capable of logical reasoning, those are two separate concepts. Nevertheless, it is possible the individual may not be capable of making some deductions precisely due to their unawareness: for example, if they are unaware of q they will not be able to deduce the concept of \"p implies q\" from knowledge of p, which would otherwise be doable to one who is aware of q. Therefore, awareness can improve people's decisions.\nThese aspects are relevant when choosing the right model class to characterize the processes that lead to increasing awareness-in our research that being the Markov Decision Process which shall be described in the model section. Notably, it assumes that increased awareness is the result of personal effort and clear focus with this specific objective in mind. Further, becoming aware reduces ignorance even if it does not reduce uncertainty, something intrinsically considered in the stochasticity of the model.\nAwareness is a term that is often interchangeably used in different contexts, however, the literature dealing with awareness can be organized around three core concepts (Carden et al., 2022). The first is cognitive awareness (Papaleontiou-Louca, 2003) which identifies awareness as an accurate and deep understanding of an individual's perception, thinking, and actions. The second perspective argues that awareness exists on the multiple levels of consciousness and unconsciousness. Here, we consider awareness as an end-stage experience that results from the filtering and processing of several possible experiences happening simultaneously in our bodies and brains (Vaneechoutte, 2000). The third definition considers awareness with regard to recognizing others' feelings (Beck et al., 2004) and taking into account one's impact on others.\nFurther, whilst dealing with an individual's self-awareness, which is a relevant component of the developed model, two typologies are identified in the literature: \"intra\" and \"inter\" personal self-awareness (Carden et al., 2022). These are linked to an individual's own internal state and their impact on others, respectively, which can be further broken down into seven different components.\n(  (7) Other Perception Corresponds to how an Individual Is Perceived by Others, and the Ability to \"Receive\" Feedback. In Conclusion Self-Awareness can be Defined as: Self-awareness consists of a range of components, which can be developed through focus, evaluation, and feedback, and provides an individual with an awareness of their internal state (emotions, cognitions, physiological responses) that drives their behaviors (beliefs, values, and motivations) and an awareness of how this impacts and influences others.\nIn developing a mathematical model, these findings allow self-awareness as a fundamental factor of awareness to be built in, including emotions, cognitive processes, motivations, believes, evaluations and feedback. All these elements can easily be embedded into a Markov Decision Process.\nAll the factors outlined above have a heavy impact on many other aspects of our social life. The ability to use tacit knowledge and intuition is common and necessary anytime people need to make decisions in complex environments that are future-oriented, highly uncertain, difficult to forecast and lacking in information (Mintzberg, 2000). For example, in the field of leadership and management, rational decisionmaking skills are required to enable processing available information clearly and logically and thus permitting accurate perception and interpretation of the incoming events, which can sometimes lead to creative and innovative solutions (Prietula and Simon, 1989). Nevertheless, apart from this type of knowledge is essential to consider that managers routinely make decisions based on knowledge grounded in experience and could use intuitive decision strategies, especially under high-stress conditions (Sayegh et al., 2004). Tacit knowledge-the work-related practical know-how acquired through direct experience and instrumental in achieving goals important to the holder (Brockmann and Anthony, 2002)-is not easily recognized or acknowledged, but it can be a key factor in enhancing the quality of strategic decisions.\nThe sensible application of tacit knowledge can partially fill information gaps ameliorating the efficiency of the decision process (Brockmann and Anthony, 1998). Further, selfawareness is now seen as a critical component in leadership and career success, joining the skills of team interaction development, effective coordination and collaboration (Dierdorff and Rubin, 2015), and non-conflictual and sensible leadership (Axelrod, 2005).\n\nDeveloping a mathematical model of awareness\nIn this section we introduce a mathematical model that can incorporate the main factors summarized in the previous sections. According to the statements discussed in Section Decision-making processes: an overview, we have applied the class of models referred to as Markov Decision Processes, which have been deemed suitable to describe human decision-making under uncertainty (Rangel et al., 2008), including autopiloted decisions (Landry et al., 2021) and addictive behavior (Mocenni et al., 2019).\n\nSequential decision models and Markov Decision Processes\nEach day people make many decisions that have both immediate and long-term consequences. Decisions must not be made in isolation, today's decisions impact tomorrow's and tomorrow's the day after; if one does not account for the relationship between present and future decisions and present and future outcomes, one may not achieve overall good performance. The Sequential Decision Models (SDMs) consider both outcomes of current decisions and future decision-making opportunities under some kind of uncertainty (Puterman, 2014).\nIn a sequential decision-making model at a specified point in time (that is also called \"decision epoch\") the Decision-maker (DM) observes the current state of the system, and based on this state, chooses an action among the ones available in that state. The choice produces two results: the DM receives an immediate reward (or incurs a cost) and the system evolves to a possibly new state in the next decision epoch. At this subsequent point in time, the DM faces a similar problem as schematized in Figure 2.\nThe key ingredients of this sequential decision model are: • A set of decision epochs; • A set of states; • A set of available actions (which can be different in different states); • A reward (or cost) function depending on state and action; • A set of state transition probabilities depending on state and action.\nWe usually assume that the DM knows all these elements at the time of each decision, thus they constitute the amount of explicit information available.\nAt each decision epoch, the DM performs a choice, and they can have a policy that provides the most favorable action in each possible state; the implementation of a policy generates a sequence of rewards (or costs). The sequential decision problem consists in finding a policy before the first decision epoch, which maximizes a function of the rewards' sequence (or minimizes the costs' sequence). A policy that accomplishes this task is defined as optimal and relative to the specific individual and function considered.\nMarkov Decision Processes (MDPs) are a particular class of SDMs in which actions, rewards, and transition probabilities solely depend on the current state, and not on occupied states and actions chosen in the past. In other words, the current state incorporates the entirety of the DM's past: it is the result of all their previous decisions, related outcomes, and experience gained from them. In this work, we try to characterize some dynamics of the awareness-raising process. Therefore, we assume awareness is a dynamic process (characterized by a sequence of states with a certain dynamic in time) involving the DM's experiences, filtered by his perspective, beliefs, values, actual state, and choices, through different reasoning attitudes. Moreover, in an MDP, we simultaneously have the presence of a decision-maker's choice and uncertainty regarding its outcomes, as always happens with our decisions due to uncontrollable factors.\nThis model should be a good trade-off between realism and simplicity: broad enough to account for realistic sequential decision-making problems while simple enough to allow it to be understood and applied by different kinds of practitioners.\nIt is worth stressing that MDP, even if incorporates in a given state s t the DM's entire past, represents a future evolution uniquely dependent on the current state and is completely independent of the particular trajectory that reaches state s t at time t. Only a posteriori reflection by a DM on the trajectory of past experience builds (in the long run) both awareness and tacit knowledge. Here we recognize the presence of two dynamics with very different time scales. The short timescale of MDP, .\n/fpsyg. . ending up in the terminal decision, and the long timescale (we can think of the physical analogy of a capacitor) that generates both awareness and tacit knowledge by reflecting on past stories of successes and failures that can, in turn, be of use for future decision processes.\n\nModel formulation Time and state\nAs mentioned above, the adopted model belongs to the framework of Markov Decision Processes (MDPs) and considers a discrete and finite time horizon of length T in which, to each time-epoch, t, corresponds to a moment of making a relevant decision-which needs some kind of reasoning process and is not purely automatic and routine. Since the life of an individual is limited, it is reasonable to consider a finite time horizon. The state, s t ∈ (0,1), of the individual is a representation of their level of awareness at each time-epoch t and belongs to the set S represented by the discrete closed interval (0,1) with a step size of 0.01. In this way, a unique definition of awareness is not explicitly given, which would be a very difficult task, it is simply said that awareness is a state of the individual which determines their well-being from a global point of view and has a considerable impact on their choices. This paper is mainly focused on discussing how awareness can be accounted for by a mathematical model more than giving an explicit definition of it. It introduces an attempt to model some mechanisms underlying the process of aware decision-making rather than quantifying the effective awareness of individuals in some way, which could be a herculean task. By considering an MDP, the current state incorporates the DM's complete history so that their awareness is a state, in some way, embodying all of the individual's past: from their personality, values, beliefs developed over their lifetime, to their education and past experiences.\n\nReasoning propensity\nThe reasoning propensity, p r ∈ (0,1) embeds the specific attitude in processing the information about the problem, and represents the trade-off between the two reasoning modalities: analytical and intuitive. This combination depends upon different individual factors like age, character, beliefs, values, desires, education, experience, and so on; it varies from individual to individual but can also change in the same individual throughout their lifetime. The reasoning propensity takes values in a continuum between the two extreme attitudes (Allinson and Hayes, 1996), called intuitive (p r = 0) and analytical (p r = 1), assuming in this way that both are always involved, to different degrees, in any decision. These two modalities refer to the dichotomy between rationality and intuition, as Section Decision-making processes: an overview brings to light.\n\nPolicy and decision\nThe reasoning propensity affects the policy, µ of the individual. Generally speaking, a policy is a function that prescribes the action to make for each possible state at any time instant, and is represented by a matrix of dimensions [|S| x T]. It can be somewhat complicated to shape different situations. Therefore, the policy turns out in the decision, u t , which belongs to the open interval U = (0,1), so that the more analytical the choice, the higher the value of u t . We have that: The choice leads to two results: the DM receives a reward, and the system possibly evolves to a new state.\n\nState evolution and transition probability functions\nThe DM's state, s t , evolves according to: That is, the future level of awareness of the individual depends on the current state, the choices they mak e, and its outcome, which is subject to some uncertainty represented by w t , the stochastic variable related to a state transition. We assume, for simplicity, that the state can remain the same or increase and decrease by only one step, in this way w t belongs to the set W = {1, 0, −1}, indicating, respectively, the possibility that the state increases, remains constant or decreases by making a decision u t . The presence of uncertainty affecting the outcomes of the decision due to uncontrollable elements in the environment makes the state evolution and the rewards sequence stochastic. There exists a known transition probability, function of u t , specified in a matrix P of dimensions [|U| x 3]. In particular, P has the form: Each one of the three columns of P specifies the probability that the state increases, remains constant, and decreases, respectively. In other words: • w t = 1 with probability P 1 (u t ) → Forward probability • w t = 0 with probability P 0 (u t ) → Stationary probability • w t = −1 with probability P −1 (u t ) → Backward probability In this way, the system dynamics can be re-written as: Notice that all the elements in the matrix P are values representing a probability, and are subject to two conditions: The stationary probability P 0 has been defined as a constant value, notably all simulations consider: It incorporates the DM's resistance to change their level of awareness, and depending on their characteristics, can be considered \"inertia\". The forward probability results from the linear combination of two fixed functions exploiting the cases of intuitive and analytical reasoning. Figure 3 shows the functions considered for the forward probability in the-only theoretical-cases of a complete analytical ( Figure 3A) and a complete intuitive ( Figure 3B) individual. The first one starts with a low value and then increases as the decision becomes more analytical. It reaches the maximum when the reasoning is highly analytical, and then, for bigger values of u t , the probability starts to decrease. This is a representation of the overfitting phenomenon described in Section Decision-making processes: an overview, exploiting the fact that an excessively analytical approach to reasoning could also turn out to be counterproductive. The second function has an opposite behavior: the more intuitive the reasoning (the smaller u t ), the bigger the probability of increasing the state. This is because the individual thinks to have access to personal abilities, not related to cognition, allowing their level of awareness to increase by using an appropriate degree of intuition; this has to do with the personal confidence in tacit knowledge. It is possible to consider that a minimum level of analyticity is indispensable to understand the framework of the decision in this case; otherwise, intuition loses contact with the reality of the decisional problem, becoming only a fantasy. An excessively intuitive individual may act without considering the context from which the decisions come, and the decision could be ineffective. We must note that all these transition functions reflect the DM's different points of view; we are putting ourselves in the shoes of the individual. It is very difficult to consider a transition probability function that generically specifies the probability of increasing the state of individual awareness without depending on any such kind of assumption.\nThe two basic functions ( Figures 3A,B) have been designed so as to represent the different theoretical assumptions exposed in Section Decision-making processes: an overview, including the drawbacks of being excessively analytical or intuitive. Certainly, different functions can be considered as long as they are capable to incorporate the same phenomena.\nAs we mentioned above, any real DM mixes, to some extent, these two modalities according to a personal proportion represented by their reasoning propensity p r . The effective forward transition probability of the DM, i.e., the probability of increasing the state's level, is computed as the convex combination of the two fixed functions-forward transition probabilities in the only theoretical case of complete analyticity or intuitiveness of the individual-using the reasoning propensity p r as coefficient: Figure 3C shows the influence of different values of p r on the transition probability P, as described in the legend.\nFinally, the backward probability is defined starting from the first two as:\n\nRewards\nThe problem now arises of how to define a function that grants the individual a reward by selecting choice (u t ) instead of another and being in a certain state s t . Is it important to maintain the focus on what are we trying to explain with the model; that is: investigate the dynamic underlying the process of awareness-raising. In fact, as exposed in Section Decisionmaking processes: an overview, the dynamic of awarenessraising emerges from personal effort and motivation. Moreover, as human agents, we are accustomed to operating with rewards that are so sparse we only experience them once or twice in a lifetime, if at all. Most goals of modern life-a good job, a house, a family, a happy life-are so abstract, complex, and far into the future that they do not provide useful reinforcement signals. Despite this, people continuously make choices in their lives, applying what psychologists call intrinsic motivation or curiosity. Motivation/curiosity have been used to explain the need to explore the environment and discover novel states. Similar to what also happens in reinforcement learning, intrinsic motivation/rewards become critical whenever extrinsic rewards are sparse (Pathak et al., 2017). In our case, intrinsic motivation refers to reaching higher states of individual awareness, which can be linked to reaching sparse, temporary, distant and extrinsic life goals.\nMathematically the reward function consists of two parts: a stage reward explicitly depending on state and choice, and implicitly on the stochastic variable w t . The second is a fixed terminal reward, r(s T ), which the DM incurs at the last timeepoch T. The stage reward linearly depends on the current state and the choice, with constant and positive coefficients α and β: r (s t , u t ) = αs t − βu t Frontiers in Psychology frontiersin.org . /fpsyg. . It is reasonable to assume that the individual's current level of awareness has a positive influence on an individual's whole life, so living with a higher level of awareness can improve well-being on all levels (physical, psychological, emotional, and so on). Consequently, the equation incorporates a positive dependence on the current level of awareness so that the higher it is, the better the individual's life is overall. On the other hand, rational/analytical reasoning is resource-consuming because it requires the acquisition of some kind of data about the problem and the possible alternative solutions, and requires time to analyze and elaborate all the data. Intuitive reasoning, as also revealed in Pascal's thought, is effortless and not resourceconsuming. Therefore, the more the decision implies analytical reasoning the more resources it needs in terms of time, personal energy, and monetary resources. This translates into a negative dependence of the reward on the choice u t , because the higher u t is the more analytical the reasoning of the DM, and so the more resources consumed. Although the current version of the model assumes a linear form for the step-reward function, to explain the thoughts behind its formulation in a simple way other typologies functions are possible and may be more suitable.\nFor the same reasons set out in the previous point, we can assume that the terminal reward the DM incurs at time T increases with the value of the ending state (Figure 4). It has been considered an exponential function that \"tries to push\" the final state as high as possible, providing a considerably different reward between ending in a \"high\" state rather than in a \"low\" state.\nHere we can also notice how tacit knowledge can be thought of as deriving from the sedimentation of past cases into an \"experience capacitor\". The terminal rewards derived from past cases lose their specific reference to the actual situation from which they stemmed, and contribute to the creation of a \"good practices\" repository no longer linked to a particular situation but a \"broad spectrum of cases\".\n\nFuture weights\nEqual rewards at different time-instants have a different value for the individual. Therefore, factor δ weigh ing future rewards has been introduced. Different applications and aspects referred to this consideration are studied in detail in Section A model extension: Including individual emotions.\n\nBackward penalty\nIn the end, we considered a vector γ of dimensions [3 x 1] to give possible different weights to the cases of increasing, maintaining constant, or decreasing the state, respectively. We assume different values of γ in different simulations, .\n/fpsyg. . accounting for different attitudes of the DM-for example strongly penalizing the eventuality of decreasing awareness.\n\nHabitual decisions\nOnce the general structure of the model and the meaning of its parameters are defined, it is possible to consider how the resulting model can be applied to different situations shaped through different policies. In the following sections we present two ways to find suitable policies aimed at solving the decision problem. The first, proposed in this paragraph, refers to the most basic and simplest mechanism governing an individual's habitual decisions (or choices), the ones made with little to no effort and without conscious control (Landry et al., 2021). They consequently assume that the DM does not have any selfawareness, so that decisions spring only from their habits as automatic, non-conscious mechanisms.\nAs mentioned in the previous section, the DM has their own reasoning propensity p r indicating how much the decisionmaking process is intuitive or analytical. It is possible to assume that this is the only characteristic governing habitual decisions, considering a naïve policy, that is accordingly called habitual or usual policy, defined as: µ (s t , t) = e t ∀s t ∈ S and t = 1, . . . , T Where e t is normally distributed with mean p r and standard deviation σ fixed to a constant value, for example 0.3, represents the fact that any individual's decision always encompasses the processing of information regarding a problem in a similar way, more or less analytically. However, a certain variation in the choices has been considered around the value representing the propensity of reasoning, supplied by other uncontrollable contextual factors which are the real drivers of the decision, making the individual unaware of being able to effectively decide the value to choose. These factors represent a source of uncertainty, influencing the choice, and can drive it far from the effective p r, highlighting the case in which people are not synchronized with their effective reasoning propensity.\nUltimately, this policy's structure shapes the case of the DM's unaware decisions. It is possible to see some conceptual similarities to the UMDPs (Unaware Markov Decision Processes) which represent an attempt to introduce the concept of unawareness in the framework of Markov processes (Halpern et al., 2010). A common idea is the restriction of the set of possible actions, even if implemented in different ways, reflecting the unawareness of the DM regarding an entire set of possible actions. In the policy described above, beyond this kind of unawareness, the individual is also unaware of their effective reasoning propensity, assuming that the effective choice randomly selects a value around it. In this way, the unaware choices are not completely random but reflect a kind of coherence of the individual and, on the other hand, shape an unawareness of what is the real p r .\nIn this work, this structure is mainly applied as a term of comparison to evaluate the effect of introducing an individual's self-awareness on the choices.\n\nSelf-aware decisions\nThe second structure that is proposed represents a first attempt to incorporate the concept of self-awareness in the process. If we think about self-awareness, we could imagine that it is an element deriving from some kind of self-observation-a \"third person\" perspective from a metacognitive point of view -and that has a consequent impact on the action/decision. Mathematically it can be represented by a feedback loop, according to the logical representation of Figure 5.\nAccordingly, a component that modifies the policy has been introduced by additionally observing the form of the transition probability function, current state, and time epoch. In this way, the DM is allowed to modify their usual, automatic process by shifting from their habitual to a new policy that mathematically results from a maximization process. This introduces the possibility of mitigating the habitual tendencies of the individual by modifying the policy.\nThis feedback is mathematically embedded in an optimization process, intended to maximize the sequence of rewards. Due to the linear dependence of the reward on the level of awareness, it is also thus modeling the fact that self-awareness results from a personal effort.\n\nComputation of the optimal policy\nAs previously mentioned, a policy is optimal when it maximizes a certain objective function which, in this case, is the cumulative sum of the rewards incurred at each time epoch. One of the methods that can be applied to compute the optimal policy in an MDP is the Stochastic Dynamic Programming (SDP) algorithm, choosing the action which maximizes the sum of the current reward and the expected future rewards at each stage. Mathematically we can say that the following problem must be solved: Considering a finite time horizon of length T, a decision is not made at time T, so that the DM's last choice is at time T-1, and the final time instant is used to fix a terminal reward the DM incurs at time T, r(s T ). From there it is possible to recursively reconstruct the optimal policy by exploiting Bellman's Principle of Optimality (Bellman and Drayfus, 2015) which affirms that \"an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting for the first decision\". The original problem can be decomposed into a recursive series of easier sub-problems, considering a shorter time horizon from τ to T and a given initial state : where V τ (s) is the value function that indicates the optimal reward cumulatively obtained considering the sub-problem with time horizon τ , . . . , T and initial state. Starting with τ = T-1 and then decreasing the value of one unit each time, it is possible to recursively calculate the optimal policy for which the current optimal value can be seen as the sum of the expected stage reward and the expected optimal value function at the next time instant: This expression in our case can be expanded considering that w t can assume three values with probabilities specified in matrix P. So, it becomes: V τ (s) = r (s, µ (s, τ )) + δ γ 1 V s + 1)P(µ (s, τ ) , 1 + γ 2 V s)P(µ (s, τ ) , 2 + γ 3 V s − 1)P(µ (s, τ ) , 3 , where 0<δ<1 is the weight given to the next-instant reward, and γ = [ γ 1 , γ 2 , γ 3 ] is the vector of the different weights given to the possibility of increasing, remaining constant, and decreasing the next state, respectively. Each constant coefficient γ i belongs to (0,1).\nIt is worth remembering that the transition probabilities explained in matrix P depend on the reasoning propensity p r of the individual, and this determines the effective shape of the curve representing forward, stationary, and backward probabilities as a function of the decision (u t ) suggested by the policy.\n\nA model extension: Including individual emotions\nImmediate emotions (also called visceral factors in economics) play a critical role in the intertemporal choice modifying the utility of an action, leading people to behave in ways that appear to greatly discount the future, ways that individuals themselves can sometimes see as contrary to their own self-interest (Loewenstein, 2000). .\n/fpsyg. . In this description, we can identify three basic aspects that could help model emotions: their relationship with time, with perception (utility) or a reward, and the fact that they could also be counterproductive. In the proposed model the ideal place to insert emotions seems to be in factor δ that weights future rewards. It permits connecting immediate emotions to the perception of the future and the value of the rewards.\nWe especially claim that emotions do not necessarily hurt an individual's choice but can also \"help\" them. We can equate emotions to the role played by \"temperature\" in simulated annealing optimization models (Bertsimas and Tsitsiklis, 1993); in order to escape eventual local minima during the optimization process the simulated annealing algorithms allow for a certain degree of stochasticity that could inhibit the system to take the \"most convenient move\" during the optimization process. This mirrors the role of temperature that can make the system exit from a potential hole, the temperature (i.e. the degree of stochasticity) decreases during the process and goes to a minimum near the end of the process so as to not destroy the reach of the optimal solution. It is worth noting that, at odds with simulated annealing, our model does not incorporate an explicit decreasing trend of temperature (contribution of emotion) but an equivalent effect is reached by introducing a dependence on awareness's dynamic along the process that in turn can make the emotions somewhat less relevant.\nThe entity of the role played by emotions depends on the level of awareness of the individual. At a low level of awareness, emotions prevail on individual reasoning, so one is completely driven by choices in search of instant gratification (independent of the reach of the actual target). In this condition, the future will have very little weight on one's choice, which can be detrimental because people behave in a way that is contrary to their self-interest. Contrarily, this dynamic is not present when the individual reaches a high level of awareness in which one could consider emotions freely and peacefully, and could, in some way enhance a benefit from the choice.\nAnother aspect to considered in the computation of δ is the relationship between future weight of choice and the age of the individual, in such a way that the older the individual, the bigger the weight they give to future rewards. An older individual with less time to live consequently gives more importance to each possible moment in the future; in contrast, a younger individual could weigh the present with less consideration of the future.\nMathematically, this additional extension changes the equation defined in the paragraph regarding the computation of the optimal policy, which becomes: with time horizon τ , . . . , T and an initial (known) state s. The term δ (s) is introduced in the model to insert an emotional component. It indicates a modification of the structure of δ which, until now, was a constant value but is now considered as a function of the state (see Figure 7A). Moreover, it reports a linear dependence on time t, where the term 1 T is a scale factor. Summarizing, the new term embeds the impact of emotions in the decisions as a factor which enforces the expected value of the future reward when either the awareness level increases, or the time horizon reaches its maximum, or both.\n\nNumerical results\nThe next step was to carry out numerical simulations to apply the different structures corresponding to the habitual and self-aware policies outlined in the previous section in order to evaluate the evolution of the dynamic. To do this, it must be also specified: • The number of simulations, N, to perform. Each of them with a time horizon of length T. • An initial state s 0 for each simulation. It can be fixed to a particular value to evaluate the dynamic starting with a specific level of the state or can be computed as a random value extracted from a discrete, uniform distribution that takes values from 0 to 1. • Notice that t 0 and s 0 are related to the instant when the observation starts, they are not intended as absolute values yet always have a relative connotation. • For each time instant in each simulation we need a realization of the random transition variable w t which takes values in W = {1, 0, −1} according to the probability functions in P evaluated at u t ; in fact, the DM implements, at each time, a choice according to the policy µ they choose (u t = µ(s t ,t)).\nPerforming N = 3,000 simulations and analyzing the average value in all of them, we can see what happens to the trends of state and rewards. Some numerical results are reported in Figure 6, obtained by considering an individual with p r = 0.6. The habitual policy for such a kind of individual is constantly centered around 0.6 with, in this case, low noise. It means that the choice of the individual always has roughly 60 percent analyticity of reasoning. From the optimal policy's matrix ( Figure 6A), one can see that the optimal policy suggests starting with a low level of u t and then increase it to 0.8 ( Figure 6C). The lack of observation of transition probabilities and the level of state creates, in the habitual case, a decreased rise in state, which at ending time reaches 0.5, whereas with the introduction of feedback the state is able to saturate near the maximum state ( Figure 6D). We have chosen this particular case to discuss in light of the phenomenon that the state initially decreases in the presence of feedback. It is generally possible to notice that the state with the feedback loop monotonously increases and is higher with respect to the other. These results are explained in our first publication on that topic (Bizzarri and Mocenni, 2022), where the embryonic idea of comparing habitual and optimal strategies in human decision-making has been presented, while the mathematical model, including the concept of overfitting, tacit knowledge and emotion, have been .\n/fpsyg. . introduced in the present paper for the first time. However, by considering a different parameter setup we can notice that even if the state temporarily decreases, the presence of a feedback loop allows for a change in the trend, reaching values that are even higher than in the case of habitual behavior. The step reward has a similar trend with an initial decrease and then a more rapid increase than in the habitual case ( Figure 6B-blue line).\n\nIncluding emotions in the model\nIt is possible to evaluate the impact of embedding emotions of the individual by performing some simulations and correspondingly modifying the computation of the optimal policy in the presence of a feedback loop, as exposed in Section 4.5.\nIt is possible to observe that a highly analytical individual starting from a low state manifests a decrease of the state in the presence of an emotional factor, as described in Figure 7B. This is due to the new form of δ(s t ) ( Figure 7A), which has a negative value for a low state of awareness, claiming that in this case the presence of emotions greatly effects future discounting which could also turn out as harmful (in case of negative values of δ). On the contrary, after gaining a certain level of awareness, δ starts to increase reaching a value near 1, meaning that an individual with a high level of awareness does not make any distinction between the present and future. Figure 7C demonstrates the possibility that emotions could enhance the state evolution: in this case, emotions are helpful in increasing the state evolution over transient times, until it stabilizes at a constant value (s t ∼ 0.7). This behavior appears when considering a longer time horizon, where T is set at 300.\nThe helpful effect of emotions starting from a high s 0 is more evident in the following when considering an intuitive individual ( Figure 8C).\nIn the case of a highly intuitive individual, the additional emotive aspect creates an oscillatory behavior when considered at a lower initial state ( Figures 8A,B). This oscillation makes it impossible to choose a stable value of u t , which oscillates between high and low values. Consequently, it stops the growth of the state.\nAs also highlighted in the analytical case, the presence of emotions at high states enhances the evolution of the state that increases faster than without emotions (see Panel C, where the red line is over the black one).\n\nDiscussion\nThe aim of this research was to investigate how to integrate facets like tacit knowledge, intuition, emotions, awareness, .\n\nFIGURE\nIntuitive individual with emotions. Now we shall consider an individual with p r = . , analyzing, as before, the trends due to emotion starting from a low initial state. (A, B) analyze the e ect of emotions starting low (s = . ). (A) reports the optimal policy whereas (B) demonstrates the state evolution and, in the insert, the e ective choice over time. (C) considers a high (s = . ) initial state and shows the state dynamic and, in the insert, the e cient choice. The red line indicates behavior in the presence of emotion whereas the black line in absence of emotion (considering a constant δ = . ). In both cases the feedback loop is included, modeling self-aware decisions. The other model parameters are the same used in Figure . and self-awareness into a mathematical model of decisionmaking, going beyond the classical analytical perspective. These factors have been considered within the framework of a richer conception of the decision-maker, and their multi-faceted effects are intensely analyzed. Even though all of them have been studied and described from a theoretical point of view in different fields of investigation, a modeling formalization is still missing, and this is the novelty that the present work introduces: the possibility to incorporate all these different aspects into a model of decision-making. A still very primitive framework has been proposed allowing the integration of non-analytical factors into a coherent frame. We achieve such integration by taking into account qualitative definitions of non-analytical factors that stem from different fields of investigations, and quantifying them within the framework of a generalized Markov Model decision process. In this context, the importance of a modeling approach resides in its capacity to focus on the principal and essential factors involved in a process, in this case of decision-making, concisely and practically describing each of them and meanwhile maintaining an overview on the entire phenomenon. We hope that this initial step could lead to further exploration, and deepen each aspect to increase the model details, such as interaction with others, which some preliminary results have been already founded by the authors.\nThis study does not have a specific psychological connotation, instead it attempts to integrate current cognitive psychology research with the more varied-and inherently uncertain-outcomes of human decision-making and could contribute to the introduction of new aspects, expanding research in this field, such as self-observation and the ensuing emotion, and the use of unexplicit information in the decision. The psychological (and philosophical) dimensions of awareness were, in turn, deeply investigated by  in terms of metacognition by stressing the need for a reflexive act in which the decision maker acts as a \"third person\", retrospectively evaluating their previous strategies and consequently building up a \"tacit knowledge reservoir\". It is not without consequence that the authors insert one of the basic pillars of metacognition, \"the internalized knowledge that awakens and drives humans towards independence and the fulfillment of each one's potential\" .\nThere is a large consensus about the presence of two distinct mechanisms in order to tackle the relationship between information and the decision process that we have called intuitive and analytical; which here have been developed, suitably revisited and extended. First, the presence of the phenomenon of overfitting derives from excessive confidence in the analytical approach, which leads the individual to focus on the details of a specific sample that is part of a much Frontiers in Psychology frontiersin.org . /fpsyg. .\nwider \"population\", losing generalization power and potentially moving towards poor predictions and thus poor decisions. We could imagine a kind of threshold beyond which the logical and analytical approach of collecting and analyzing data becomes disadvantageous. Indeed, beginning to model the singularities of the particular reference set that have no equivalent outside the narrow scope from which the data may prevent considering properties \"common\" to a certain class of problems. We mathematically formulate this phenomenon by introducing the forward probability transition of an analytical individual (see Figure 3B), claiming that after a certain threshold of u t , a further level of analyticity results as a decrease in the probability of reaching a higher value of awareness. On the other hand, the forward probability transition of an intuitive individual claims that the more intuitive the reasoning (the smaller u t ), the bigger the probability of increasing the state is until a given lower bound is reached. This happens because the individual thinks they have access to personal abilities, distinct from cognition, allowing the level of awareness to increase by using a kind of unexplicit acquaintance related to tacit knowledge. Thus, the idea expressed by Pascal's esprit de finesse, an effortless ability available to each individual but often unknown, is accounted for by our model. Tacit knowledge is inherently difficult to express, extract or demonstrate with objective data but, despite these setbacks, it could possibly be formalized which is one of the novelties introduced in this work. The model questions a purely analytical \"one-size-fitsall\" approach, stressing the importance of considering the uniqueness of each single individual who could in any case autonomously change their habits thanks to the implementation of a kind of self-observation mechanism, and recognizing the effectiveness of their personal and unique repository of tacit knowledge.\nMoreover, the specificity of an MDP allows bidirectional vision with a look to the future in the evaluation of the optimal choice at each time instant, and a retrospective reconstruction of the entire sequence of choices and the dynamics of the state enabling different perspectives of observation. Considering time an independent variable it is possible to observe the mechanisms by which the state evolves. The model does not provide a univocal definition of awareness, but rather considers it as the result of a series of processes, as described above, which can allow the individual to retrospectively observe the process that led them to be the person they are today.\nIn the end, interesting aspects arise from the introduction of emotions in the model. We have started from the consideration that emotions impact an individual's intertemporal choice, modifying perceived utility and leading people to behave in ways that seem to disregard the future, thus sometimes damaging the individual themselves. All these aspects are considered in the definition of weight δ that the DM attributes to future rewards. Typically, when included, emotions are evaluated as \"noise\" to avoid or minimize. The different point of view proposed in this work claims that emotions do not necessarily hurt an individual's choice, they can be \"helpful\". This depends on one's level of awareness, which can be considered as strictly related to the ability to manage and integrate emotions in decision-making, and in turn enhance the individual's awareness. From the simulations it is possible to appreciate the validity of the above considerations. Starting from a low initial level of awareness in both analytical and intuitive individuals ( Figures 7B, 8B), emotions have a damaging effect. The difference is that in the first case the state irrevocably decreases to minimal one, whereas in the second it stopped at a local value without increasing anymore. The analytical case can be interpreted as the typical idea that emotions disturb choice, but, in our model this is only true when considering low states of awareness. At low states the analytical individual is not able to relate with and manage emotions, and this reflects their state decreasing to zero. In the intuitive case, on the other hand, the state stops increasing due to the appearance of an oscillatory dynamic in choice, where the decisions oscillate from a low to a high value without maintaining a constant trend in time. Emotions create an unstable dynamic that does not permit constant and long-lasting decisions over time, resulting in a stationary state. The interesting aspect is that at high levels of awareness, these behaviors do not manifest, and indeed emotions can exert a beneficial influence (Figures 7B, 8C). This is more evident in the intuitive case, whereas in the analytical case they improve in the transient before the state stabilizes to a constant value. This is an indication that emotions are not necessarily a nuisance in the decision process.\nAnother relevant result has to do with the mathematical formalization of emotions that resonates with the concept of awareness, typical of oriental traditions as connected to the capacity of living in the \"present moment\", where the individual is focused on the present occurrence of experience without interference from past or anticipated images (Kang and Whittingham, 2010). It is not by chance that the exhortation of \"living in the present moment\" is shared by diverse philosophies, from monastic Christian (Merton, 2010) to mindfulness techniques (Carpenter et al., 2019). Living in the present moment does not mean to be prey to the search of immediate gratification (that means weighing the future with a negative delta), rather it corresponds to the capacity to give an equal weight to each instant (in our case having a delta that reaches one). This is exactly what happens in our model by increasing time and s t , thus becoming older and more aware.\n\nConclusions\nThis work incorporates essential drivers for human decisions, analyzing their reciprocal relations and influences into a model grounded in the Markov process. From the . /fpsyg. .\nanalytical/intuitive dichotomy to the inclusion of tacit knowledge and the impact of emotions, all the different facets of a decision have been discussed from both a theoretical and a mathematical point of view. Individual awareness emerges from the comparison between habitual strategies and the ones sprung from the addition of an individual self-awareness feedback, and its dynamic nature can be appreciated from an individual's retrospective observation. Moreover, the impact of emotions is re-thought with an explicit dependence on the level of awareness of the individual, so that their conception that emotion is a noise to be filtered is mitigated by the consideration that it is true at low state of awareness, and can thus be enhancing for aware individuals. From an epistemological point of view, our aim was to demonstrate how commonly first sight decisions are taken for granted (resonating in diffuse expressions like \"clinical eye\"), and cannot be considered as a purely \"emotional\" process opposing \"strictly analytical strategies\"; instead, they are the result of a long and largely tacit learning process. This concept was already present in the words of Blaise Pascal nearly 400 years ago but progressively forgotten by specialist literature.\nHere, we give a proof-of-concept of the possibility to insert this kind of knowledge into a mathematical model alongside the philosophical issues we think this result could help solve, from problems encountered by machine intelligence to facing problems relevant for biomedical applications (Gavrishchaka et al., 2019;Beaulieu-Jones et al., 2021). The limitations are the obvious and inherent ones in creating a mathematical model of such complex phenomena as human decisions and awareness are. Mathematically, modeling awareness is a herculean task, and the model will inevitably be \"sloppy\" due to the inability to enclose the immensity of human thought into a few functions that are, at best, a stimulus for a more realistic consideration of decision-making process.\nOne way to overcome the above limitations could be by testing the model in reality, for example developing surveys and designing experiments that can allow for the collection of estimations of the model's parameters and adapt the model to specific cases. A second crucial step forward in modelunderstanding is to also consider the presence of interactions among individuals. Taking a cue from some preliminary results obtained by the authors in this direction, there is a plan to investigate the effects on the decision process and awareness evolution introduced by interaction within a network of individuals and the different impacts due to the structure of the relationships.\n\nData availability statement\nThe original contributions presented in the study are included in the article, further inquiries can be directed to the corresponding author.\nAuthor contributions CM, AG, and FB: Conceptualization and writing. CM and FB: Mathematical modeling, simulation, and software development. All authors listed have made a substantial, direct, and intellectual contribution to the work and approved it for publication.\n\nFunding\nThis study was financially supported by University of Siena."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-09"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 54219,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "251461876"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Combinational Regularity Analysis (CORA) — a new method for uncovering complex causation in medical and health research\n\nBackground Modern configurational comparative methods (CCMs) of causal inference, such as Qualitative Comparative Analysis (QCA) and Coincidence Analysis (CNA), have started to make inroads into medical and health research over the last decade. At the same time, these methods remain unable to process data on multi-morbidity, a situation in which at least two chronic conditions are simultaneously present. Such data require the capability to analyze complex effects. Against a background of fast-growing numbers of patients with multi-morbid diagnoses, we present a new member of the family of CCMs with which multiple conditions and their complex conjunctions can be analyzed: Combinational Regularity Analysis (CORA). Methods The technical heart of CORA consists of algorithms that have originally been developed in electrical engineering for the analysis of multi-output switching circuits. We have adapted these algorithms for purposes of configurational data analysis. To demonstrate CORA, we provide several example applications, both with simulated and empirical data, by means of the eponymous software package CORA. Also included in CORA is the possibility to mine configurational data and to visualize results via logic diagrams. Results For simple single-condition analyses, CORA’s solution is identical with that of QCA or CNA. However, analyses of multiple conditions with CORA differ in important respects from analyses with QCA or CNA. Most importantly, CORA is presently the only configurational method able to simultaneously explain individual conditions as well as complex conjunctions of conditions. Conclusions Through CORA, problems of multi-morbidity in particular, and configurational analyses of complex effects in general, come into the analytical reach of CCMs. Future research aims to further broaden and enhance CORA’s capabilities for refining such analyses. Supplementary Information The online version contains supplementary material available at 10.1186/s12874-022-01800-9.\n\nPage 2 of 17 Thiem et al. BMC Medical Research Methodology (2022) 22:333 of these relations is defined by the so-called INUS Theory [6][7][8], which enjoys a long intellectual pedigree in the philosophy of causation [9,10] (an \"INUS condition\" is an insufficient but non-redundant part of an unnecessary but sufficient condition). For identifying INUS structures, which are represented in the formal language of propositional logic, CCMs employ optimization algorithms that operate on Boolean-algebraic functions [11][12][13][14].\nSince the late 2010s, CCMs have also started to make inroads into medical and health research. For example, QCA has been used in pediatrics to analyze the effect of social networks on lice infestation among Mexican children [15], in ophthalmology to investigate the association between dietary patterns and macular degeneration [16], and in obstetrics for studying the effect of family policies and public health initiatives on breastfeeding initiation [17]. According to a recent review, QCA has been used so far in at least 26 studies to analyze public health interventions [18]. CNA has been employed, for instance, to examine the strategies that Veteran Affairs sites use for implementing new hepatitis C treatments [19]. In addition, the method has recently been introduced in implementation science [20].\nAnother development that set in about 25 years ago in medical and health research concerns the increasing relevance of the concept of multi-morbidity, which is generally defined as the co-occurrence of at least two chronic or acute conditions in patients [21]. Many numerical indicators attest to the growing need of devoting attention to problems of multi-morbidity. According to one study, 65 percent of aged Medicaid beneficiaries in the United States had suffered from multiple chronic conditions towards the end of the 1990s already [22]. By the early 2010s, adults with multiple chronic conditions became the major users of health care services at all adult ages and accounted for more than two-thirds of health care spending [23], a trend that has been projected to persist [24]. Reflective of these developments, journals such as Health Psychology have also published special issues on topics around multi-morbidity [25]. In short, multi-morbidity represents a problem of growing concern in medical and health research. Consequentially, the need for analytical methods suitable for the scientific study of data from contexts of multi-morbidity continues to increase.\nExisting CCMs still face limitations in this connection. Neither QCA nor CNA offer the possibility to analyze complex effects, and neither method is thus able to correctly analyze multiple conditions and their possible conjunctions simultaneously. On the one hand, this should come as no surprise because the INUS Theory has so far focused on the complexity of causes, but not the complexity of effects. On the other hand, some prominent clinical psychologists had already pointed out the close connection between configurational thinking in terms of INUS causation and the intensifying problem of multimorbidity about twenty years ago [26]. An initiation of efforts to incorporate the notion of complex effects into current configurational methodology and the INUS Theory thus appears a long overdue and worthwhile undertaking.\nAgainst the background of fast-growing numbers of patients with multi-morbid diagnoses and the acknowledged yet untapped analytical potential of CCMs in this regard, the present article introduces a new method with which data from contexts of multi-morbidity can be modelled configurationally. We call this new method Combinational Regularity Analysis (CORA). In addition to its technical innovations that allow the simultaneous analysis of multiple conditions and their conjunctions under a coherent inferential framework, CORA introduces the possibility to mine configurational data and to visualize results by means of logic diagrams. The eponymous software package CORA [27] brings all these procedures together and makes them available for application by the scientific community.\n\nMethods\nIn the first subsection, we briefly revisit the state of the art in configurational data analysis with QCA and CNA, with an emphasis on the type of causal structures these methods are able to identify. Furthermore, we recapitulate basic inference requirements for CCMs. In this connection, we also discuss problems that currently arise when working with multiple effects. In the second subsection, we introduce the notion of the multi-output switching circuit and define all relevant concepts. For bridging the gap to configurational causal inference under the INUS Theory, we also translate these concepts into CCMs' language of propositional logic. This can be done with ease as propositional logic and switching circuit theory (and also set theory) are equivalent branches of the same underlying Boolean algebra (see [28] for a concise overview). In the third subsection, we present logic diagrams as a useful device for visualizing complex configurational cause-effect relations. In the fourth section, we briefly explain the data-mining feature of CORA. In the fifth and final section, the software package CORA is introduced.\n\nConfigurational State of the Art\nUS sociologists Kriss Drass and Charles Ragin have developed QCA in the mid-1980s [29,30]. By importing the so-called Quine-McCluskey algorithm (QMC) from electrical engineering into the social sciences, their major-yet initially unintended-accomplishment was to find a functional procedure that could operationalize the central ideas of the INUS Theory. As it turned out, the second phase of the two-phase protocol of QMC also solved the so-called \"Manchester-Factory-Hooters Problem\", which had stood in the way of a broader acceptance of the INUS Theory until then [14]. In this way, QCA has not only provided a new lease of life to the INUS Theory, which by that time had been marginalized in the literature on the philosophy of causation [31], but it has also reverberated more generally throughout the area of social research methodology [32,33]. Regardless of its early achievements in the social sciences, QCA has always remained restricted to the simple analysis of exactly one effect, usually called \"outcome\" in configurational parlance [34]. Although some tentative attempts at loosening this restriction have been made [35], the possibility that data may contain evidence for the existence of more than one outcome, not to mention the question of how such data could be adequately analyzed, has never been put on QCA's methodological agenda. This stagnation in the development of the method's analytical capabilities cannot be due to the fact that hardly any set of data features more than one possible outcome. In fact, many QCA studies have analyzed several distinct yet clearly co-occurring outcomes as part of the same set of data (e.g., [36][37][38][39][40]).\nCNA has attempted to relax the restriction to single outcomes from the beginning by adding an analytical step to those performed in QCA: for each outcome that the method has identified a possible solution for, called atomic solution formula (ASF), CNA seeks to conjunctively combine these formulae into a so-called complex solution formula (CSF). CSFs can take on the form of a causal-chain structure or a common-cause structure. In the former, at least one effect features as a cause to at least one other effect. In the latter, at least one cause features as a cause to at least two effects. Although its developers have emphasized that CNA is custom-built for analyzing causal structures with multiple outcomes [5], the method still operates within the same limits as QCA with regard to the complexity of effects. The option to analyze multiple outcomes clearly represents an advantage over QCA, but CNA continues to treat outcomes in complete isolation from each other. It does not allow for the possibility that effects-not only causes-may interact in complex ways.\nBesides clarifying the general structure of relations both QCA and CNA can identify-complex causes, simple effects-it is important to revisit the basic requirements for configurational causal inference. Under the INUS Theory, any potential cause must be a Boolean difference-maker to its effect: a cause must, at the very least and ceteris paribus, be a consistent concomitant of its effect while the absence of that cause must be a consistent concomitant of the absence of its effect [6]. If a candidate for a cause occurs, ceteris paribus, in conjunction with the analyzed effect as well as the absence of that effect, it can never be a difference-maker to that effect. If it is no difference-maker, it is redundant. Any causal explanation of an effect must therefore be functionally minimal, in the sense that all redundancies must have been eliminated beforehand. More specifically, every QCA solution and every ASF in CNA must be a Boolean expression representing a minimally necessary disjunction of minimally sufficient conjunctions in order to be causally interpretable [41,42]. Such a disjunction is then usually called a model. The process of Boolean optimization, which can be carried out in very different algorithmic ways [13], seeks to ensure the generation of such models.\nAfter having summarized the structure of causal relations QCA and CNA can identify and the general foundations of configurational causal inference, we next need to sensitize readers to a relatively unknown problem in multi-outcome analyses with CNA: the so-called \"causal-chain problem\" [43]. Although it has received virtually no shrift so far in the literature, a closer look turns out to be a perfect didactic stage setter for CORA. The gist of the problem is that no causal chain is ever strictly identifiable because every chain-type CSF can be transformed, by simple syntactical substitution, into an equivalent common-cause-type CSF that does not feature chain-type elements any longer. Put differently, it is impossible for CNA to ever unambiguously identify a causal chain. While disadvantageous, the non-identifiability of causal chains per se does not seem to create any deeper problems. Yet, what seems to be a minor inferential downside at first turns out, at closer inspection, to create major first-order disturbances for the requirement of functional minimality.\nAs an example of this problem, consider the causal chain identified by CNA in [44] in Expression 1 (for simplicity but without loss of generality, all complications which are of no relevance for the ensuing argument have been dropped): where the italicized letters l, t, s, x and m (and all italicized letters in the remainder of this article) stand for propositional variables taking on specific values (the substantive meaning of l, t, s, x and m is irrelevant), \" ′ \" symbolizes the logical concept \"not\", formally called negation, \" · \" stands for the logical concept \"and\", formally called conjunction, \" + \" for the logical concept \"or\", formally called disjunction, and \" ⇔ \" for the logical concept \"if, and only if, \", formally called equivalence. A literal is an occurrence of a propositional variable, either negated or not negated. As usual, in the remainder, we will drop the and-operator, \" · \", if no risk of confusion exists. In both QCA and CNA, a wide variety of other syntactical symbols and conventions is often used. In the remainder of this article, we stick to the above nomenclature in relation with the use of CCMs because of its compactness. As x features not only as an effect, but also as a cause of m in Expression 1, we can transform, by direct substitution of x in the ASF of m, the causal-chain CSF into the common-cause CSF shown in Expression 2: Both CSFs are also presented graphically in Fig. 1, the causal-chain CSF in panel (a), the equivalent commoncause CSF in panel (b). Black dots at the outgoing end of a line indicate negation, joining lines conjunction, and arrows (minimal) sufficiency. This substitution process, however, brings to light an obvious redundancy in the ASF of m in Expression 2, in consequence of which the CSF loses its causal interpretability. More precisely, literal t ′ is redundant, as proven in Expressions 3a to 3c: Instead of a formal demonstration of redundancy, one could also approach the problem from the perspective of configurational causal inference under the INUS Theory: in order to assign t ′ the status of a Boolean difference-maker in conjunction with l ′ , m must not occur in (2) by commutativity and distribution, = t + l � by identity.\nconjunction with l ′ t . However, if t alone is already sufficient for m, by extension, so must be l ′ t . Put differently, if t alone is inferred to be a cause of m, it is impossible to ever infer at the same time that t ′ is a cause of m in conjunction with l ′ . To ensure redundancy-freeness, CNA therefore eliminates t ′ from the ASF of m in the common-cause CSF in Expression 2, but does not further manipulate the corresponding chain CSF. Thus, the question arises whether such unwanted redundancies are an exclusive problem of common-cause CSFs. After all, it seems as if the problematic redundancy has been induced by the very process of substitution. That, however, is a false impression. In fact, the redundancy has already been present, albeit less obviously so, in the chain CSF. To prove this, there are several routes. One is to demonstrate that the original chain CSF in Expression 1 and the redundancy-affected common-cause CSF in Expression 2 are, in fact, strictly identical. We provide such a proof of identity in Additional file 1: Appendix.\nOver the following subsections, we argue that the indiscriminate elimination of all redundancies, as currently demanded in CNA, does not provide an adequate solution for restoring causal interpretability once configurational analyses move beyond the study of single effects. Instead, the current approach to configurational data analysis must be generalized to consistently absorb them. What we show is that such a generalization has already been proposed in concept more than 50 years ago in a field that has not had any place in CNA's development, and whose contribution has never received due recognition in QCA despite QCA's heavy reliance on QMC. The field we allude to is that of electrical engineering. In the remainder of this article, we will demonstrate that the relevance of electrical engineering extends far beyond the use of QMC in QCA. In fact, we have chosen the name Combinational Regularity Analysis (CORA) for our new method because that subfield of electrical engineering from which we import most of our procedures is called \"combinational circuit design\". \"Regularity\", on the other hand, indicates CORA's firm anchoring in the group of regularity accounts of causation, to which also the INUS Theory belongs [9].\n\nMulti-Output Switching Circuits\nElectrical engineering is centrally concerned with building switching circuits for operating digital devices. At the most basic level, these circuits consist of switches working in parallel, switches working in series, and inverters that open a closed switch and close an open switch, respectively. Parallel switches are implemented through so-called OR-gates: it is sufficient to activate at least one of the switches to close the circuit. Serial switches, in contrast, are implemented through AND-gates: all switches need to be activated to close the circuit. For instance, every domestic appliance with an on-offswitch and a safety switch to protect children from accidents contains, in one form or another, a serial circuit component.\nThe mathematical framework for analyzing the conversion of a given set of input signals to a desired set of output signals in order to make a circuit perform according to a prespecified behavior is provided by the algebra of switching circuits, a branch of the same Boolean algebra of which also propositional logic and set theory are varieties [45,46]. As propositional logic and switching circuit theory (and set theory) are so intimately linked, it is straightforward to translate concepts from one language to the other(s): OR-gates correspond to propositional disjunctions (and to set-theoretic unions), AND-gates to propositional conjunctions (and to set-theoretic intersections), and inversions to propositional negations (and to set-theoretic complements).\nIn devising more complex electrical devices, it is frequently necessary to simultaneously specify several switching functions that share the same inputs (because there is no risk of confusion, we will drop the addition \"switching\" in \"switching function\" from now on). Such a set of functions is called a system of functions. As more than one possible circuit layout usually fulfills the desired specification, the optimization of multi-output circuits is an important stage in the design process of a switching circuit [47,48]. Encoders and decoders, for example, are generic applications.\nOne of the most crucial questions electrical engineers have to address in the process of designing a circuit concerns the optimization of its hardware infrastructure. More specifically, given two different circuits that produce the same outputs when provided with the same set of inputs, the circuit demanding less costly infrastructure is preferred. More formally and generally, this problem can be phrased as follows: Central Problem of Multi-Output Optimization: Given a system of functions . . , f m (x)} and an objective function O defined on the set of F-equivalent systems S F , what is the set S * F ∈ S F for which O reaches an optimum?\nPotentially, there are many ways in which O could be defined. It can relate to the number of gates, gate contacts, or a multi-dimensional requirement of the form aP + bQ + cR , where P, Q, and R represent the number of gates of a certain type and a, b and c are weighting coefficients on unit price, reliability or other economical or technical criteria [49].\nA very common specification of O is called sum irredundancy, which, at least up to the late 1950s, also provided the objective function for QMC in optimizing switching circuits with single outputs. With sum irredundancy set as the objective function, the purpose of the optimization algorithm, whether QMC or else, is to find all possibilities for a circuit infrastructure that does not contain any unnecessary AND-gates [46], that is, AND-gates that are redundant in ensuring that the output of a circuit given a certain combination of inputs corresponds to the desired specification. A possible circuit layout that results from this process is correspondingly called an \"irredundant sum\"; \"sum\" because AND-gates-the first level of two-level circuits-can more generally be called Boolean products, while ORgates-the second level-can more generally be called Boolean sums. An AND-gate that could, but not necessarily is, a component of an irredundant sum is called a prime implicant (PI).\nIn contrast to single-output optimization problems, situations involving two or more outputs require additional considerations. Figure 2 shows two possible approaches to the optimization of a system of two functions: under Approach 1, the two functions f 1 and f 2 of inputs x 1 , x 2 and x 3 can be optimized separately as two quasi-independent systems, F 1 and F 2 , shown in panels (a) and (b), respectively. Alternatively, they can be optimized jointly as a 2-output system, F 3 , as shown under Approach 2 in panel (c). It may be suspected that the two approaches produce the same result, simply through different routes. However, this conjecture does not hold. The reason is that Approach 1 and Approach 2 may not generate the same set of PIs. Most importantly, under Approach 2, the complexity of a circuit's infrastructure may regularly be reduced by explicitly searching for PIs that are shared between functions. These PIs may not be PIs in the separate optimization of each function. Moreover, PIs that do not become parts of any irredundant sum under Approach 1, called \"useless\" PIs, may become useful, that is, part of at least one irredundant sum, under Approach 2.\nConsider the example of a system of functions f 1 (x, y, z) = (1, 3, 7) and f 2 (x, y, z) = (3, 6, 7) (as usual, functions are most compactly represented with decimal numbers; for instance, 1 is the decimal equivalent of x ′ y ′ z , 3 of x ′ yz because in binary-number notation, 1 is expressed as 001, 3 as 011). Thus, at x ′ y ′ z , x ′ yz and xyz it is the case that f 1 = 1 , and f 1 = 0 otherwise; at x ′ yz , xyz ′ and xyz it is the case that f 2 = 1 , and f 2 = 0 otherwise. Any optimization algorithm with sum irredundancy set as its objective function reveals the two irredundant sums f 1 = x ′ z + yz and f 2 = xy + yz , respectively, under Approach 1. If the corresponding circuits were built back into one system, four ANDgates and two OR-gates would thus be required. However, it is obvious in this case that f 1 and f 2 share yz as a PI. A circuit in which one of the corresponding ANDgates could be dispensed with would thus represent a strictly preferable alternative.\nA similar yet far less obvious example involves the 2-output system of functions f 1 (x, y, z) = (1, 3, 7) and f 2 (x, y, z) = (2, 6, 7) . In this case, the irredundant sums resulting under Approach 1 are f 1 = x ′ z + yz and f 2 = xy + yz ′ , respectively. If both circuits were built, again, four AND-gates and two OR-gates would be required. More difficult to see is that the alternative single-circuit system f 1 = x ′ z + xyz and f 2 = xyz + yz ′ requires only three AND-gates because one of these gates could use x, y and z as joint inputs to f 1 and f 2 . In contrast to the previous example, however, xyz is no PI of either function optimized independently because it contains redundant elements. For example, with regard to f 1 , Expressions 4a to 4c provide one way of proving x to be redundant in xyz: Respecting f 2 , Expressions 5a to 5c provide one way of doing the same with regard to z in xyz: At this stage, obvious similarities between the occurrence of redundancies in configurational data analyses of multiple outcomes with existing CCMs and the separate optimization of one system's functions in electrical engineering already start to become noticeable. Modern CCMs search for minimally necessary disjunctions of minimally sufficient conjunctions in order to generate causally interpretable models. In switching circuit theory, PIs are what minimally sufficient conjunctions are in configurational data analysis, minimally necessary disjunctions of minimally sufficient conjunctions are what irredundant sums are for electrical engineers. As propositional logic and switching circuit theory are merely two branches of the same underlying Boolean algebra, these concepts are completely equivalent.\nIn electrical engineering applications, where the primary objective of functional optimization is a reduction in circuit build costs, the inclusion of redundancies by absorption.\n(5a) xyz + yz ′ = xyz + yz ′ + xyy by consensus, (5b) = xyz + yz � + xy by idempotency, (5c) = xy + yz � by absorption. results in unnecessarily high build costs because a redundant input to an AND-gate or an OR-gate does not make a difference to the required operation of the circuit. In configurational data analysis with QCA and CNA, redundancies render models returned by these methods causally uninterpretable because a redundant element can never be a Boolean difference-maker [recall the causalchain problem above and the redundancy of literal t ′ in Expression 2]. Motivated by the possibility to reduce build costs through complete redundancy elimination, electrical engineers have already noticed about 60 years ago that it is inadequate to optimize each function separately when addressing problems that involve multiple outputs [46,[50][51][52]. In order to realize cost savings, all possible products of functions must be considered in addition to and simultaneously with each individual function. In consequence, the concept of the \"prime implicant\" has been generalized from the simple single-output to the multi-output framework. A PI resulting under such a framework is called a \"multi-output prime implicant\" (MOPI).\n\nDefinition 1 A multi-output prime implicant (MOPI) of a system of functions\nh;i with h ≤ k and 1 ≤ i j ≤ k , which is either a PI of some f j ∈ F with j = 1, 2, . . . , m or a PI of one of the product functions f 1 (x)f 2 (x) · · · f m (x).\nOn the basis of Definition 1, we can now also generalize Approach 2 introduced above in Fig. 2. Diagrammatically sketched in Fig. 3, any system of functions F can potentially have k inputs and m outputs. For m > 1 , PIs become MOPIs.\nIf, for multi-output optimization problems, redundancies must be made room for, the crucial question then is how to ensure that the switching circuit is most efficient according to the objective function O , that the result of Boolean optimization in configurational data analysis remains causally interpretable, respectively. Above, we have seen that the requirement of absolute redundancy elimination can create problems because the generation of minimally sufficient conjunctions with respect to one outcome may no longer remain minimally sufficient beyond that single outcome. Electrical engineers have also solved this problem by elevating the concept of irredundancy from the level of simple functions to the level of systems of functions [51].\nDefinition 2 An F-equivalent system of functions S ∈ S F is called an irredundant system S * ∈ S * F if it is impossible to cancel any literal in the writing of its MOPIs and any MOPI in the writing of its functions f j and still be able to ensure F-equivalence.\n\nDefinition 2 leaves it open whether a process of\nBoolean optimization results in only one irredundant system, two systems, a dozen or hundreds of systems. It is well possible-and usually the rule rather than the exception-that multiple irredundant systems represent potential candidates for a circuit's infrastructure. Without any further criteria, none of these systems is preferable to another because they all comply with the objective function of sum irredundancy.\nIn configurational data analysis with QCA or CNA, the existence of multiple models that fit the data equally well has been referred to as \"model ambiguity\" [14,53,54]. Under the multi-output approach of CORA, we will speak of \"systems ambiguity\" instead because each system comprises as many models as there are outputs, but these models are not alternatives to each other, whereas different systems are. To put this observation on a formal footing, we further introduce the concept of the solution to CORA in Definition 3. At this stage, we have all necessary theoretical concepts in place. In the following subsection, we introduce a core feature of CORA that has also been imported from electrical engineering: logic diagrams.\n\nLogic Diagrams\nIrrespective of how carefully a research design has been constructed and of how sophisticated the employed method is, if results cannot be communicated effectively, the impact of a study may be reduced considerably. Thus, graphics and visualization have played an increasing role in conveying the results of scientific work. So far, neither QCA nor CNA have offered consistent means of visualization. Depending on software, academic discipline, and personal preferences, researchers have used Venn diagrams, bivariate scatter plots, Tosmana maps and numerous other means for communicating their findings [55].\nIn contrast to QCA and CNA, CORA offers an established and standardized means for communicating its results graphically: logic diagrams. Initially, these diagrams have been developed by electrical engineers to visualize the architecture of switching circuits, but according to Judea Pearl, these diagrams also capture \"in my opinion, the very essence of causation\" [56]. Despite their apparent usefulness, however, only very few scientific disciplines in which causal inference plays a central role have so far adopted logic diagrams [57,58].\nA common standard for the production of logic diagrams is provided by MIL-STD-806B, a document that establishes uniform engineering and technical requirements for military or commercial processes, procedures, practices, and methods [59]. For two-level circuits, three core elements of this standard suffice: one for the and-operator / conjunction, one for the or-operator / disjunction, and one for the not-operator / negation. If multivalent inputs and outputs, that is, factors having more than two levels, should be allowed as well, level indicators must be added. These four elements, which together make up the graphical repertoire of logic diagrams in CORA, are shown in Fig. 4.\n\nData Mining\nBesides the possibility to analyze configurational multioutput problems and to visualize results by means of logic diagrams, a third advantage of CORA over QCA and CNA is the option to mine data. The basic idea behind this approach is that any system that is found with a given number of inputs, must, ceteris paribus, also always be found in an analysis with only those inputs present in the system. For example, if a solution includes a system that consists only of inputs x 1 , x 3 , x 5 , in whatever constellation, following an optimization process involving the input set x a = {x 1 , x 2 , x 3 , x 4 , x 5 } , then this system should also be found following an optimization process involving the reduced input sets Although the basic idea behind this approach to input selection has first been tested in the context of QCA [60,61], CORA is the first CCM to offer an in-built and systematic possibility to apply a tuple selection procedure. If, for example, a researcher has four potential inputs x = {x 1 , x 2 , x 3 , x 4 } available for inclusion, CORA can be asked to test whether the inclusion of x = {x 1 } alone or x = {x 2 } alone or x = {x 3 } alone or x = {x 4 } alone suffices to generate a solution that meets the researcher's criteria. If unsuccessful, CORA proceeds to tuples of two, i.e. x = {x 1 , x 2 } , x = {x 1 , x 3 } , and so on. From this perspective, CORA's data-mining approach represents a type of Occam's Razor, which says that explanations that involve fewer variables are, ceteris paribus, to be preferred over explanations that are more complex. Note that this is not tantamount to setting the objective function in Boolean optimization to what is called \"sum minimality\". A minimal sum is that irredundant sum which has the smallest number of PIs, but not necessarily the smallest number of inputs.\nNot least of all, there are additional practical considerations that motivate the option of data mining. Often, researchers have more variables available than can reasonably be included in a configurational analysis. For example, in one study on the effectiveness of health promotion networks, the authors have identified no fewer than 42 potential determinants of effectiveness while having only 13 cases of health promotion networks [62].\nMoreover, the more inputs researchers feed into the optimization process given a fixed number of cases, the higher their measure of fit statistics tend to become, but the higher the degree of model ambiguity also becomes. The relationship between the number of inputs and the number of models in a QCA or CNA solution has not yet been systematically studied, but existing data experiments suggests that beyond four inputs, model ambiguity starts to become the rule rather than the exception and tends to increase in severity with every additional input [53]. For instance, a recent meta analysis of 215 peer-reviewed QCA articles from across 109 management, political science and sociology journals found that one in three QCA studies was affected by (unreported) model ambiguity, one in ten severely so [14]. Absent other means of ranking multiple and equally well-fitting systems, the option of data mining provides researchers with a practical way to achieve a reduction in systems ambiguity.\n\nSoftware\nMethods and algorithms can be theoretically developed and also methodologically evaluated, but without appropriate software, they have no value to applied researchers. All procedures described above, plus additional ones, have thus been made available to the scientific community in the open-source Python/C++ package CORA [27], a screenshot of whose interface is shown in Fig. 6.\nThe workflow in CORA is pre-determined to guide users through the analysis. It comprises nine steps, the last two of which are optional: (1) the initialization of the framework and (2) default settings, (3) the choice and (4) import of data, (5) the specification of the inputs and outputs, (6) the setting of search parameters and thresholds for data fit statistics, (7) the computation of the solution, (8) the initialization of CORA's visualization module and finally (9) the drawing and export of logic diagrams. In CORA, logic diagrams are integrated via a stand-alone visualization module called LOGIGRAM [63]. Accordingly, the particular form of logic diagram generated in CORA is called a \"logigram\". For reasons of space, we cannot introduce CORA in detail here. This will be done in a separate software tutorial.\n\nResults\nIn this section, we provide four basic example applications of CORA. The first example uses a relatively simple set of artificial data, the second example provides a showcase analysis of data from a typical context of multimorbidity, the third example uses a more complex set of artificial data, and the fourth example is taken from a multi-outcome QCA study that analyses the impact of structural factors on the injury rate in 12 European countries [64]. In this first example, we use artificial data on a simple multi-output problem, to which numerous basic applications could potentially fit. The main objective is not to generate any substantive insights, but to demonstrate the generic workings of CORA and to describe how the method's output has to be interpreted. To the three inputs x, y, and z, consider the following system of two functions f 1 and f 2 given in Expressions 6a to 6b: Under a certain combination of inputs, namely xyz (term 7), both outputs are present. Under all other combinations, either only one output is present or none. From an applied perspective, data of this structure may signal that the simultaneous presence of some combination of the risk factors x, y and z is responsible for the simultaneous presence of medical conditions f 1 and f 2 . If these data were analyzed with QCA or CNA, each would find output f 1 to be caused by x ′ z or yz and output (6a) f l x, y, z = m(1, 3, 7) (6b) f 2 x, y, z = m(2, 6, 7) f 2 by xy or yz ′ . They would not find any commonalities between these two outputs.\nWith CORA, an analysis of complex effects is straightforward. For the given data, CORA's solution consists of two irredundant systems, as shown in Expression 7: S * 1 reveals the complex cause xyz of the complex effect f 1 f 2 . Once this complex effect is explained, individual causes of f 1 alone, f 2 alone, respectively, remain. Under S * 1 , CORA identifies x ′ z for f 1 alone, yz ′ for f 2 alone, respectively. Alternatively, there may not be any common cause, but each effect is brought about by distinct causes. S * 2 reveals this alternative possibility, which is identical with the result QCA and CNA generate.\nThis simple example illustrates in a direct way why CORA's inferential capabilities extend beyond those of QCA and CNA. If there are indeed no complex effects to be explained, CORA will detect this possibility in the same way as QCA or CNA will. However, if common causes to complex effects exist, CORA will be the only method that can reveal this possibility because it operates under a system-level conception of irredundancy, whereas both QCA and CNA are restricted to an outputlevel conception of irredundancy.\n\nExample 2: Applied example on multi-morbidity\nIn this example, we illustrate the potential of CORA for problems related to the study of multi-morbidity. Table 1 shows data on eleven patient groups p 1 to p 11 . The first four columns contain information on four socio-demographic and economic characteristics, namely gender, income (level), family history (of depression or diabetes) and marital status. The last two columns show data on two health conditions, namely diabetes and depression. Although the data in Table 1 have been purposefully chosen for demonstration purposes, several studies point towards strong relationships within such data. For instance, it has been argued that \"depression comorbid with other chronic diseases produced significantly greater decrements in health than from one or more chronic diseases, and that this additive effect is substantially amplified in the case of depression comorbid with diabetes\" [65]. Other studies have shown striking associations between socio-economic and socio-demographic factors, and chronic diseases such as depression and diabetes [66,67]. Last, but not least, a significant body of epidemiologic studies emphasizes that a positive family history increases the risk among first-degree relatives for diabetes [68].\nCORA's solution to these data is given in Expression 8. It says that a co-morbid condition of diabetes and depression has at least two (complex) causes, the first of which contains low income and the status of being married, and the second of which contains a family history of diabetes or depression. For depression without diabetes, the status of being married is by itself part of the explanation. Again, we do not seek to interpret these findings substantively. Our primary goal here is only to explain how CORA's findings are to be read.\nIn this connection, it is important to add two further notes. First, there is no empirical evidence for the causal relevance of gender for diabetes or depression, or a comorbid condition. More generally, it must be emphasized that only because some input is not contained within CORA's solution, this does not mean that the respective input is generally causally irrelevant to the analyzed output(s). It just means that either the input is indeed irrelevant or the data do not contain sufficient information to reveal the input as causally relevant when it is truly relevant.\nSecond, the data in Table 1 are such that every diabetic patient has also depression, but the opposite does not hold. There are patients who have depression but are not diagnosed with diabetes. This association of the two outputs is correctly reflected only through CORA's generalized process of multi-output optimization. If the analyst had used QCA or CNA, the two outputs would have had only one cause in common, namely a family history of depression or diabetes. However, since the set of patient groups with diabetes is a proper subset of the set of patient groups with depression, every complex cause of diabetes must also be a part of a potential causal explanation of depression. Under the restricted notion of irredundancy in QCA or CNA, input m could never appear Table 1 Socio-demographic factors of eleven patient groups for two health conditions: diabetes and depression a\n\nExample 3: Artificial data; complex structure\nIn this third example, we increase the complexity of the data by adding one more input as well as another output.\nTo the four inputs a, b, c and d, consider the following system of three functions f 1 , f 2 and f 3 given in Expressions 9a to 9c: We have aligned all common input combinations visually so that it becomes easier to see which outputs have which input combinations in common. Outputs f 1 and f 2 co-occur for input combinations 2, 3, 5, 7, 10, 11 and 15; outputs f 2 and f 3 co-occur for input combinations 6, 7, 14 and 15; outputs f 1 and f 3 co-occur for input combinations 7 to 9, 13 and 15; and for input combinations 7 and 15, all three outputs co-occur. For these data, CORA's solution consists of five irredundant systems, as shown in Expression 10: With just one more input and one more output than in the previous example, we see that the complexity of the solution may increase markedly. Instead of two systems as in Example 1, we now have five alternative systems explaining the data equally well. Each of these systems reveals a distinct possibility for a complex causal relation between the four inputs and the three outputs. S * 1 is, functionally speaking, the most complex system. It reveals several common causes of several complex effects. First, it shows that outputs f 1 and f 2 have three (9a) (2, 3, 5, , 7, 8, 9, 10, 11, 13, , 15) (9b) (2, 3, 5, 6, 7, , 10, 11, , 14, 15) (9c) 6, 7, 8, 9, , 13, 14, 15) (10) complex causes in common, namely a ′ bd , bcd and b ′ c . Second, it shows that outputs f 1 and f 3 also have three complex causes in common, namely ab ′ c ′ , bcd and a ′ cd .\nIn contrast, outputs f 2 and f 3 co-occur as a complex effect only of bc. Lastly, all three outputs co-occur as a complex effect of bcd. As no other causes remain, no effect occurs in isolation. In applied terms, S * 1 thus suggests a purely co-morbid explanation of diseases f 1 , f 2 and f 3 .\nA purely co-morbid explanation is also offered by S * 3 . Unlike S * 1 , however, S * 3 does not suggest any common cause of all three outputs. The extent the data can be explained through co-morbid causal relations is therefore lower than under S * 1 . At the same time, no effect can be explained in isolation from another effect. Only systems S * 2 and S * 4 include causes of isolated effects, in the former for f 1 (ad) and in the latter also for f 1 (bd). In fact, this is the only difference between these two systems. The two systems S * 1 and S * 3 are visualized in Fig. 7, the former in panel (a) and the latter in panel (b). Note that logigrams in CORA (currently only) allow the use of upper-case notation for inputs.\n\nExample 4: Empirical data on injury rates in West European countries\nThe aim of the study in [64] is to analyze how socioeconomic factors, such as per capita income, unemployment and alcohol consumption (which have been found to have an impact on traffic fatalities and suicides in other studies) as well as culture-oriented factors such as religion and education are related to injury mortality in 12 West European countries. Traffic injuries were chosen as the indicator for environment-related injury and suicides as that for its socially related counterpart. Table 2 shows the data for the two outputs and the five inputs of this analysis. Table 3 presents the joint truth table generated from the data in Table 2. Inclusion / consistency scores are also provided in this table. Using QCA, two separate optimization runs are necessary, one for output mvta and one for output ssii.\nThe PI chart in Table 4 shows the list of PIs per output. It suggests that the two outputs have nothing in common because there is no shared PI. The one essential PI necessary to cover all instances of mvta completely is roca, while mys has the same function with respect to ssii.\nThis analysis with QCA suggests that the death rate for motor vehicle traffic accidents and the death rate for suicides and self-inflicted injuries have completely independent causes. There seems to be evidence of causal relevance only for the percentage of Roman Catholics with regard to motor vehicle accidents and only for the years of schooling with regard to suicides and self-inflicted  a gnp = Gross national product per capita (1: above limit / 0: below limit); mys = Mean years of schooling (1: above limit / 0: below limit); apac = Annual pure alcohol consumption (1: above limit / 0: below limit); unem = Unemployment rate (1: above limit / 0: below limit); roca = Roman Catholics percentage (1: above limit / 0: below limit); mvta = Age-standardized death rate for motor vehicle traffic accidents (1: above limit / 0: below limit); ssii = Age standardized death rate for suicides and self-inflicted injuries (1: above limit / 0: below limit)    injuries. We demonstrate these conclusions to be unsatisfactory. Table 5 shows the MOPI chart for the truth table in  Table 3. MOPI charts can be generated via different algorithmic routes in CORA. Currently, users have two options, an on-dc and an on-off algorithm [69][70][71][72]. On-off algorithms enjoy significant computational advantages when the size of the dc-set is large relative to the size of the off-set, whereas on-dc algorithms enjoy significant computational advantages when the size of the off-set is large relative to the size of the dcset, given a fixed size of the on-set. The possibility to choose between distinct yet equivalent algorithms also demonstrates that, in contrast to QCA, where researchers regularly worry about the use of logical remainders, solutions types and contradictory simplifying assumptions [73,74], CORA is completely unaffected by such problems. Irrespective of the algorithmic choice, the objective function for deriving the complete set of irredundant systems that faithfully reflect the empirical evidence is hardwired into CORA.\n\nCountry\nInternally, CORA then applies an enhanced version of Petrick's method to the PI chart for identifying all irredundant systems [75]. In the present replication, this process results in four such systems, which are shown in Table 6. In Expression 11, these systems are translated back using the original variable names. System S * 1 mirrors the result of separate optimization with no Table 4 The PI chart resulting from separate optimization of the truth table in Table 3   mvta  ssii   3  5  7  23  31  24  30 31 roca ′ x x p 10 Table 5 The PI chart resulting from multi-output optimization of the truth table in Table 3   mvta  ssii   3  5  7  23  31  24  30 31 roca ′ x x p 11 Table 6 Multi-output solution to MOPI chart in Table 5 Systems Output mvta p 3 p 3 p 6 p 4 p 6 p 4 p 6 ssii p 10 p 6 p 11 p 6 p 10 p 6 p 11 shared causes. However, three other possibilities exist, all of which share the MOPI mys · roca ( p 6 in Table 5).\nComparing the PI chart derived from separate optimization in Table 4 with the PI chart derived from multioutput optimization in Table 5, three observations can be made. First, while simple single-output optimization suggests that the two analyzed outputs have nothing in common, multi-output optimization reveals a shared complex cause that feeds into three alternative explanations for the analyzed data. Second, multi-output optimization leads to the identification of a (multi-output) PI that is not part of any PI chart under separate optimization. Third, three PIs (p4, p 10 and p 11 ) are present in the PI charts of both separate and joint optimization. However, under separate optimization these PIs are useless because they are dominated by essential PIs, whereas the same PIs become useful under multi-output optimization and thus part of CORA's solution.\n\nConclusions\nModern CCMs, such as QCA and CNA, have started to make inroads into medical and health research over the last decade. At the same time, these methods remain unable to process data on multi-morbidity because such data require the capability to analyze complex effects. In this article, we have presented CORA, a new member of the family of CCMs with which multiple conditions and their complex conjunctions can be analyzed. CORA takes its inspiration from electrical engineering, and switching circuit analysis in particular. Leveraging this source of inspiration has allowed redundancies, which have prevented a causal interpretation so far when analyzing multiple effects, to be straightforwardly absorbed into CORA's more general framework. To demonstrate CORA, we have provided several example applications, both with simulated and empirical data, in which CORA has been shown to be able to simultaneously explain individual conditions as well as complex conjunctions of conditions. Through CORA, problems of multimorbidity in particular, and configurational analyses of complex effects in general, thus come into the analytical reach of CCMs.\n� mys · roca + roca ⇔ mvta mys · roca + roca ′ ⇔ ssii S * 3 = � mys · roca + mys ′ ⇔ mvta mys · roca + mys ⇔ ssii S * 4 = � mys · roca + mys ′ ⇔ mvta mys · roca + roca ′ ⇔ ssii Despite the significant advances offered by CORA, important avenues for further refinements in configurational data analysis remain to be explored in future work. For example, researchers currently still have to be able to determine in advance which of the variables in their data belong to the input and which to the output side. A fully naive yet completely open approach to configurational data analysis would let the method determine the assignment. With respect to the more general structure of configurational cause-effect relations, sequential circuit analysis offers yet another possibility to expand the limits of configurational data analysis in significant ways. Incorporating a dimension of sequence would allow analysts to specify the exact order of occurrence of inputs. A third research avenue is provided by heuristic procedures for contexts of (very) big data. These procedures seek to strike a balance between the desire to optimize Boolean functions and the requirement to process high-dimensional data. Lastly, switching circuit theory distinguishes between the analysis of deterministic circuits and that of probabilistic circuits. So far, CCMs have exclusively moved within the realms of the former. A shift towards probabilistic circuits, however, would call for an accompanying change in the theory of causation which CCMs currently work under. The implications of such a move would have to be examined thoroughly.\nAs varied as the possibilities for advancement are, as large are the challenges and questions to be addressed. Nonetheless, we believe that CORA has demonstrated unequivocally that configurational analysts need not reinvent the wheel. With the famous Quine-McCluskey algorithm, electrical engineering and logic design have developed already in the 1950s the procedures that continue to represent the technical state-of-the-art of QCA. Since the 1950s, however, electrical engineering and logic design have progressed considerably. These fields thus still hold numerous tools on offer that could help researchers to improve their understanding of complex medical and health conditions."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-23"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 58677,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254779577"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Explaining Management Phenomena\n\nOne key objective of management research is to explain business phenomena. Yet understanding the nature of explanation is essentially a topic in philosophy. This is the first book that bridges the gap between a technical, philosophical treatment of the topic and the more practical needs of management scholars, as well as others across the social sciences. It explores how management phenomena can be explained from a philosophical perspective, and renders sophisticated philosophical arguments understandable by readers without specialized training. Covering virtually all the major aspects of the nature of explanation, this work will enhance empirical and theoretical research, as well as approaches combining the two. With many examples from management literature and business news, this study helps scholars in those fields to improve their research outcomes."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-15"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 61474,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254707493"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Gracián y la modernidad: la indisponibilidad del mundo y el papel constituyente de los otros como claves de la virtud\n\n. The aim of this text is to defend a reading of Gracian’s philosophy in terms of another, non-oppositional modernity that engages in a tensional and con-stitutive relationship with the canonical reading of modernity. To this objective, a contrast will be offered between Heidegger’s conceptualisation of modernity as the «epoch of the image of the world» and the world that can be glimpsed in the works of Baltasar Gracián. The Gracián’s world is unavailable to the human being. More-over, according to this reading, the supposed isolation in which the modern subject was submerged is repeatedly rejected by Gracián. Both in the task of becoming a person and in the formation of eminent individuals, which the first stage of his work deals with, the author gives a fundamental role to others. Both elements give rise to a conception of virtue which, being distinctly modern, allows us to broaden our reading of our own tradition."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 70875,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255082318"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Wittgenstein and censorship\n\nThe current debates around censorship are about more than whether or not censorship is desirable. These debates are also about what counts as censorship. The question of what counts as censorship is a relatively new one since the Liberal conception of censorship was taken as given until the 1980s. Since then, a new approach to understanding censorship has gained momentum. What Matthew Bunn calls ‘New Censorship Theory’ argues that the Liberal conception is far too narrow to properly encompass the vast complexities of censorship. New Censorship Theory does not deny the insights offered by the Liberal conception, but expands upon them. This expansion pushes the notion of censorship out of the censor’s office and into the marketplace, politics and social life. New Censorship Theory also recognizes the way that censorship is both prohibitive and productive. In light of this, some authors have argued that New Censorship Theory overstretches the concept of censorship to such a degree that it risks becoming useless and it risks equating all forms of censorship. Beate Müller borrows the notion of family resemblances from the philosophy of the later Wittgenstein to try to avoid getting stuck in the debates around terminology. She does this by trying to identify the essential elements of censorship, distinguishing between its core and periphery characteristics and by mapping censorial actions and reactions systematically. I argue that Müller uses the philosophy of Wittgenstein to make an anti-Wittgensteinian argument. In order to show why I think that this is the case, I will review the censorship debate before providing my own Wittgensteinian contribution."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 75412,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "252553775"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Reading Angela Davis Beyond the Critique of Sartre\n\nThis paper examines Angela Davis’s 1969 Lectures on Liberation and her critique of Jean-Paul Sartre’s views regarding freedom and enslaved agency. Across four sections, the paper etches out Davis’s response to what she calls Sartre’s ‘notorious statement’ through her own existential reading of Frederick Douglass’s resistance to chattel slavery. Instead of interpreting Davis’s existential insights through the work of Sartre or other Western continental philosophers, the paper engages Lewis Gordon, George Yancy, Frank Kirkland, and LaRose Parris to develop an alternative frame for assessing Davis’s existential thinking. Embracing a diverse lineage of existential philosophy, the paper argues for Black-centered approaches to existential philosophy that resonate with, but are not reducible or indebted to, European existentialism."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-01"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 77736,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255143841"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Philosophical Foundations of the Legal Language\n\nThis paper analyzes the essence of the philosophical foundations of the legal language as it is used in certain theories in the legal philosophy. The purpose of the paper is to provide a full study of the legal language theory to determine its place in modern philosophical legal thought.\nThe paper used methods of the history of philosophy, especially the method of rational reconstruction, and is based on the interpretation of the classical philosophical and legal texts (W. Waismann, J. L. Austin, H. Kelsen, H. Hart).\nThe main result of the paper is the justification that the unity of logic and epistemology  became the ground of application of the analytical method in the field of legal knowledge from the legal language point of view.\nThe main conclusion of this paper is that the linguistic analysis of legal concepts for the justification of the legal decisions and their consequences expands the horizons of analytical legal philosophy and allows us to reveal the essence of legal reality in a new way.\nThe paper was carried out within the framework of the Narikbayev KAZGUU University research project “Readability of Law”."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-25"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 83401,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255034709"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Avenues from the written Ethics back to the unwritten “Philosophy”\n\nFirst of all, let me express my gratitude to the editors of the Journal of Spinoza Studies for the invitation to contribute to the first issue of the journal. I belong to the community of so-called “continental” philosophers, and accordingly, my interpretation of this task may be slightly different from an analytic interpretation. What the authors of this first issue of JSS are expected to do is not to present arguments regarding some well-discussed topics, but rather to map the most exciting avenues for future Spinoza research. My own way of doing this will be to propose a holistic vision concerning Spinoza’s philosophy. The main focus will be on the Ethics, but I will also touch upon some general methodological issues along the way. I will present a vision of Spinoza’s philosophy as a whole, as well as some consequences of this vision. My main contention is that Spinoza’s frame of mind is Neo-Platonic: his system of philosophy presupposes a primordial vision in the sphere of a not-yet-explicated, implied unitary wisdom—i.e., what is before any articulation by conceptual and linguistic means. So understood, Spinoza follows the logic of Plotinus’s “On the three primary levels of reality.” I then consider the articulated Ethics and distinguish the layers of language-based cognition in Spinoza’s œuvre, in order to decipher the proper messages of particular passages more successfully. Basically, the Ethics sub specie aeternitatis should be distinguished from the Ethics sub specie vitae cottidianae. I compare the corresponding difference of these manners of thought and speech to the distinction between the analytic and the synthetic methods in Descartes. I maintain that Spinoza coupled the “synthetic” argumentation of the propositions and demonstrations with the series of other types of texts; in so doing he integrated the “analytic” part into the “synthetic.” I adopt the distinction of the TTP between Euclidean-style and Biblical-style books. Given that the Ethics is not written entirely in the Euclideanstyle, I maintain that its major part allows for, and even calls for, investigation via the hermeneutical 2773-0107"
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-22"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 83935,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254685797"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Intensional First Order Logic for Strong-AI Generation of Robots\n\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust strong AI capable of reasoning, learning, and cognitive modeling. In this paper we consider the intensional First Order Logic (IFOL) as a symbolic architecture of modern robots, able to use natural languages to communicate with humans and to reason about their own knowledge with self-reference and abstraction language property. We intend to obtain the grounding of robot's language by experience of how it uses its neuronal architectures and hence by associating this experience with the mining (sense) of non-defined language concepts (particulars/individuals and universals) in PRP (Properties/Relations/propositions) theory of IFOL. We consider three natural language levels: The syntax of particular natural language (Italian, French, etc..), and two universal language properties: its semantic logic structure (based on virtual predicates of FOL and logic connectives), and its corresponding conceptual PRP structure which universally represents the composite mining of FOL formulae grounded on the robot's neuro system.\n\n\nIntroduction\nLast 15 years of my work in AI was mainly dedicated to development of a new intensional FOL, by integrating Montague's and algebraic Bealer's [2] approaches, with a conservative Tarski's semantics of the standard FOL. Basic result was the publication of the conservative extension of Tarski's semantics to intensional FOL [3], and two-step intensional semantics [4], which guaranteed a conservative extension of current RDB, but more than 50-years old technology, toward new IRDB (Intensional RDB). Indeed, in my next Manifesto of IRDB [5], I hoped also to find interested research groups and funds to begin the realization of IRDB as a new platform (compatible with all previously developed RDB application), able also to support NewSQL for Big Data, and ready for other AI improvements.\nThe central hypothesis of cognitive science is that thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures. Most work in cognitive science assumes that the mind has mental representations analogous to computer data structures, and computational procedures similar to computational algorithms. Knowledge representation, strongly connected to the problem if knowledge processing, reasoning and \"drawing inferences\", is one of the main topics in AI. By reviewing the knowledge representation techniques that have been used by humans we will be aware of the importance of language. The predominant part of IT industry and user's applications is based on some sublanguage of the standard (extensional) FOL (First Order Logic) with Tarski's semantics based (only) on the truth; my effort is to pass to a more powerful evolution of the FOL able to support the meaning of knowledge as well, by replacing the standard FOL and its DB theory and practice in IT business. All this work is summarized and extended also to AI applications of many-valued logics in my recent book [1].\nThis paper instead is dedicated to show how this defined IFOL in [1] can be used for a new generation of intelligent robots, able to communicate with humans with this intensional FOL supporting the meaning of the words and their language compositions. As in [6] we can consider three natural language levels: The syntax of a particular natural language (French, English, etc..) its semantic logic structure (transformation of parts of the language sentences into the logic predicates and definition of corresponding FOL formulae) and its corresponding conceptual structure, which differently from the semantic layer that represents only the logic's semantics, represents the composed meaning of FOL formulae based on the grounding of intensional PRP concepts.\nThus, intensional mapping from the free FOL syntax algebra into the algebra of intensional PRP concepts, I : A F OL → A int provided by IFOL theory, is a part of the semantics-conceptual mapping of natural languages. Note that differently from the particularity of any given natural language of humans, the underlying logical semantics and conceptual levels have universal human knowledge structure, provided by innate human brain structure able to rapidly acquire the ability to use any natural language. Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, and has to be improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. We consider that the natural language (first level) can be parsed into a logical FOL formula with a numbers of virtual predicates and logic connectives of the FOL. By such a parsing we obtain the second, semantic logic, structure corresponding to some FOL formula. However, natural language is grounded in experience. Humans do not always define all words in terms of other words, humans understand many basic words in terms of associations with sensory-motor experiences for example. People must interact physically with their world to grasp the essence of words like \"blue,\" \"could,\" and \"left.\" Abstract words are acquired only in relation to more concretely grounded terms.\nTheoretical neuroscience is the attempt to develop mathematical and computational theories and models of the structures and processes of the brains of humans and other animals. If progress in theoretical neuroscience continues, it should become possible to tie psychological to neurological explanations by showing how mental representations such as concepts are constituted by activities in neural populations, and how computa-tional procedures such as spreading activation among concepts are carried out by neural processes. Concepts, which partly correspond to the words in spoken and written language, are an important kind of mental representation.\nAlan Turing developed the Turing Test in 1950 in his paper, \"Computing Machinery and Intelligence\". Originally known as the Imitation Game, the test evaluates if a machine's behavior can be distinguished from a human. In this test, there is a person known as the \"interrogator\" who seeks to identify a difference between computer-generated output and human-generated ones through a series of questions. If the interrogator cannot reliably discern the machines from human subjects, the machine passes the test. However, if the evaluator can identify the human responses correctly, then this eliminates the machine from being categorized as intelligent.\nDifferently from the simulation of AI by such Turing tests and the Loebner Prize 1 and in accordance with Marvin Minsky 2 , in this paper I argue that a real AI for robots can be obtained by using formal Intensional FOL (with defined intensional algebra of intensions of language constructions) for the robots as their symbolic AI component, by defining the sense to ground terms (the words) in an analog way, associating to these words the software processes developed for the robots when they recognize by these algorithms (neural architectures) the color \"blue\" of visual objects, the position \"left\" etc... In this way we would obtain a neuro-symbolic AI which attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation as, in this case, an IFOL. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation. The IFOL defined in [1] is provided by abstraction operators as well.\nDaniel Kahneman [7] describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition 1 The Loebner Prize was an annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. The prize is reported as defunct since 2020. [1] The format of the competition was that of a standard Turing test.\nIn each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which. 2  while System 2, in uor case based on IFOL, is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed. So, for the words (ground linguistic terms), which can not be \"defined by other words\", the robots would have some own internal experience of the concrete sense of them. Thus, by using intensional FOL the robots can formalize also the natural language expressions \"I see the blue color\" by a predicate \"see(I,blue color)\" where the sense of the ground term \"I\" (Self ) 3 for a robot is the name of the main working coordination program which activate all other algorithms (neuro-symbolic AI subprograms) like visual recognition of color of the object in focus. But also the auto-conscience sentence like \"I know that I see the blue color\" by using abstracting operators \"⋖ ⋗\" of intensional FOL, expressed by the predicate \"know(I,⋖ see(I, blue color)⋗)\", etc...\nConsequently, we argue that by using this intensional FOL, the robots can develop their own knowledge about their experiences and communicate by a natural language with humans. So, we would be able to develop the interactive robots which learn and understand spoken language via multisensory grounding and internal robotic embodiment.\nThe grounding of the intensional concepts i PRP theory of intensional logic was not considered in my recent book [1] from the fact that this book was only restricted on the symbolic AI aspects (IFOL); so by this paper we extend the logic theory developed in [1] with concrete grounding of its intensional concepts in order to obtain a strong AI for robots. So, in next Section we will provide a short introduction to IFOL and its intensional/extensional semantics [1].\n\nAlgebra for Composition of Meanings in IFOL\nContemporary use of the term \"intension\" derives from the traditional logical doctrine that an idea has both an extension and an intension. Although there is divergence in formulation, it is accepted that the extension of an idea consists of the subjects to which the idea applies, and the intension consists of the attributes implied by the idea. In contemporary philosophy, it is linguistic expressions (here it is a logic formula), rather than concepts, that are said to have intensions and extensions. The intension is the concept expressed by an expression of intensional algebra A int , and the extension is the set of items to which the expression applies. This usage resembles use of Frege's use of \"Bedeutung\" and \"Sinn\" [8].\nIntensional entities (or concepts) are such things as Propositions, Relations and Properties (PRP). What make them \"intensional\" is that they violate the principle of extensionality; the principle that extensional equivalence implies identity. All (or most) of these intensional entities have been classified at one time or another as kinds of Universals [9].\nIn a predicate logics, (virtual) predicates expresses classes (properties and relations), and sentences express propositions. Note that classes (intensional entities) are reified, i.e., they belong to the same domain as individual objects (particulars). This endows the intensional logics with a great deal of uniformity, making it possible to manipulate classes and individual objects in the same language. In particular, when viewed as an individual object, a class can be a member of another class.\nis a particular fixed sequence of the set of all free variables in φ. This definition contains the precise method of establishing the ordering of variables in this tuple: such an method that will be adopted here is the ordering of appearance, from left to right, of free variables in φ. This method of composing the tuple of free variables is unique and canonical way of definition of the virtual predicate from a given open formula.\nThe virtual predicates are useful also to replace the general FOL quantifier on variables (∃x) by specific quantifiers Virtual predicates are atoms used to build the semantic logic structures of logic-semantics level of any given natural language.\nLet us define the FOL syntax algebra , with the set of joined variables (their positions in the first and second virtual predicate, respectively) S = {(4, 1), (2, 3)}, so that its extension is expressed by an algebraic expression R 1 ⊲⊳ S R 2 , where R 1 , R 2 are the extensions for a given Tarski's interpretation I T of the virtual predicate φ, ψ relatively, and the binary operator ⊲⊳ S is the natural join of these two relations. In this example the resulting relation will have the following ordering of attributes: (x i , x j , x k , x l , x m , y i , y j ). In the case when S is empty (i.e. its cardinality |S| = 0) then the resulting relation is the Cartesian product of R 1 and R 2 . For the existential quantification, the FOL formula For logic negation operator we will use the standard symbol ¬.\nBased on the new set of logical connectives introduced above, where the standard FOL operators ∧ and ∃ are substituted by a set of specialized operators {∧ S } S∈P(N 2 ) and {∃n} n∈N as explained above, we can define the following free syntax algebra for the FOL: be an extended free syntax algebra for the First-order logic with identity . =, with the set Ł of first-order logic formulae with the set of variables in V, with ⊤ denoting the tautology formula (the contradiction formula is denoted by ⊥ ≡ ¬⊤).\nWe begin with the informal theory that universals (properties (unary relations), relations, and propositions in PRP theory [10]) are genuine entities that bear fundamental logical relations to one another. To study properties, relations and propositions, one defines a family of set-theoretical structures, one define the intensional algebra, a family of set-theoretical structures most of which are built up from arbitrary objects and fundamental logical operations (conjunction, negation, existential generalization,etc..) on them.\n, where by f and t we denote the empty set ∅ and set {<>} respectively. The intensional interpretation is a mapping between the set Ł of formulae of the FOL and intensional entities in D, I : Ł → D, is a kind of \"conceptualization\", such that an open-sentence (virtual predicate) φ(x 1 , ..., x k ) with a tuple of all free variables (x 1 , ..., x k ) is mapped into a k-ary concept, that is, an intensional entity u = I(φ(x 1 , ..., x k )) ∈ D k , and (closed) sentence ψ into a proposition (i.e., logic concept) v = I(ψ) ∈ D 0 with I(⊤) = T ruth ∈ D 0 for the FOL tautology ⊤ ∈ Ł (the falsity in the FOL is a logic formula ¬⊤ ∈ Ł). A language constant c is mapped into a particular a ∈ D −1 (intension of c) if it is a proper name, otherwise in a correspondent concept u in D I . Thus, in any application of intensional FOL, this intensional interpretation that determines the meaning (sense) of the knowledge expressed by logic formulae is uniquely determined (prefixed) (for example, by a grounding on robot's neuro system processes, explained in next section).\nHowever, the extensions of the concepts (with this prefixed meaning) vary from a context (possible world, expressed by an extensionalizzation function) to another context in a similar way as for different Tarski's interpretations of the FOL:\n\nDefinition 4. EXTENSIONS AND EXTENSIONALIZATION FUNCTIONS:\nWe define the function f <> : R → R, such that for any R ∈ R, The extensions of the intensional entities (concepts) are given by the set E of extensionalization functions h : Consequently, intensions can be seen as names (labels) of atomic or composite concepts, while the extensions correspond to various rules that these concepts play in different worlds.\nThe intensional entities for the same logic formula, for example x 2 + 3 = x 2 1 − 4, which can be denoted by φ(x 2 , x 1 ) or φ(x 1 , x 2 ), from above we need to differentiate their concepts by I(φ(x 2 , x 1 )) = I(φ(x 1 , x 2 )) because otherwise we would obtain erroneously that h(I(φ(x 2 , x 1 ))) = h(I(φ(x 1 , x 2 ))). Thus, in intensional logic the ordering in the tuple of variables x in a given open formula φ is very important, and explains why we introduced in FOL the virtual predicates in Definition 1.\n\nDefinition 5. Let us define the extensional relational algebra for the FOL by,\nwhere {<>} ∈ R is the algebraic value correspondent to the logic truth, R = is the binary relation for extensionally equal elements, with the following operators: 1. Binary operator ⊲⊳ S : R×R → R, such that for any two relations R 1 , R 2 ∈ R , the R 1 ⊲⊳ S R 2 is equal to the relation obtained by natural join of these two relations if S is a non empty set of pairs of joined columns of respective relations (where the first argument is the column index of the relation R 1 while the second argument is the column index of the joined column of the relation R 2 ); otherwise it is equal to the cartesian product R 1 × R 2 . 2. Unary operator ∼: R → R, such that for any k-ary (with k ≥ 1) relation R ∈ P(D k ) ⊂ R we have that ∼ (R) = D k \\R ∈ P(D k ), where '\\' is the substraction of relations. For u ∈ {f, t} = P(D 0 ) ⊆ R, ∼ (u) = D 0 \\u. 3. Unary operator π −n : R → R, such that for any k-ary (with k ≥ 1) relation R ∈ P(D k ) ⊂ R we have that π −n (R) is equal to the relation obtained by elimination of the n-th column of the relation R if 1 ≤ n ≤ k and k ≥ 2; equal to, from (1), f <> (R) if n = k = 1; otherwise it is equal to R.\nWe will use the symbol '=' for the extensional identity for relations in R.\nThe intensional semantics of the logic language with the set of formulae Ł can be represented by the mapping = (x, y)) and T ruth = 4. h(exists n (u)) = π −n (h(u)), where π −n is the projection operation which eliminates n-th column of a relation and exists n (u) ∈ D k−1 if 1 ≤ n ≤ k (otherwise exists n is the identity function).\nWe define a derived operation union : (P(D i )\\∅) → D i , i ≥ 0, such that, for any B = {u 1 , ..., u n } ∈ P(D i ) and S = {(l, l) | 1 ≤ l ≤ i} we have that union({u 1 , ..., u n }) = u 1 , if n = 1 neg(conj S (neg(u 1 ), conj S (neg(u 2 ), ..., neg(u n ))...), otherwise (3) Than we obtain that for n ≥ 2: h(union(B)) = h(neg(conj S (neg(u 1 ), conj S (neg(u 2 ), ..., neg(u n ))...) Note that it is valid also for the propositions in u 1 , u 2 ∈ D 0 , so that h(union(u 1 , 1. The logic formula φ(x i , x j , x k , x l , x m )∧ S ψ(x l , y i , x j , y j ) will be intensionally interpreted by the concept u 1 ∈ D 7 , obtained by the algebraic expression conj S (u, v) where u = I(φ(x i , x j , x k , x l , x m )) ∈ D 5 , v = I(ψ(x l , y i , x j , y j )) ∈ D 4 are the concepts of the virtual predicates φ, ψ, relatively, and S = {(4, 1), (2, 3)}. Consequently, we have that for any two formulae φ, ψ ∈ Ł and a particular operator conj S uniquely determined by tuples of free variables in these two formulae, I(φ ∧ S ψ) = conj S (I(φ), I(ψ)). 2. The logic formula ¬φ(x i , x j , x k , x l , x m ) will be intensionally interpreted by the concept u 1 ∈ D 5 , obtained by the algebraic expression neg(u) where u is the concept of the virtual predicate φ, u = I(φ(x i , x j , x k , x l , x m )) ∈ D 5 . Consequently, we have that for any formula φ ∈ Ł, I(¬φ) = neg(I(φ)). 3. The logic formula (∃ 3 )φ(x i , x j , x k , x l , x m ) will be intensionally interpreted by the concept u 1 ∈ D 4 , obtained by the algebraic expression exists 3 (u) where u = I(φ(x i , x j , x k , x l , x m )) ∈ D 5 is the concept of the virtual predicate φ. Consequently, we have that for any formula φ ∈ Ł and a particular operator exists n uniquely determined by the position of the existentially quantified variable in the tuple of free variables in φ (otherwise n = 0 if this quantified variable is not a free variable in φ), I((∃ n )φ) = exists n (I(φ)).\nSo, we obtain the following two-steps interpretation of FOL based on two homomorphisms, intensional I, and extensional h: We can enrich the expressivity of such a minimal FOL intensionality by new modal operators, or in different way provided in what follows. As, for example, in Bealer's intensional FOL, where he introduced the intensional abstraction operator, which will be considered in rest of this section, as a significant enrichment of the intensional FOL considered above. In reflective languages, reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect. Reification data is often said to be made a first class object. In programming language design, a first-class citizen (also type, object, entity, or value) in a given programming language is an entity which supports all the operations generally available to other entities. These operations typically include being passed as an argument, returned from a function, modified, and assigned to a variable. The concept of first and second-class objects was introduced by Christopher Strachey in the 1960s when he contrasted real numbers (first-class) and procedures (second-class) in ALGOL.\nIn FOL we have the variables as arguments inside the predicates, and terms which can be assigned to variables are first-class objects while the predicates are the secondclass objects. When we transform a virtual predicate into a term, by using intensional abstraction operator, we transform a logic formula into the first class object to be used inside another predicates as first-class objects. Thus, abstracted terms in the intensional FOL are just such abstracted terms as reification of logic formulae. For example, the sentence \"Marco thinks that Zoran runs\", expressed by thinks(Marco, ⋖runs(Zoran)⋗) by using binary predicate thinks and unary predicate runs where the ground atom runs(Zoran) is reified into the predicate thinks.\nIf φ(x) is a formula (virtual predicate) with a list (a tuple) of free variables in x = (x 1 , ..., x n ) (with ordering from-left-to-right of their appearance in φ), and α is its subset of distinct variables, then ⋖φ(x)⋗ β α is a term, where β is the remaining set of free variables in x. The externally quantifiable variables are the free variables not in α. When n = 0, ⋖φ⋗ is a term which denotes a proposition, for n ≥ 1 it denotes a n-ary concept.\n\nDefinition 7. INTENSIONAL ABSTRACTION CONVENTION:\nFrom the fact that we can use any permutation of the variables in a given virtual predicate, we introduce the convention that if α is not empty such that α β is the set of all variables in the list (tuple of variables) x = (x 1 , ..., x n ) of the virtual predicate (an open logic formula) φ, and α β = ∅, so that |α| + |β| = |x| = n. Only the variables in β (which are the only free variables of this term), can be quantified. If β is empty then ⋖φ(x)⋗ α is a ground term. If φ is a sentence and hence both α and β are empty, we write simply ⋖φ⋗ for this ground term.\nMore about this general definition of abstract terms can be find in [1]. In this paper we will use the most simple cases of ground terms ⋖φ⋗, where φ is a sentence.\n\nCase: Human Robot Spatial Language Interaction\nLet us consider a model of robot for understanding language about space and movement in realistic situations [11,12], as finding video clips that match a spatial language description such as \"People walking through the kitchen and then going to the dining room\" and following natural language commands such as \"Go down the hall towards the fireplace in the living room.\" Video retrieval is a compelling application: in the United States alone, there are an estimated 35 million surveillance cameras installed, which record four billion hours of video per week. Analyzing and understanding the content of video data remains a challenging problem. A spatial language interface to video data can help people naturally and flexibly find what they are looking for in video collections. Studying language used to give directions could enable a robot to understand natural language directions. People talk to robots even if they do not have microphones installed, and it makes sense to build systems that understand what they say. A robot that understands natural language is easy for anyone to use without special training. By using the deductive properties of the IFOL, the robot can make logic deductions as well about the facts that it visually recognized and also to obtain its own autoepistemic deductions about obtained knowledge, as shortly explained in introduction, by using intensional abstractions in Definition 7.\nConsequently, I will focus on a narrow subset of a natural language, grounding that language in data collected from a real world. This strategy has two benefits. First, it decreases the scope of the language understanding problem, making it more tractable. Second, by choosing a semantically deep core domain, it offers an opportunity to explore the connection between linguistic and non-linguistic concepts.\nThe linguistic structure extracted from spatial language expressions and many of the features in the model for spatial relations are based on the theories of Jackendoff [6], Landau and Jackendoff [13] and Talmy [14]. For example, the implementation of the mining of \"across\" in [14] is obtained by an algorithm (of robot's AI neuro-system) for computing the axes a figure imposes on a ground, and set of features which quantify \"roughly perpendicular\", using a machine learning algorithm to fine-tune the distinctions by training on labeled data. Regier [15] built a system that assigns labels such as \"through\" to move showing a figure relative to a ground object. Bailey [16] developed a model for learning the meanings of verbs of manipulation such as \"push\" and \"shove\". Kelleher and Costello [17] built models for the meanings of static spatial prepositions such as \"in front of\" and \"above\". Siskind [18] created a system for defining meanings for words such as \"up\" and \"down.\" The framework reasons about formal temporal relations between primitive force-dynamic properties such as \"supports\" and \"touches\" and uses changes in these properties to define meanings for verbs. His framework focuses on word-level event recognition and features, etc..\nReasoning about movement and space is a fundamental competence of humans and many animals. Humans use spatial language to tell stories and give directions, abstracting away the details of a complex event into a few words such as \"across the kitchen.\" A system that understands spatial language could be directly useful to people by finding video that matches spatial language descriptions, or giving natural language directions. We will consider a robot which retrieves video clips that match a natural language description using a probabilistic graphical model that maps between natural language and paths in the environment [11].\nIn this particular environment, spatial relations are modeled as probabilistic distributions for recognizing words paired with scenes. The distributions are trained from labeled examples using a set of geometric features that capture the semantics of spatial prepositions. The distribution modeled is the probability of a particular spatial relation given a trajectory and an object in the environment. This distribution corresponds to the probability that a spatial relation such as \"across\" or \"to\" describes a particular trajectory and landmark. The input to the model is the geometry of the path and landmark object; the output is a probability that the spatial relation can be used to describe this scene. These distributions are trained using labeled path examples, and in robot's brain correspond to its AI neuro system. The system learns distributions for spatial relations, for example, by using a naive Bayes probabilistic model.\nSo, now we can focus to the integration of such robot's AI neuro system with its AI symbolic system based on three natural language cognitive levels: The syntax of a particular natural language (French, English, etc..) its semantic logic structure (transformation of parts of the language sentences into the logic predicates and definition of corresponding FOL formulae) and its corresponding conceptual structure, which dif-ferently from the semantic layer that represents only the logic's semantics, represents the composed meaning of FOL formulae.\nIn this example, we focus on spatial language search of people's motion trajectories which are automatically extracted from video recorded by stationary overhead cameras. The system takes as input a natural language query, a database of surveillance video from a particular environment and the locations of non-moving objects in the environment. When the robot performs video retrieval by its AI neuro system, clips are returned in order according to the joint probability of the query and the clip. Thus, for each video clip in given database, this robot's neuro system computes the probability that considered clip satisfies a natural language query, parsed into logic FOL formula (second natural language semantic level) and consequently into intensional algebra A int term with intensional concepts which labels are grounded by robot's neuro system processes (algorithms). Let N Ł be a given natural language. If we denote the set of finite nonempty lists of a given natural language words by N Ł list , then this parsing can be represented by a partial mapping where Ł is the set of logic formulae of intensional FOL. We suppose that the concepts in the conceptual structure expressed by the intensional algebra A int of atomic concepts u ∈ D, and their corresponding logic atoms expressed by virtual predicates φ(x) ∈ Ł of FOL are the part of innate robot's knowledge, such that for robot's innate and unique intensional interpretation I : Ł → D, u = I(φ(x)). Moreover, we suppose that robot has a parser capability to transform the sentences of particular natural language into the formulae of FOL with innate set of the atoms expressed by virtual predicates.\nIn this example we consider the predicates of IFOL as the verbs (V) of natural language, as follows where the time-variable x 1 (with values \"in past\", \"in present\", \"in future\") indicates the time of execution of this action, the variable x 2 is used for the object given to robot (in this case a video clip) and x 3 for the statement (users query) that has to be satisfied by this object, and virtual predicate where the time-variable x 1 (with values \"in past\", \"in present\", \"in future\") indicates the time of execution of this action, variable x 2 for the figure (F) that moves (\"person\", \"cat\", etc..), x 3 for the initial position of walking figure (defined by the spatial relation (SR) \"from\", for example \"from the table\") , x 4 for the intermediate positions during movement of the figure (defined by (SR) \"through\", for example \"through the corridor\") , and x 5 for the final position of figure (defined by (SR) \"to\", for example \"to the door\"). The robot takes as input a natural language query, a database of surveillance video from a particular environment and the locations of non-moving objects in the environment. It parses the query into a semantic structure called a spatial description clause (SDC) [12]. An SDC consists of a figure (F), a verb (V), a spatial relation (SR), and a landmark (L). The system extracts SDCs automatically using a conditional random field chunker. Let us consider the example illustrated in Figure 3 in [12] of a natural language query nq ∈ N Ł list , defined by a sentence: Remark: Note that all SDC components different from (V), are particulars in D −1 in PRP domain D, provided by Definition 3. The sense (mining) of the components (F) and (L) are grounded by the machine-learning video-recognition processes of the robot, that is by its neuro systems. The sense of the (SR) components is grounded by the meaning of the spatial relations, provided by different authors methods, mentioned previously, and implemented by particular robots processes. What we need in next is to extend this grounding also to the virtual predicates of the FOL open formulae in Ł.\nConsequently, from these Spatial Description clauses, for the (V) of the past-time verb (V) \"to walk\", the semantic logic structure recognized by robot is the sentence φ ∈ Ł based on the virtual predicate toW alk, W alk(in past, person, f rom the couches in the room, N U LL, to the dining room table) that is, from (7), φ = pars(nq) Note that the inverse parsing of such logic sentence φ to natural language sentence is directly obtained, so that the robot can translate its semantic logic structures into natural language to communicate by voice to the people. We consider that each grammatically plural word name \"videoclips\", robot can define by generalization by creating the virtual unary predicate videoclips(y), such that its intensional concept u 2 = I(videoclips(y)) ∈ D 1 in PRP domain, whose meaning is grounded by robots patern-recognition process fixed by a machine learning method. In a similar way, each unary concept of visual objects can be created by robot by a machine learning method for enough big set of this type of objects.\nSo, each grammatically singular word name, like \"John's videoclip\" is a particular (element of D −1 ) in PRP domain, whose meaning is grounded by the internal robot's image of this particular videoclip, recognized as such by robots patern-recognition process. Thus, for a given extensionalization function h in (2), and fixed robot's intensional mapping I, from the diagram (5), we obtain that the set C, of video clips in a given database of videoclips presented to this robot, is equal to Consequently, the human command in natural language nc ∈ N Ł list to this robot, \"Find videoclip such that φ in the given set of videoclips\" (where φ has to be substituted by the sentence above) is parsed by robot into its second level (semantic logic structure) by virtual predicate F ind of the verb \"to find\" (in present) and a variable y of type \"videoclip\" (objects of research) and substituting \"that φ\" by abstracted term ⋖φ⋗, and by substituting \"in the given set of\" with the logic conjunction connective ∧ S of the IFOL expressed by the following formula ϕ(y) F ind(inpresent, y, ⋖φ⋗) ∧ S videoclips(y) where S = (2, 1) for joined variables in two virtual predicates. That is, from (7), The meaning of the unary concept u 1 = I(F ind(in present, y, ⋖φ⋗)), corresponding to the natural language subexpression \"Find videoclip such that φ\" of the command above, is represented by its AI neuro system process of probabilistic recognition of video clips [12] satisfying the natural language query φ (In fact, u 2 is just equal to the name of this process of probabilistic recognition). However, during execution of this process, the robot is able also to logically deduce the truth of the sentence Know(in present, me, ⋖toF ind(inpresent, y, ⋖φ⋗)⋗ y ) of the virtual predicate Know(x 1 , x 2 , x 3 ), where the time-variable x 1 (with values \"in past\", \"in present\", \"in future\") indicates the time of execution of this action, the variable x 2 is used for the subject of this knowledge and x 3 is used for an abstracted term expression this particular knowledge). Thus, by using deductive properties of the true sentences of FOL, this autoepistemic sentence about its state of selfknowledge, the robot would be able to comunicate to humans this sentence, traduces in natural language as \"I (me) know that I am finding videoclip such that φ\" From the fact that robot defined the type of the variable y to be \"videoclip\", by traduction of the FOL deduced formula above into the natural language, this variable will be traduced in natural language by \"videoclip\". In the same way, during the execution of the human command above, expressed by the FOL formula ϕ(y) in (10), with composed concept u 3 = I(ϕ(y)) ∈ D 1 , that is, by using the homomorphic property of intensional interpretation I, the robot can deduce also the true epistemic sentence Know(in present, me, ⋖F ind(in present, y, ⋖φ⋗) ∧ S videoclips(y)⋗ y ) and hence the robot would be able to communicate to humans this sentence, traduces in natural language as \"I (me) know that I am finding videoclip such that φ in the set of videoclips\" Note that the subset of videoclips extracted by robot from a given set of videoclips C = h(u 2 ) in (9), defines the current extensionalization function h, in the way that this subset is Thus, for the grounding of spatial language for video search, the robot's internal knowledge structure is divided into four levels, in ordering: natural language, semantic logic structure, conceptual structure and neuro structure, as represented by It is easy to see that the conceptual system, based on PRP domain D composed by particulars in D −1 and universals (concepts) in D I = D 0 + D 1 + D 2 + ... of the IFOL, is the level of grounding of the natural language of the robot to its neuro system composed by the following processes:\n\nConclusion\nComputation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. . . Machine learning is a sub-field of artificial intelligence. Classical (non-deep) machine learning models require more human intervention to segment data into categories (i.e. through feature learning). Deep learning is also a sub-field of machine learning, which attempts to imitate the interconnectedness of the human brain using neural networks. Its artificial neural networks are made up layers of models, which identify patterns within a given dataset. Deep learning can handle complex problems well, like speech recognition, pattern recognition, image recognition, contextual recommendations, fact checking, etc.. However, with this integrated four-level robot's knowledge system presented in diagram (14), where the last level represents the robot's neuro system containing the deep learning as well, we obtain that also the semantic theory of robot's intensional FOL is a procedural one, according to which sense is an abstract, pre-linguistic procedure detailing what operations to apply to what procedural constituents to arrive at the product (if any) of the procedure.\nWeak AI, also known as narrow AI, focuses on performing a specific task, such as answering questions based on user input or playing chess. It can perform one type of task, but not both, whereas Strong AI can perform a variety of functions, eventually teaching itself to solve for new problems. Weak AI relies on human interference to define the parameters of its learning algorithms and to provide the relevant training data to ensure accuracy.\nStrong AI (also known as full AI) aims to create intelligent robots that are quasi indistinguishable from the human mind. But just like a child, the AI machine would have to learn through input and experiences, constantly progressing and advancing its abilities over time. If researchers are able to develop Strong AI, the robot would require an intelligence more close to human's intelligence; it would have a self-aware consciousness that has the ability to solve problems, learn, and plan for the future.\nHowever, since humans cannot even properly define what intelligence is, it is very difficult to give a clear criterion as to what would count as a success in the development of strong artificial intelligence. Thus, we argue that this example, used for the spatial natural sublanguage, can be extended in a similar way to cover more completely the rest of human natural language, and hence the method provided by this paper is a main theoretical and philosophical contribution to resolve the open problem of how we can implement the deductive power based on IFOL for new models of robots heaving strong AI capacities."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-14"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 87120,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254854264"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation\n\nPre-trained language models, despite their rapid advancements powered by scale, still fall short of robust commonsense capabilities. And yet, scale appears to be the winning recipe; after all, the largest models seem to have acquired the largest amount of commonsense capabilities. Or is it? In this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., GPT-3), if the smaller models are powered with novel commonsense distillation algorithms? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce a novel commonsense distillation framework, I2D2, that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale models as the teacher model by two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model's own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-Tomic, that is of the largest and highest quality available to date.\n\nIn this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., , if the smaller models are powered with novel commonsense distillation algorithms ? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce a novel commonsense distillation framework, I2D2, that loosely follows the Symbolic Knowledge Distillation of (West et al., 2022) but breaks the dependence on the extreme-scale models as the teacher model by two innovations: (1) the novel adaptation of NeuroLogic Decoding (Lu et al., 2021) to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model's own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is of the largest and highest quality available to date.\nA bicycle has two wheels.\nBicycles are a great way to commute gen-a-tomic Bicycles bicycles are a great way get around the city, but they [repetitive, incomplete] GPT2-XL off-the-shelf ✅ ✅ ❌ ❌ ❌ \"A bicycle has…\" \"Bicycles…\"\n\nIntroduction\nLanguage models (LMs) become increasingly better with scale. However, even the largest LMs continue to fail in unexpected ways due to their lack of commonsense (Brachman and Levesque, 2021). Knowledge models -custom LMs trained to generate knowledge-provide on-demand access to task-specific knowledge to address this gap (Bosselut et al., 2019). Today, the best strategy for training a knowledge model depends on large-scale, albeit noisy knowledge generated from a large LM (West et al., 2022). Are massive-scale LMs the only way to build commonsense capabilities? In addition to being an interesting scientific inquiry, if smaller LMs can indeed generate high-quality commonsense, training knowledge models will become far more efficient and accessible compared to state-of-the-art.\nWe study the generation of commonsense knowledge from GPT-2 (a small LM) and compare that against GPT-3, a model that is orders of magnitude larger. Specifically, we fo-Seed Concept Prompt Construction \"bicycle\" \"Bicycles…\" \"A bicycle has…\" Step 3: Fine-tune the model (and iterate) Self-Imitation Constrained Decoding\n\nGenerate prompts from seed concepts\nStep 2: Use constrained decoding for controlled generation LM Fine-tune LM on its own high-quality generations + (count(function_words) = 1) ∧ (count(connective_words) = 1) ∧ concept_2 ∧ … Updated LM Figure 2: I2D2 is specifically designed to elicit generics-general statements about the world. I2D2 works by collecting a list of concepts and generates generics using Neurologic Decoding to constrain generations at decoding time. To ensure quality, I2D2 includes the use of a supervised critic to filter out false generations. The quality of the generations is further improved via iterative self-imitation learning whereby the language model is finetuned on the high-quality generics selected by the critic.\ncus on the task of generating generics -i.e. statements of commonsense knowledge about everyday concepts. While generics express general truths (e.g. \"birds can fly\"), exceptions abound (e.g. penguins do not fly nor do sleeping or injured birds). Nonetheless, generics form the basis of how we express our commonsense about the world (Hampton, 2012;Leslie, 2014). We present I2D2, a new framework for generating generic statements from GPT-2 (depicted in Fig. 2). 1 Out of the box, GPT-2 generations are anything but valid generics -often being repetitive, trivial or resembling narratives. The key breakthrough for overcoming this challenge comes from (i) constrained decoding: in which generations are controlled to satisfy manually constructed lexico-syntactic constraints using Neurologic Decoding (Lu et al., 2021), and (ii) self-imitation learning: in which GPT-2 is iteratively fine-tuned on its own highquality generations, automatically identified using a supervised critic model.\nThe marked disparity in scale makes the comparison between I2D2 and GPT-3 seem like an impossible match. However, constrained decoding and self-imitation enable I2D2 to overcome this limitation and even surpass the quality of knowledge generated by GPT-3. We formulate a binary-classification task on a humanannotated test set of generic statements and 1 I2D2: Iterative Imitation and Decoding for Distillation compare the precision-recall trade-off between I2D2 and Instruct-GPT-3 by ranking statements using their critic and perplexity scores, respectively. 2 I2D2 achieves an average precision of 0.92 and outperforms Instruct-GPT-3, with an average precision of 0.82. 3 Next, we show that iterative self-imitation learning dramatically improves the accuracy of generations from GPT-2 XL, even before applying the critic; increasing from 45% − → 58% − → 62% over three iterations. Finally, we construct Gen-A-tomic -a knowledge resource of generic statements generated by applying I2D2 to 40K everyday concepts. Compared to GenericsKB (Bhakthavatsalam et al., 2020), Gen-A-tomic is judged by humans to be more accurate (75% Gener-icsKB vs. 90% I2D2) while being larger (over 2X) in scale. Moreover, unlike GenericsKB, which was created through information extraction over text, I2D2 can provide commonsense knowledge for unseen concepts on-demand.\n\nThe I2DFramework\nGenGen is a new framework for automatically generating generic statements using pretrained language models. In this work, our language model of choice is GPT-2 XL. However, any auto-regressive pre-trained language model can be used within the framework. 4 I2D2 generates generics in four stages. First, in prompt construction, we collect seed concepts (e.g. bicycle) and automatically construct several morpho-syntactically varying prompts (e.g. \"A bicycle has . . . \") ( §2.1) for each concept. The prompts are used as inputs to I2D2. Second, we employ constrained generation to control the style of text generated from the pre-trained LM at to mimic the style of generic statements( §2.2). Third, a supervised critic is used to filter out false and ill-formed generations ( §2.3). Finally, the language model is finetuned on its own high-quality generations selected by the critic in an iterative self-imitation learning setup( §2.4). Figure  2 illustrates the overall framework.\n\nPrompt Construction\nSource of seed concepts: Our first set of concepts for generating generic knowledge are common noun phrases (e.g. \"fruits\"), selected from two resources: GenericsKB (Bhakthavatsalam et al., 2020) and ConceptNet (Speer et al., 2017). From GenericsKB, we retrieve all noun phrases for which there are at least five generic statements in the resource, resulting in a total of 8.5K seed concepts. 5 From Concept-Net, we retrieve noun phrases associated with the types artefact and human, identified based on hypernymy relationships to the corresponding WordNet senses. The two lists are then manually vetted to compile a list of artefacts and humans, totaling 1.4K seed concepts. 6 Our second set of seed concepts are highlevel human goals (e.g. \"get better at chess\") obtained from two sources: ProScript (Sakaguchi et al., 2021) and ATOMIC . We extract all goals that appear in the ProScript training data. From ATOMIC, we extract all base events and filter out hypothetical events (e.g. \"PersonX expects to win\") 4 In the rest of the paper, I2D2 refers to I2D2 using GPT-2 XL. 5 GenericsKB was found to consist of uncommon or specialized terminology (e.g. orpiment) that are not conducive for commonsense generation. Therefore, we select nouns with at least five generic statements so that the collected nouns are those that are capable of forming commonsense generics 6 We choose human and artifact as much commonsense knowledge centers around these types. The list of concepts can be extended to other types as well (e.g. animals, natural phenomena) in the future.\nbased on a short exclusion list (see Appendix A.1).\nTo scale the number of seed concepts we prompt GPT-3 (Brown et al., 2020) with a set-expansion template, which are prompt templates for GPT-3 to generate items similar to a given set of items; see more details in Appendix A.1.1. Overall, after GPT-3 based expansion, we have 39K seed concepts, consisting of 26K noun phrases and 13K goals. Note that GPT-3 is only used for seed expansion and not for the generics generation.\n\nMorpho-Syntactically Varying Prompts:\nWe programmatically construct a large number of morpho-syntactically divergent prompts for each concept to facilitate the generation of a diverse set of generic statements. Prompts for noun phrases are constructed based on the template described in Table 2: Each concept is paired with a relational phrase, e.g. \"can be\", \"is found in\" etc., from a manually constructed list; Appendix A.1.2 presents more details. Inspired by Leslie (2008), we prefix adverbs (such as \"generally\", \"usually\", and \"typically\") to the prompts. We find, empirically, that these prefixes encourage the language model to generate general statements, instead of long-form, narrative-like text. An article is optionally prefixed before the concept for grammaticality. For a given (concept, relational phrase) pair, we construct all prompt combinations according to the template above and choose the one with the lowest PLM (GPT-2 XL in our experiments) perplexity. For the goal seed concepts, from each goal we create four separate prompts by prepending each of these prefixes: \"In order to\", \"Before you\", \"After you\" and \"While you\".\nFinally, we filter out all prompts whose perword perplexity under GPT-2 XL is above a threshold of 250. This allows us to apriori filter out ill-formed prompts such as \"Typically, a hall are planted at . . . \". This results in a total of 1.6M prompts.\n\nConstrained Generation\nGiven a prompt (e.g. \"Generally, a bicycle has\"), a key challenge for a language model is to generate continuations consistent with the linguistic style of generic statements. Unconstrained, a language model tends to go into a Input prompt Related Concept I2D2 output In order to get better at chess -. . . you have to practice chess. In order to get better at chess improve . . . you have to improve your strategy. In order to get better at chess tactics . . . you have to practice tactics. In order to get better at chess strategy . . . you have to learn strategy. Table 1: Example outputs of I2D2 for the concept \"get better at chess\". We add constraints to our constrained generation algorithm to include the related concept.\n[Generally|Typically|Usually]? [a|an|the]? <noun phrase> <relational phrase> \"story-telling\" mode which can result in long sentences that resemble a narrative. In contrast, generic statements are typically simple and short (Tessler and Goodman, 2016). Thus, we aim to control the number of function words (e.g., \"in\", \"on\", \"of\") in the generated sequence to limit the length of the sequence. We further wish to disallow the generation of connectives (e.g., \"although\", \"since\", \"furthermore\") to make the statements short and succinct. We also want to avoid repetition and trivia in the statements. To steer the generations to meet these various requirements, we apply Neurologic Decoding (Lu et al., 2021), a beam-based decoding algorithm that allows us to impose desired lexical constraints during generation. We devise negative constraints that limit generation of certain tokens (e.g., function words or repeated tokens as above) in the outputs. We devise positive constraints to encourage the inclusion of the optional related concept to generate generic statements relating two concepts. We describe all the constraints in more detail in Appendix A.1.4. Given the 1.6M programmatically constructed prompts, we generate ten generations for each prompt, resulting in 16M generics which must now be filtered to preserve quality.\nMany applications require knowledge that relates two known concepts. For example, to solve a QA problem, it might be important to have background knowledge about a the relationship between a \"hotel\" and a \"credit card\", e.g. \"At a hotel, credit cards can be used to make a payment\". Our constrained decoding algorithm enables us to do that. We obtain concepts related to a seed concept from GPT-3 using a custom template; see details in Appendix A.1.3. In I2D2, we aim to generate continuations to a prompt that also mention a concept related to the seed concept.\n\nSupervised Critic\nWhile our constrained decoding method results in generations that resemble generic statements, a lot of the generics tend to be invalid or false statements. This is consistent with the finding that LMs can generate hallucinations and false statements about the real world (Ji et al., 2022). To address this, we train a supervised critic model to predict the veracity of a generation. We create a training set of ∼12K statements, containing up to four sampled generations for each concept from a held-out set of ∼3K concepts. The labels for each generation are collected using the same procedure as the evaluation data, which is described in Section 3.1. We train a RoBERTa-Large (Liu et al., 2019) classifier as our critic model and used as a binary classifier to identify valid generic statements.\n\nSelf-Imitation Learning\nWhile the constrained decoding followed by filtering with the supervised critic can already result in a desired set of generics, we intend to leverage the strength of these stages to further generate more high quality generics. We achieve this by finetuning GPT-2 XL on the high-quality subset of generics in an iterative fashion. In particular, we perform two iterations of fine-tuning in our experiments. This method can be viewed as self-imitation learning (Oh et al., 2018), such that the 'actor' corresponds to the base GPT-2 language model and the 'critic' corresponds to the RoBERTa discriminator. Some related recent works (Oh et al., 2018;Thoppilan et al., 2022;Haluptzok et al., 2022) show that language models can become better at generating high-quality programs if they are fine-tuned on their own previously generated correct code examples, as evaluated by a compiler. However, previous work did not test multiple iterations of self-imitation using language models.\n\nI2D2 generates better generics than GPT-3\nOur goal in this work is to investigate whether smaller language models like GPT-2 XL can still offer use, while larger models like GPT-3 continue to outperform them on virtually all NLP tasks. In particular, we show that our I2D2 approach for generating generic statements outperforms GPT-3 while being considerably more cost-effective, accessible and scalable.\nSystems We wish to compare how GPT-3, given the same set of prompts, can generate and identify valid generics. For a given prompt, we generate ten generations from each system. GPT-3 is prompted in a few-shot manner with an instruction and six examples. We use different sets of few-shot examples for noun phrases and goals. Appendix A.1.6 presents more details about the instruction and in-context examples provided to GPT-3. I2D2, using a supervised critic, assigns a score to each generated statement. For GPT-3, we use the perplexity assigned to a generation as an indicator of validity. As an additional baseline, we also compute perplexity under off-the-shelf GPT-2 XL.\nEvaluation Data We set aside 300 concepts for evaluation. Each concept is associated with several prompts (on average 40). We generate ten generic statements for each prompt from I2D2 and GPT-3. Next, from all generations for a concept, we randomly sample four statements generated by each system being compared. A generic statement is considered valid if it is a generally true statement about the world. Three annotators on the Amazon Mechanical Turk platform rate the validity of each generated statement. Details of the annotation template and instructions are provided in Appendix A.1.5. At least two out of three annotators agreed on a label 92.5% of the time over all 4 statements. 7 For the annotation task, we maintain a pay rate of at least $15/hr.\nMetrics Given the human-annotated test set of generics, we compare the precision-recall trade-off between I2D2 and GPT-3. Each system assigns a score to each generic statement, allowing us to rank the statements from most likely to least likely to be a generic. Combined with the human annotations of the validity of a statement, we can plot a precision-recall (PR) curve. PR curves allow us to evaluate the accuracy of each system's output as the number of statements output varies, which is important since different tradeoffs between quantity and quality of output may be desired for different application settings.\nResults Figure 3 shows the impact of including a supervised critic to identify valid generic statements. We find that GPT-3, while impressive, lags significantly behind our supervised critic in identifying which generic statements are valid. Off-the-shelf GPT-2 XL model is the worst at identifying valid generic statements. Perplexity alone is not a good indicator of what a valid generic is. I2D2 uses both a generator and a discriminator. To evaluate the generator, we sample from its generations over the test set of prompts. For a given set of generations, human annotators judge whether the statement is true or false. We compute accuracy against human labels and use that as a metric to measure the quality of the generator.\n\nI2D2 gets better through iterative self-imitation learning\nSystems For self-imitation learning, we generate a large corpus of generations and filter out invalid statements using the supervised critic to yield a \"purified\" subset. We compare generations from I2D2 using off-the-shelf GPT-2 XL and outputs from two additional iterations  Evaluation Data We use the same held-out test set of prompts for this experiment.\nMetrics We evaluate the accuracy of the generations before and after applying the supervised critic.\n\nResults\nWe show that a language model gets iteratively better as it gets finetuned on its own high-quality generations over each iteration. The raw accuracy of the generations, before applying the critic, improves from 45% → 58 % → 62% over three iterations. We also compare the precision-recall trade-off between the three iterations. Figure 4 shows the effectiveness of self-imitation learning over three iterations.\n\nGen-A-tomic corpus is more accurate than GenericsKB\nGenericsKB (Bhakthavatsalam et al., 2020) is a static resource of generic knowledge created through information extraction over three large text corpora: the Waterloo corpus, Sim-pleWikipedia and the ARC corpus. This work released a large scale dataset of 14M generations and a high-quality subset of 1M generic statements. We compare GenericsKB's best 1M against our corpus. We randomly sample 1K generic statements from GenericsKB and I2D2 and ask annotators on Amazon Mechanical Turk to rate the validity of the generic statement. We find that while only 76% of statements in GenericsKB were annotated as accurate, over 90% of statements in I2D2 were judged as valid. The results show that I2D2 is more accurate than than GenericsKB, in addition to being larger.\n\nSmaller, better-trained versions of GPT-3 outperform larger ones\nWe compare three versions of the GPT-3 model available on the OpenAI APIdavinci, curie-instruct and davinci-instruct (Ouyang et al., 2022;Brown et al., 2020). Interestingly, we find that the curie-instruct model, despite being a much smaller model, generates more valid generic statements compared to the much larger davinci model. The instruct models (including curie-instruct) were trained using reinforcement learning on human feedback. The accuracy (validity) of statements generated by the three GPT-3 models on the same set of test prompts are 53.3% (davinci), 60.6% (curie-instruct), and 81.9% (davinci-instruct). These results further demonstrate that better training can result in smaller models performing better than larger models.\nOur work adds to the growing body of evidence from recent work that large language models have not been trained optimally (Kaplan et al., 2020) and it would be worthwhile to look for better training strategies to achieve high performance using smaller, affordable, greener models.\n\nAnalysis\nDiversity For diversity analysis we employ a survey method called Mark and Recapture (MnR) (Seber et al., 1982;The U.S. Geological Survey, 2018) commonly used by ecologists to estimate a large population size via sampling. This method captures individuals of a population in two (or more) stages. In the first capture, the individuals captured are marked and released. At a later capture, the number of recaptured individuals are counted and the population size estimated.\nWe use MnR to estimate the unique population size of our large datasets thereby gauging the diversity of the dataset. For our implementation of MnR, we perform two random captures using a sample size of 30% (of the total dataset size) at each capture. A generic in the second capture is considered a recapture (i.e., individual seen in the first capture) if it exceeds a textual similarity threshold (BLEU score > 0.85) 8 with the generics of the same concept from the first capture as the reference. Then, we employ the Chapman estimator for MnR (Brittain and Böhning, 2009;Chapman, 1951) to estimate the population size of unique generics in the dataset.\nWe compare the estimated per concept average count of unique generics for Generic-sKB and Gen-A-tomic. Overall, we find that Gen-A-tomic is estimated to include at least triple the amount of generics per concept in comparison to GenericsKB. We also observe that the estimated unique generics per concept is higher for the best cuts of the Gen-A-tomic dataset.\nA more direct approach for diversity perhaps is an embedding-based method where the unique generic population is estimated via its textual difference from its semantic neighbors. However, the process becomes quickly unscalable for large datasets. In preliminary experiments, however, we find that the embeddingbased approach also results in similar trends.\n\nRelated Work\nGenerics Generics like \"dogs are friendly\" are generalizing statements about observed 8 The threshold was determined via several rounds of experimentation and manual evaluation to determine a reasonable level of textual similarity. \"truths\" or defaults about the world for which exceptions can be found (e.g., not all dogs are friendly in practice). Generics have been studied quite extensively in philosophy, linguistics, and psychology. While they are clearly important to human reasoning-especially to nonmonotonic reasoning (Carlson and Pelletier, 1995;Pelletier and Asher, 1997), they have also been long debated for their puzzling properties which renders them difficult to formally analyze (Leslie, 2012(Leslie, , 2008Hampton, 2012;Liebesman, 2011). Bhakthavatsalam et al. (2020) demonstrated the usefulness of generics in language understanding by feeding large-scale generics knowledge to text models and showing improvement on question-answering and explanation generation. However, being a static resource, GenericsKB cannot provide knowledge for unseen concepts. To be useful across a wide range of tasks and datasets, a more comprehensive resource of generics is required. I2D2 can generate generics for arbitrary concepts and generics relating two concepts -a feature unique to I2D2. I2D2 can also be easily extended to generate temporal (\"during a cold night, people need a blanket\") or comparative (\"a tennis ball is smaller than an office chair\") generic knowledge, leading to a commonsense knowledge model that is more comprehensive.\nCommonsense Knowledge Various methods for representing commonsense konwledge have been proposed in the literature. Con-ceptNet (Speer et al., 2017) focused on the conceptual commonsense relationship among various concepts and entities in their knowledge graph. Atomic  and Atomic2020 (Hwang et al., 2021) have offered symbolic commonsense knowledge graphs representing relational inference focusing on the \"If-Then\" (cause-effect) reasoning. Fine-tuned on Atomic, Comet (Bosselut et al., 2019) have offered a neural knowledge model that can reason about situations beyond the symbolic knowledge graphs. Unlike our current framework however, previous commonsense knowledge models typically only handled data in the form of structured triples and were predominantly focused on commonsense about events.\nI2D2 is the first knowledge model that has focused on generic knowledge, generated in natural language. Uniquely, we also provide a critic model that can filter invalid or ill-formed generations.\nSymbolic Knowledge Distillation Collecting high-quality knowledge at scale has been a longstanding challenge. The traditional way is to collect by human annotation (Speer et al., 2017;, which can be timeconsuming and expensive. Bhakthavatsalam et al. (2020) extracted generics by filtering and cleaning based on 1.7B sentences from three large text corpora. However, manually constructed resources and resources extracted from large corpora can be difficult to extend. Recent works showed that pre-trained language models can be a good source of knowledge (West et al., 2022;Zhang et al., 2022). Symbolic knowledge distillation (SKD) (West et al., 2022), for instance, have generated even-centric inferential knowledge from GPT-3 and distill it into GPT-2. While these methods presents promising results, they primarily relies on using GPT-3 and only handles knowledge about events in a structured triple format. I2D2, on the other hand, relies only on GPT-2's own generations to improve itself and generates knowledge in natural language.\nSelf-Imitation Learning Self-imitation learning (Oh et al., 2018) was proposed as a reinforcement learning method in which an agent learns to replicate past good actions. More recently, a similar approach was applied in dialog models (Thoppilan et al., 2022;Xu et al., 2022) and code generation (Haluptzok et al., 2022). However, recent applications have relied on models much larger than GPT-2 XL used in I2D2. Moreover, while (Haluptzok et al., 2022) have explored the idea of self-imitation learning in language models, their work crucially relies on a compiler to identify correct programs. However, compilers are deterministic and by definition, 100% accurate. Instead, the supervised critic in I2D2 can be noisy, especially for identifying generics, which have paradoxical properties that make its formalization very difficult (Mari et al., 2012). We also show that self-imitation learning is beneficial when done over multiple iterations. In principle, I2D2 could be improved iteratively through a life-long learning process. But, under what conditions would the performance gains plateau is an interesting open future research question.\n\nConclusion\nWe present I2D2-a novel framework for generating generic knowledge from language models using constrained decoding and self-imitation learning. I2D2, while using orders of magnitude fewer parameters, can still outperform GPT-3 at the task of generating high-quality generic statements. We also show that I2D2 is higher-quality, larger-scale and more diverse than the static GenericsKB dataset. I2D2 provides on-demand access to generic knowledge that can bridge the gap in commonsense knowledge, often observed in even the larget LMs available today."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 91297,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254520716"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "A Thousand Agambens to Replace the One We Have\n\nReview of Adam Kotsko (2020), Agamben’s Philosophical Trajectory. Edinburgh: Edinburgh University Press, pp. 241.\n\nWhoever has read a Giorgio Agamben chapter or essay has probably wondered about one of his peculiar stylistic habits: he often writes disconnected paragraphs on widely diverging topics. On a single page, he mixes a philological remark about Aristotle, criticisms of Hobbes, and Benjaminian musings about divine violence. Nonetheless, readers are always struck by the intuition that these disparate paragraphs evoke a unified argument. They never doubt that these statements have a single clear message, however disconnected their topics. Agamben freely associates across the history of Western thought, yet every statement forms a microcosm bearing the signature of the entire chapter. Readers themselves are consequently tasked with reconstructing the underlying idea that animates these diverse remarks. They take on the role of psychoanalysts decoding the sentences as symptoms of an implicit apparatus pulling the strings from behind a curtain of words. This temptation to decipher an arché-text behind Agamben's explicit discourse has convinced many interpreters to look for a single philosophical problematic not only in Agamben's essays or chapters, but also in his overall philosophical oeuvre. Leland De La Durantaye, for instance, argues that Agamben's philosophical trajectory is one singular sustained meditation on potentiality (De La Durantaye 2009). He argues that Agamben journeys through metaphysics, political philosophy, and linguistics to ultimately come to terms with what it means for a human subject to have the potential to do something and to have that potential taken away from them when they are reduced to the status of bare life. Sergei Prozorov, on the other hand, reads Agamben's itinerary as a persistent attempt to escape sovereign politics (Prozorov 2014). In this reading, even books on arcane topics in animal biology or theology serve to reflect upon the political opportunities to escape the power of the State. Agamben himself has encouraged such readings by often presenting his oeuvre as if it were motivated by a single purpose. In the epilogue of The Use of Bodies, for instance, he writes that he wanted to \"call into question the place and the very originary structure of politics, in order to bring to light the arcanum imperii\" (Agamben 2015, 263). After all, one does not write a multiple-book project spanning 20 years and 9 books if one does not believe to be engaged in a single philosophical inquiry.\nSuch readings have become troublesome during the last few years due to the Corona Pandemic. Agamben has become notorious for his criticisms of governmental policies like lockdowns, vaccination requirements, and social distancing. 1 There are clear similarities between Agamben's opposition to these policies and his critique of modern biopolitics in books like Homo Sacer, so one cannot simply dismiss Agamben's controversial political interventions as somehow unrelated to his overall philosophy. There is no way of distinguishing clearly between Agamben the philosopher and Agamben the person in this debate. It is thus tempting to re-read Agamben's entire oeuvre as a prefiguration of his political missteps today. If Agamben's critique of modern biopolitics leads to wrongheaded opinions today was Agamben's approach then not absurd all along? The pandemic subsequently becomes the new arché-text for the interpretation of Agamben's philosophical development (cf. Bratton 2021). The downside of\n\nA Thousand Agambens to Replace the One We Have\nTim Christiaens this approach is that other, more interesting potential readings of Agamben's oeuvre are marginalized in favour of one master narrative. Adam Kotsko's Agamben's Philosophical Trajectory, however, takes aim at this monolithic interpretive strategy. He even chooses not to mention the Coronavirus Pandemic to avoid such kind of distractions. The aforementioned reading strategy ignores the multifarious shifts and turns in Agamben's philosophical career and even in the \"Homo Sacer\"-project itself. Agamben frequently changed his mind about the ordering of the books in the overall project, often rephrased earlier arguments to fit newer concerns, and he even added chapters to Stasis and The Use of Bodies at the very end, when the project was published separately in an omnibus edition. These are not signs of a man who, with the publication of Homo Sacer in 1995, knew exactly how the project would end in 2014. Nor is it very likely that Agamben would have already developed his entire philosophy from the start of his career, as some claim. Many concepts vanish or are rearticulated over the course of a career that spans more than 50 years. Whoever reads about Aristotle's distinction between potentiality and act in The Man without Content, Agamben's first book, will not recognize the \"official\" Agambenian interpretation from almost 30 years later. Concepts central to his thought at some point, like \"Voice\", \"whatever being\", or \"testimony\", simply disappear in later books.\nKotsko chooses a different approach to writing an overview of Agamben's oeuvre. His concern does not rest on the discernment of a single philosophical apparatus animating all of Agamben's individual writings. Other interpreters tend to reduce Agamben's books to steps in a uniform argument about a single problematic, like potentiality, anti-sovereign politics, or pandemic biopolitics. But, if this were truly possible, then why would Agamben ever have written more than one book? If all his texts amount to the same argument anyway, it seems that Agamben could have spared himself the trouble of publishing almost 40 books. Kotsko, on the contrary, has read all books in chronological order and simply reports his findings without striving toward a unified message. Aided by personal conversations with Agamben, he carefully tracks the multiple thought processes, the promising hypotheses, and creative conclusions, but also the mistakes, hesitations, and inconsistencies across Agamben's texts to highlight the discontinuities. Kotsko's meticulous reading dismantles all hope of finding a single arché-text in Agamben. He rather divides Agamben's philosophical trajectory roughly into four periods, though he emphasizes that there is never any hard break akin to a Heideggerian Kehre. Old thoughts or assumptions never truly disappear, but become rearticulated into new contexts. Likewise, concepts that seem to be new are often already signaled in earlier texts without being fully elaborated upon.\nIn the first phase, between the 1960s and '80s, Agamben is an almost aggressively apolitical thinker interested in art and linguistics. If one would stop reading before the '90s, there would be no way of guessing that Agamben would become one of the most famous political theorists today. He was entirely enveloped in philosophy of art and the establishment of a so-called \"general science of the human\" built on a critique of structuralist linguistics. According to Agamben, the structuralist definition of language as a system of signs ignored that language actualizes itself only through human beings actually speaking that language. This created, for Agamben, a productive rift in language itself between the fixities of its signifying system and its incarnation in human speech. Agamben believed, in that stage of his career, that (political) philosophy had ignored this rift and that poetry was a superior medium for reflecting on humanity's linguistic condition. Only in the 1990s did Agamben enter a second stage of his philosophical itinerary with a turn to the political. Though he previously had held the politics of his time in dire contempt, his friendships with Guy Debord and Jean-Luc Nancy, together with his worries about the emergence of refugee camps in Italy after the Yugoslav Civil War, convinced him to start studying politics. He fears that the state of exception is the ultimate truth of modern biopolitical government: once the State apparatus and the survival of the population is put under pressure, governments tend to suspend democratic participation and the Rule of Law. Ultimately, the State itself and its violent response to social disruption paradoxically becomes the main threat to people's survival. Here, Agamben embarks on the \"Homo Sacer\"-project that would define the rest of his career. This is also the period where Agamben reaches the peak of his fame with books like Homo Sacer, State of Exception, and The Time that Remains.\nIn a third phase, at the end of the 2000s, Agamben turns increasingly to the history of Christian theology. He becomes convinced that an adequate critique of Western modernity must reckon with the latter's roots in medieval Christianity. In books like The Kingdom and the Glory, Opus Dei, and The Highest Poverty, Agamben stresses the ways Christianity has given rise to modern capitalist government. This strategy allows Agamben not only to critique of (neo)liberal economics as secularized theology, but also to incorporate Foucault's newly published governmentality lectures and to finally articulate the link between his own critique of modernity and that of Debord, which was explicitly promised in the introduction to Homo Sacer. He argues that Debord's pessimistic analysis of the modern public sphere as a big capitalist spectacle was prefigured in the way the medieval Church supported its popular legitimacy through strict rituals and grandiose iconography. Just like the Church kept up the illusion of an authoritative God through rituals that cunningly suggested God's glory without ever having to prove it, the State and capitalism sustain their legitimacy through the illusion of public debate and ceremonial displays of power. This is also the period that Agamben starts reflecting more thoroughly on his philosophical method, mainly in The Signature of All Things. Homo Sacer had given rise to multiple misunderstandings about the way Agamben formulated his political philosophy, so Agamben felt he needed to clarify the contours of his basic methodological concepts like \"paradigm\", \"signature\", and \"archaeology\". In phase four, which spans from when Agamben started working on The Use of Bodies to today, he has increasingly looked back on his philosophical life with more autobiographical writings, like his autoritratto, and books that discuss the fundamentals of his oeuvre or return to his earliest interests, like What is Philosophy? or Adventure. Now that the \"Homo Sacer\"-project is finished, Agamben has taken the opportunity to reflect on his philosophical career and to delve into some side-projects that he failed to incorporate in earlier volumes. Though Kotsko does not mention them, Agamben's Corona essays can also be understood as a late side-project where Agamben tries to come to terms with his own legacy. And one can rightly be skeptical about whether this Agamben succeeds at living up to his former self (Esposito 2020).\nKotsko discourages the reader to decipher a single master narrative hidden in all of Agamben's works. The entire oeuvre is rather a multitude of attempts to engage with manifold, different topics. Agamben has tried to balance his own personal creative insights with adequately responding to the challenges of his days, and both have shifted over the years. However, that does not mean all of Agamben's works are simply standalone pieces with no internal consistency. Agamben frequently recapitulates and rearticulates old ideas to put them to work in new contexts. He is, above all, interested in the so-called \"Entwicklungsfähigkeit\" of philosophical concepts. He takes concepts from their original contexts and puts them together to generate developmental capacities that were not present in the original context. By, for instance, confronting Foucault's analysis of biopolitics with Carl Schmitt's theory of sovereignty in Homo Sacer, Agamben managed to produce reflections that were only vaguely present in both of these authors' own texts. The aim has always been to experiment with the inherent productivity of concepts, which means Agamben has never been the master of his own discourse, but has rather been following where the concepts' developmental capacities led him.\nKotsko calls for a similar approach to Agamben's own concepts: \"I aim […] to prepare the ground for a thousand Agambens to bloom -in their own enigmatic, idiosyncratic, and fascinating ways\" (Kotsko 2020, 13). In reading Agamben's oeuvreor any philosophical text for that matter -the audience actively reconstructs the text's developmental capacities. This constitutes a creative encounter that cannot be simply replicated for everyone in exactly the same way. Each reader must uniquely gauge the potential of the text. Every singular encounter with Agamben's writings can give rise to a new Agamben that is not necessarily compatible to all other readings. This implies, for instance, that Agamben's particular response to the Corona Pandemic is not necessarily the only \"Agambenian\" response imaginable. Other readings of his work can be provided with other outcomes. To mention just one example, one could use his concept of \"bare life\" not to criticize lockdowns, but rather to criticize the precarization of essential workers. While many middle-class families have been working from home in relative comfort, working-class individuals have had to expose themselves to the risk of infection to keep themselves financially afloat (cf. Butler & Yance 2020, De Cauwer & Christiaens 2021. To use a Foucaultian metaphor, Kotsko repurposes Agamben's philosophy as a conceptual toolbox with which the philosophers of the future can build new theoretical edifices. Kotsko himself suggests to redirect Agambenian thought to issues of race, gender, or environmentalism, but one can readily imagine even more Entwicklungsfähigkeiten for Agamben, like digital ethics, neo-fascist populism, or financialization. As the Corona Pandemic has demonstrated, a single Agamben can be deeply flawed, but a 1000 Agambens might be up to the task of prying open the arcana imperii of contemporary politics.\nThere is, however, one serious risk in Kotsko's strategy that his book leaves untouched. Though I agree with his proposal to repurpose Agamben's oeuvre as a polyvalent toolbox, I doubt whether Kotsko has adequately identified his intellectual opponent. It might be true that, in the past, many Agamben scholars have organized his thought under a single header. The same trend can be found in the earliest introductions to Martin Heidegger, Ludwig Wittgenstein, or Michel Foucault. Many secondary literatures on \"new\" thinkers go through a phase of scholars presenting the philosophers' thought as a uniform project. Once this overview has been established, however, critics emphasize the uniqueness of particular books or discontinuities in a philosopher's development. Especially when archives open up and collected works are being published, scholars leave the general narrative behind to focus on the particulars. With Agamben as well, the last few years have predominantly seen publications on particular themes in Agamben's overall oeuvre rather than general overviews. Though such attention to detail delivers fascinating new insights, there is also a looming danger of reducing the writings of these thinkers to stand-alone museum pieces that communicate nothing but their mere useless presence. Like a Greek vase in a museum only presented in order to be admired and catalogued but never used, philosophical concepts can suffer from sclerotic museification as well. Instead of putting philosophers like Wittgenstein or Foucault to use as conceptual toolboxes, scholars subsequently argue over whether the word \"game\" has the same meaning in two different aphorisms of Wittgenstein's Philosophical Investigations or they merely list the 14 different meanings of the word \"norm\" in Foucault's lectures from January 1974 to February 1975. These concepts are withdrawn from the sphere of general use and put on a pedestal to be admired, described, and categorized. Such detailed philological scrutiny is essential to proper philosophical research, but if the underlying concepts lose their connection to the world they describe, the whole endeavour becomes pointless. Agamben himself is no stranger to the minutiae of philology, but he has also been the victim of a professorial class that endlessly complains about his creative readings not being \"true\" to authors' original intentions. Agamben's interpretations of impotentiality in Aristotle, boredom in Heidegger, or bare life in Benjamin might not have been entirely up to date with contemporary philological research, but they have withheld these philosophers from becoming useless museum pieces in an entirely self-referential hall of the Western Canon. Philosophical Investigations, Discipline and Punish, or Homo Sacer have been written to reflect on the human condition, not to be archived in a sterile history of the philosophy curriculum. By defending the chronological reading of Agamben with a focus on the discontinuities, Kotsko might encourage the blossoming of a 1000 Agambens reflecting on issues of race, gender, or the environment, but he might likewise be playing in the hands of the museum curators who want to keep the 1000 Agambens safe behind protective glass. The emphasis on discontinuity should thus be coupled on an equally vocal emphasis on use over curation. Though Kotsko himself explicitly makes this connection, it is up to future Agamben scholarship to keep this project alive."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-08"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 93909,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255251027"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Hermeneutics of Reception by Hans Robert Jauss: An Alternative Approach Toward Quranic Studies\n\n: This research attempts to offer hermeneutics of reception by Hans Robert Jauss as an alternative to understanding the Quran. Starting from the concern of some contemporary Islamic thinkers about the limitations of the classical Quran in overcoming human problems that are always dynamic, hermeneutics has become a much-demanded approach. In addition, the methodological principle of classical interpretation tends to forget the role of human participation in interpreting the Quran. Nevertheless, the practice of the reception approach has stagnated in the study of the living Quran, which only performs a simple analysis of a tradition into three reception typologies: exegesis, functional, and aesthetic, without any more profound critique. This article aims to describe Han s Robert Jauss’ hermeneutics of reception as a relevant offer to fill the void of Quranic studies from the reader's perspective. For this reason, this article is compiled using qualitative methods based on literature studies so that essential aspects that need to be considered as material for hermeneutics of reception analysis can be well elaborated. There are three crucial aspects to the hermeneutics of reception; the Horizon of expectations, the three levels of reading, and the validity of the aesthetic experience.\n\n\nIntroduction\nSo far, the life of Muslims is inseparable from the Quran, which is used as a guide to life. The Quran, which introduces itself as hudan li al-nas, is believed to embody God's desires that humanity must realize. This dogma always makes people communicate emotional life problems with these sacred texts. Muslims interpret the Quran as a concrete effort by Muslims to get problem-solving according to the corridor. This interpretation activity has been carried out since the beginning of the passing down of the Quran today and will continue to be carried out until the future. This activity then explores methodological concepts to interpret the Quran that we are currently studying in the disciplines of Quranic interpretation and its classical theory (ulum al-Quran). As times progressed, the methodology of interpretation evolved; from the tafsir bi al-ma'tsur (Tradition-based tafsir) in the time of the prophet, companions, and tabi'in, to the tafsir bi al-ra'yi (reason-based tafsir) that developed during the time of the Abbasiyyah daulah. (Wibowo, 2017, pp. 1-2). The ever-dynamic problems experienced by humanity are not accommodated by normative methodological devices that tend to forget the audience and only dwell on the interpreter's relationship with the text of the Quran. The effect is that classical interpretations do not give their clear meaning and role to the life of humanity. (Wibowo, 2017, p. 3).\nM. Amin Abdullah, affirmed by Abdullah Saeed, views the classical approach and method of interpretation as not fully accounting for the ever-evolving and changing needs and problems of Muslims. The classical performance so far has only repeated comments, destabilizations, and repetitions of explanations, yet to be suggestive of human problems and needs. The awareness of the absence of methodological renewal has led Muslim scholars to move quickly to answer the challenges of an increasingly diverse era, along with the problem of the distance between the time the Quran was revealed and the time the Quran was interpreted. Seeing this phenomenon, Muslim thinkers tried to break the stagnation through other perspectives by making it a new paradigm in studying the Quran. So that the problem of distance between the first time the Quran was handed down to the present can be solved, and the Quran is always relevant in all ages. Reflecting on this reality, hermeneutics became one of the offers that complemented the gap from the weakness of classical interpretation science to the problems of the modern era (Wibowo, 2017, pp. 3-4).\nIn line with the concerns of interpreters about the lack of role of the reader and his need to understand the text of the Quran, Jauss had the same problem about the lack of readers' role in finding meaning due to the two great understandings that dominated at that time, the formalists and Marxists. Orthodox Marxists tend to return the importance of the text to its author, in which a reader must be the author (Sujarwa, 2012, p. 62). While the formalist is the opposite, it sees the text as autonomous so that the reader can only talk about the intrinsic elements of the text and its intertextual aspects (Parris, 1999, │ 162 p. 137). These two great groups leave the essential dimensions; the reader and the reception (Sujarwa, 2012, p. 62).\nAs Quranic studies developed, in the triadic of hermeneutics, author, reader, and text, it turns out that the focus of meaning is not only on the author. An approach that examines more deeply the reader's perspective as a meaning-giving authority can be found in reception theory. So far, the term reception is often mentioned in living Quran studies. Reception theory is the formal object of study of living Quran or hadith (Hasbillah, 2019, pp. 54-55), but reception theory in this realm often simplifies three reception typologies by Ahmad Rafiq; hermeneutical receptions, available receptions, and aesthetic receptions (Rafiq, 2014, pp. 147-154). The emerging research only classifies Quranic phenomena in society based on the reception of the three categories without detailing how the reader builds his understanding so that such acceptance appears. Instead of examining traditions or rituals in a community, these studies seem to be descriptive descriptions of phenomena or patterns that occur in society and simplify the reception typology proposed by A. Rafiq subjectively and less analytically. In his article entitled \"Reception of the Quran of the Gemawang Mlati Society of Yogyakarta,\" M. Ulil Abshor classified the typology of the Gemawang community reception for reading the Quran and did not focus on one particular lesson. Hermeneutical receptions by local religious leaders based on lectures or recitations of their interpretations, available receptions based on the tingkeban tradition, and aesthetic receptions based on the writing of the Quran as wall hangings (Abshor, 2019, pp. 47-51). A. Farih Dzakiy wrote a similar study in a scientific article entitled \"Hadis dan Resepsi Estetis Pesantren (Studi Kitab Fada'il Ramadan Karya Taufiqul Hakim).\" According to him, the book by KH. Taufiqul Hakim contains three categories of reception; cultural, hermeneutical, and aesthetic (Dzakiy, 2016). Although the focus of this study is on aesthetic receptions, explanations only come down to nadzam being chosen as an expression of acceptance of Kiai Taufiqul Hakim without explaining matters related to the aesthetic experience of a receptor so it can be said that the reception that occurs is aesthetic (Parris, 1999, p. 173). Thus the articles in the study of the Quran that use the reception approach so far tend to be subjective and do not have clarity of flow in analysis.\nIn response, the reception approach to studying the Quran needs to be emphasized in methodological measures. This paper was written to elaborate on how hermeneutics can be used to understand the Quran, especially regarding Hans Robert Jauss's hermeneutics of reception offer, which emphasizes the reader's acceptance as retrieval of meaning. Through this research, it is hoped that studies on various kinds of Quranic meanings can be revealed so that the study of the Quran continues to experience significance and get diverse points of view regarding a verse or phenomenon related to it.\nThe Quran defines itself as a text (Setiawan, 2005, p. 51), So the consequence is that it can be studied with approaches as other texts are studied, one of which is through the lens of hermeneutics. From the constructed premise, the author tries to offer the hermeneutics of the reception as one of the avenues to know how the Quran was received by its readers. It is known that readers of the Quran have such diverse backgrounds that it is very likely to give rise to a significant acceptance of one another. The hermeneutics of reception explores the realm of the Horizon of expectation, the level of reading, and the aesthetic experience of a reader in receiving a text (Parris, 1999, p. 156). Simply put, a hermeneutics of reception works in the abstract realm before an understanding occurs and has a focus of study on the reader.\nMeanwhile, reception hermeneutics is a typology of acceptance in the form of interpretation or exegesis of a reading (Rafiq, 2014, p. 147). The difference in applying the terms hermeneutics of reception and reception hermeneutics in the Quranic research tradition is clear. As the name implies, the hermeneutics of reception indeed focuses on the reader, which in the study of the Quran will be found in the study of figures, works of interpretation, and or sharpening analysis in the study of the living Quran. The expectation built through this approach is to reveal the details of the reader's understanding of a text to allow for significance between each other. Abdullah Saeed, quoting from Ismail al-Faruqi, argues that so far, Muslims have stagnated in deconstructing a way of thinking related to the interpretation of ethical-legal verses. So in 1960, Ismail al-Faruqi initiated the \"Islamization of Science\" movement to ignite the spirit of interpreting the Quran with a more rational approach (Saeed, 2017, p. 30). One approach that is felt to be able to answer contemporary problems is hermeneutics (Wibowo, 2017, p. 4). According to Sahiron Syamsuddin, the tendency of scholars to use hermeneutics as a method of interpretation is at least divided into three groups; groups that reject as a whole, groups that accept, and groups that argue that some hermeneutic approaches can be applied in Quranic studies (Syamsuddin, 2017, p. 1).\nThe idea of hermeneutics for the study of the Quran is corroborated by contemporary Muslim thinkers who support, disseminate, and apply hermeneutics in understanding the Quran, including Fazlurrahman with his double movement, Mohammad Shahrour with his limit theory, to Amina Wadud who carries the issue of gender equality. Meanwhile, in the context of Indonesia, attention to hermeneutics in Islamic studies is shown by some of the Islamic Religious State High Groups through the publication and writing of scientific articles, theses, theses, and dissertations that use the hermeneutic method in understanding the Quran (Wibowo, 2017, p. 4).\nOn the contrary, many groups are opposed to hermeneutics, such as Imarah in his book entitled Hadza Huwa al-Islam: Qira'at al-Nashsh al-Dini bayna al-Ta'wil al-Gharbi wa al-Ta'wil al-Islami, for him to use hermeneutics as a method of interpreting the Quran is tantamount to turning off the role of God as the author of the Quran. Hermeneutics tends to focus on the reader's interpretation and open up multiple meanings. Hermeneutics is not suitable for religious texts because hermeneutics generalizes the entire text. So when faced with the Quran, it should not be used because there is no distinction between the muhkam verses that are unnecessary takwil and the mutasyabih verses that can receive takwil (Syamsuddin, 2017, p. 4). The criticism of the despicable ta'wil was also delivered by Manna' Al-Qaththan for the cleric muta'akhkhirin, who was excessively in ta'wil (Al-Qaththan, n.d., p. 210).\nAs for scholars who accept hermeneutics but within certain limits an example is Quraish Shihab; in his book, The Rules of Interpretation, he wrote down his views on hermeneutics. He did not close tightly to the probability of hermeneutics in understanding the Quran, but he gave strict signposts in its use. One must first understand the rules of interpretation carefully, fully believe that the Quran is the Haq word of Allah and avoid spirituality so that there are no mistakes in it from any aspect. In line with Islamic scholars long before Western Hermeneutics emerged, it has been careful to apply the ideas and mindsets brought by philosophers because of differences in scientific, socio-historical, and tendencies so that they also influence conclusions and methodologies. However, not all hermeneutic ideas are harmful. Its existence must provide benefits to broaden horizons and enrich interpretation (Shihab, 2015, pp. 426-427).\nThe use of reception as a paradigm in Islamic studies is well-known. A. Rafiq's dissertation became one of the primary references for students in the application of reception in the study of interpretation or hadith in Indonesia in particular. The novelty displayed by A. Rafiq is the reception typology of non-Arab receptors in receiving the Quran. The three typologies mentioned in the previous section are contained in his work entitled The Reception of the Qur'an in Indonesia: A Case Study of the Place of the Qur'an in a │ 164 Non-Arabic Speaking Community. Another study that implicitly elaborates on a person's reception of verses from the Quran was also written by Bruce Lawrence. This paper talks about the reception or reception of the Quran of a king named Shah Jahan, who mourned the departure of his consort, Mumtaz Mahal, in the form of calligraphic carvings that filled the building of his wife's tomb. Certain verses in the Quran in this context have their impressions of the king (Lawrence, 2006, p. 147). Unfortunately, Lawrence tends to write down stories about verses written in calligraphy without directly displaying the analysis through reception theory.\nSeeing the intensity of this reception paradigm contributes to the research traffic of Islamic studies, it is essential to elaborate the hermeneutics of the reception approach so that it becomes more stable and can be an approach that contributes to Islamic studies. In addition, considering that the methodological principle of classical interpretation is felt to give less space to the function of human agency in interpreting the meaning of the text or the performative function of the audience, the author sees a probability of offering the hermeneutics of Hans Robert Jauss' reception as an approach to the study of the Quran, namely from the aspect of its receptors.\nThe hermeneutics of reception was chosen based on the author's concern about using the term reception, which is very limited to Islamic Studies research, especially towards the Quran. The diversity of understanding of the Quranic audience should help broaden the Horizon of understanding of the Quran, providing a new view and practical significance for life, and at least having another perspective on understanding a verse. However, this approach has not been widely used to the fullest, so the analysis that comes to the surface is still limited to descriptive.\nReflection on the previous, this paper is written with a study of literature. It is qualitative(Qualitative research method is a research method based on the philosophy of positivism, used to examine the condition of natural objects Sugiono, 2015, p. 9). According to Kirk & Miller, qualitative research is a specific tradition in the social sciences that fundamentally relies on observing the human being in his faith and relates to those people in his language and his distillation (Rahmat, 2019, pp. 1-8). This research uses the library research method by digging into data from books, books, or scientific articles to get broad conclusions.\nThe process of collecting data was carried out by the author by first grouping readings on hermeneutics and hermeneutics reception of Hans Robert Jauss. Furthermore, the author reads reading sources consisting of primary sources, namely Literary History as a Challenge to Literary Theory by Hans Robert Jauss and Elizabeth Benzinger, Toward an Aesthetic of Reception translated from German into English by Timothy Bahti, and secondary sources containing primary data, writings from David Parris containing the hermeneutic concept of Jauss's reception in systematic, entitled Reception Theory: Philosophical Hermeneutics, Literary Theory, and Biblical Interpretation. Other readings containing hermeneutics in general, the hermeneutics of reception, reception theory, and the use of reception approaches in Islamic studies are supporting sources in writing this paper.\n\nAspects in Hermeneutics of Reception\nHermeneutics comes from the Greek vocabulary hermeneutic, which means translating or acting as an interpreter (Hardiman, 2015, p. 11). The term hermeneutics is not new in the modern world, but it has been known since ancient Greece. The Hermes, in the Greek tradition, is an ancient mythological figure who had a role in conveying messages, so it is often associated with the historicity of the meaning of the word hermeneutics itself. Before Hermes conveyed the messages of the gods to humans, he first understood and then interpreted these messages to make them easier for humans to understand. Through these activities, the delivery of meaning no longer seems simple because at least two problems arise; First, the party delivering the message must first understand what the message creator wants; second, the adjustment of diction and articulation with the recipient of the message so that the meaning can be absorbed properly. This gap between the messenger and the recipient of this message is bridged by hermeneutics (Hardiman, 2015, pp. 10-11).\nAccording to Richard E. Palmer, there are six descriptions of the definition of hermeneutics that have evolved. First, hermeneutics is a theory of bible interpretation. This understanding still survives from the post-Protestant Reformation period to the present day. Second, the definition developed as a result of rationalism in Europe, namely hermeneutics as a philosophical methodology used to interpret a wide variety of texts. Third, hermeneutics is a science of linguistic understanding (language) developed by Schleiermacher in the \"art of understanding.\" Fourth, the definition promoted by Dilthey is that hermeneutics is the methodological basis of the social sciences of humanity. Fifth, hermeneutics as phenomenology and existential understanding. And sixth, hermeneutics as a system of interpretation. That complete sense was narrowed down to two systems; restoration of meaning and iconoclasm (Hardiman, 2015, pp. 13-14).\nBased on the period, hermeneutics is divided into three eras; (1) classical, which emphasizes hermeneutics as a method of interpreting texts, (2) romance, which emphasizes and lays down methods to avoid misconceptions; and (3) philosophical, which discusses the nature of understanding (Shihab, 2015, p. 406). As for hermeneutics, Gadamer belongs to the second period. Gadamer's philosophical hermeneutics arose out of his concern over the status of hermeneutics as a method and an opinion that the essence of understanding could be obtained through methods formulated by earlier hermeneutic figures such as Schleiermacher and Dilthey. They express an opinion about an understanding close to the author's intent, and the final for the text is an understanding closer to the truth. Gadamer agrees more with Heidegger's concept that works have their form and are independent of the author. Therefore, knowing the author's purpose is no longer critical (Shihab, 2015, p. 418). Gadamer influenced much of his successor's perspective on hermeneutics in the following period. As a disciple of Gadamer, Hans Robert Jauss inherited Gadamer's philosophical hermeneutics concept despite differences in the horizon concept section, reading level, and esthetical aspects of the reader (Parris, 1999, p. 156).\nThe three primary variables in hermeneutics are author, reader, and text. (The hermeneutics of reception emphasizes meaning on the reader (Readers, listeners, and audiences have a significant role in literary history. See more... H. R. Jauss & Benzinger, 1970, p. 7). In the literary tradition, the reception means how the reader gives meaning to the literary work (text) he reads to respond to his reading (Junus, 1985, p. 1). According to the Merriam-Webster dictionary, reception comes from English reception, which means receiving behavior. So the hermeneutics of reception means how one receives and gives an impression of the text he reads (Junus, 1985, p. 87). Several names appeared when reception terminology was discussed, including Wolfgang Iser and Hans Robert Jauss. As for this paper, the author will discuss it from Jauss' side.\n\n│ 166\nHans Robert Jauss, born in Germany in 1921 (Ratna, 2017, p. 108), claimed to be the foundation of the reception through his work entitled \"Literary History as a Challenge to Literary History,\" published in Konstanz, Germany, in 1967. Initially, Jauss' thoughts on the reader's authority in the text's meaning were reasonably slow in development. This relates to the tradition of structuralism, which assumes that the author has authority over its meaning while the text has no meaning. So when a theory that goes against the current arises, it is certainly not easy to be accepted. In addition, Jauss' writings use German, whose distribution is not as aggressive as English, further narrowing the space for his thinking (Junus, 1985, p. vii).\nThe history of literature as a challenge to literary theory stems from the never-ending dispute between the Marxist and formalist traditions. Jauss' efforts to mediate between the two are carried out by bridging literature and history between aesthetic and historical approaches, starting from both points of departure of these two schools (H. R. translated by S. M. Jauss, 1984, p. 7). According to Jauss, in building literary history, the reader's role in it is necessary as a giver of meaning. Literary history is not just a sequence of events but a series of receptions of the reader in understanding an event. The Horizon of hope initially turns passive acceptance into active, due to the presence of the reader's aesthetic experience that influences it (Ratna, 2017, pp. 108-110). A historian is not enough to passively expose the facts of the past, more than that, he must be able to take part in uncovering the facts now thoughtful observer (Sujarwa, 2012, p. 61).\n\nThe Horizon of Expectation; Erwartungshorizont\nA literary text will not live without the active participation of its audience. This is obtained from the interaction between texts that reach the Horizon of hope that continues to widen continuously in response to understanding and providing perception, from passive to active, and from normative aesthetic norms to a new product (new perspective) that transcends previous understandings (H. R. Jauss & Benzinger, 1970, p. 8). The acceptance of a text always supposes the context of aesthetic experience: raising questions regarding the subjectivity of interpretation and the interests of the reader or the different levels of readers can be questioned when one has first clarified the transsubjective Horizon by which understanding conditions the influence of the text (Jauss, Hans Robert, translated by Bahti, 1983, p. 23). The conceptual Horizon proposed by Jausswas heavily influenced by Gadamer (Jauss adopted the term horizon from Gadamer) (Parris, 1999, p. 156).\nA significant difference between the two is the form of interaction between his horizons, Gadamer with horizon fusion and Jauss with horizon meditation (The aesthetic perspective of the reception mediates between the normative reception and the new reception. If literary history is judged as an interaction between text and the reader, the difference between the two will continue to be sought for its midpoint (mediated). Thus, a common thread will be found between old and new understandings previously hindered by historicism. See more... H. R. Jauss & Benzinger, 1970, p. 8). Horizon is a range of vision that includes everything that can be seen from a certain point of view. The Horizon may narrow or widen (Hardiman, 2015, p. 180). The Horizon of hope in the context of a hermeneutics of reception means the Horizon of the reader, the impression or image that arises from and the effect of previous readings (Ratna, 2017, p. 109). In this section, researchers must decipher how far a receptor's Horizon of hope is, along with \"filling\" that Horizon. So that slowly, something will be known behind the reader's image of the text. \"Horizon 1\" depicts the Horizon of hope, everything that affects the outcome of the reading, whether the text of an earlier text, environmental conditions, or can be discussed with literary history. Moreover, literary history is not just a chain of events and can even form a new aesthetic image and shape the initially passive reading into active (Ratna, 2017, p. 110).\n\nThe 3 Levels of Reading\n• First Reading (Understanding) The three-step interpretation of Jauss' perspective is based that the hermeneutic process must be understood as a single whole of three levels; understanding (intelligence), interpretation (interpreter), and applications (appliance) (Jauss, Hans Robert, translated by Bahti, 1983, p. 139). A reader will be at this level at his first reading. A reader will dialogue with himself and interact with the text. Previously, it could be that a reader brings pre-understanding to compare after interacting with the text he is reading. Jauss agreed with Gadamer about his \"first reading,\" which is the first impression he gets when reading. The first reading can also be interpreted as a savings of understanding used as a provision to build understanding in the next reading. (Parris, 1999, pp. 166-167) • Second Reading (Interpretation) The result of the first reading becomes a critical capital to go to the second level, interpretation (Parris, 1999, p. 168). When someone has understood something, the reader will be able to turn the message into diction and language that is easier to accept. Like the task of the God Hermes in Greek mythology, who was in charge of transforming the articulation of the divine message, he carried into a language that humans understood (Hardiman, 2015, p. 11). The change in the Horizon of the first level reading to a higher level can be assumed as a partial understanding into a (likely) more thorough understanding than before, resulting from periodic readings of the text (Jauss, Hans Robert, translated by Bahti, 1983, p. 145).\n\n│ 168\n• Third Reading (Application) After going through the previous two levels, the text will seem to speak, \"what did the text say?\". This meaning is formed from the history of the text. Next, the text will interact with \"what did the text tell me?\". Movement from one question to the last indicates an increase in the reading level (Jauss, Hans Robert, translated by Bahti, 1983, p. 113;Parris, 1999, p. 169). The notion of application level does not mean practical action. However, it can serve the equally valid purpose of leveraging literary relationships with the past to measure and expand one's experience concerning the experiences of others.\n\"...then application here certainly cannot dissolve into practical action, but instead can satisfy the no less legitimate interest of using literary communication with the past to measure and broaden the horizon of one's experience vis-a-vis the experience of the other.\" (Jauss, Hans Robert, translated by Bahti, 1983, p. 147) Save the author; these three stages go through the reader gradually and erratically. A person can be at the first level with repeated readings in the truest sense. One's reading level depends on the Horizon of expectations in the reader.\n\nHermeneutic Validity to Aesthetic Experience\nEverything related to feelings, whether sad, happy, worried, or hateful, is part of the aesthetic experience. According to Gadamer, aesthetic experience is purity in seeing, hearing, and feeling a phenomenon so that it affects giving meaning (Parris, 1999, p. 173). The result of a person's reading is, of course, also influenced by the reader's aesthetic experience. The state of the reader's soul will radiate through the understanding he awakens. So it is necessary to analyze aspects of the reader's aesthetic experience in the realm of a hermeneutics of reception. Concrete steps are needed to know the aesthetic experience of a reader. For example, in the study of mufassir figures, an interview is needed with the author of the interpretation book so that the analysis follows what is experienced by the reader, not just wishful thinking. The aesthetic experience of a person with another is very likely to be different. Poiesis involves the reader's active participation in constructing an aesthetic object, or as Heidegger puts it, the world projected by the text. At the same time, aesthetics is a feeling of pleasure that comes from looking at and recognizing. This experience is only gained when we read a text. The third function is catharsis, displacing the reader's beliefs and freeing his mind to consider new points of view (Parris, 1999, p. 174).\n\nHermeneutics of Reception in Quranic Studies\nThe Qur'an is a holy book of Muslims that contains life instructions for Muslims and humanity. The Qur'an contains Divine revelations handed down to the Prophet Muhammad SAW. The Qur'an becomes a communication between God and Man, although the two have different dimensions. It does not mean that because the two sides have different dimensions causing their inability to be studied, it makes studies in Islamic science always relevant to be studied (Setiawan, 2005, p. 51). A communication code is needed in the communication process, one of which is language. God, the All-Powerful, who is nonsensory, transforms His godly language into a human sensory language through angelic intermediaries so that the Qur'an becomes a perfect medium for both of them to communicate (Setiawan, 2005, p. 55).\nTalking about the concept of communication between God and man and the Qur'an as a code of communication, the vocabulary of the Qur'an itself becomes essential to discuss. The cause of it will be known what he is. Through this vocabulary, the Qur'an defines itself as a text, and its textuality character is depicted in the vocabulary in question (Qur'an word meaning to collect, read, voice, read and understand, or read. Reading activities are impossible without something readable. Therefore, the Qur'an supports its status as a reading text. See... Setiawan, 2005, pp. 57-59).\nConsequently, when the Qur'an is considered a text, it can be applied literarily. Through the lens of a hermeneutics of reception, the reader has the right to give a reaction or response according to what he has received from his reading of the Qur'an (Setiawan, 2005, p. 69). Studying the hermeneutics of reception will enrich insights into the reader's view of the Quran, which is undoubtedly unique. In contrast to the hermeneutical reception (Hermeneutical reception is a reaction to the reading of the form of interpretation. See more... Rafiq, 2014, p. 147), the hermeneutics of reception becomes an excellent capital to analyze the extent of one's understanding and what is the result of understanding obtained from reading.\nThrough the study of the hermeneutics of reception, it will be known that the Quran is a source of inspiration and influences the aesthetic aspects of a reader. The application of hermeneutics of reception in the study of the Quran is still minimal, but the reception theory has stabilized, and many studies use this theory. Thus, the hermeneutics of reception is a path/way that can be followed.\nA final project using hermeneutics of reception by Hans Robert Jauss has been written as one of the real achievements of applying these hermeneutics in studying the Quran. Research with title The Hermeneutics of Reception Toward Social Media Ethics in KH. Taufiqul Hakim's Interpretation on Chapter Al-Hujurat Verses 6 and 10-23 (A Study of Tafseer Al-Mubarok) trying to uncover how a receptor, Kiai Taufiqul Hakim perceives QS. Al-Hujurat verses 6 and 10-13, along with things that affect the results of the reception, including the surrounding environment, the circumstances and situation when writing the interpretation, as well as the encouragement of the spiritual teacher Kiai Taufiqul Hakim, namely Gus Mustofa Bisri (Asna, 2021). Although through this research, researchers claim that there is still much to be explored from the application of reception hermeneutics in the study of the\n\n│ 170\nQuran, the proximity of hermeneutic of reception is expected to be used in studying the traditions of the Quran that has been developing, especially in Indonesia which has a heterogeneous society and culture. The study of the hermeneutics of reception has an excellent potential to broaden the Horizon of the richness of interpretation from the reader's point of view.\n\nConclusion\nHermeneutics has excellent potential in developing interpretive studies, especially hermeneutics of reception, which gives the reader the authority of its meaning. In the process, studies with a hermeneutics of reception approach do not justify the results of a person's reading but only seek to construct how the understanding occurs. In addition, this approach can be used to study figures, whether they have worked or not. The three aspects that become an analysis of the reader's understanding are; the Horizon of expectations, the level of reading, and the validity of the aesthetic experience.\nThe downside of this approach is that the variety of meanings sometimes obscures the text's original intent and may stay away from the truth. When the text has been thrown to the audience, it has its meaning, and the reader brings his own Horizon. So it requires good preparation in its application. This paper is imperfect, so it is hoped that there will be constructive suggestions and criticisms so that this approach to the hermeneutics of reception is more stable and becomes a method that can contribute to the study of Quranic interpretation."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-28"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 95034,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254853718"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Positive-incentive Noise\n\nNoise is conventionally viewed as a severe problem in diverse fields, e.g., engineering, learning systems. However, this paper aims to investigate whether the conventional proposition always holds. It begins with the definition of task entropy, which extends from the information entropy and measures the complexity of the task. After introducing the task entropy, the noise can be classified into two kinds, Positive-incentive noise (Pi-noise or $\\pi$-noise) and pure noise, according to whether the noise can reduce the complexity of the task. Interestingly, as shown theoretically and empirically, even the simple random noise can be the $\\pi$-noise that simplifies the task. $\\pi$-noise offers new explanations for some models and provides a new principle for some fields, such as multi-task learning, adversarial training, etc. Moreover, it reminds us to rethink the investigation of noises.\n\n\nI. INTRODUCTION\nNoise, which is conventionally regarded as a hurdle in pattern recognition and machine learning, is ubiquitous due to a variety of reasons, e.g., human factors, instrumental error, and natural disturbances. Noise can be generated from different phases: (1) During the low-level data acquisition, noises could come from instrumental errors; (2) at the data level, noises may be caused by the differences of data storage and representation; (3) at the feature level, noises are usually generated by the imprecise modelings; (4) there may exist instance-level noises as well, i.e., irrelevant data points. There is a potential assumption in existing works: the noise always causes a negative impact to the current task. Therefore, how to design a model insensitive to noise is an important topic in various fields of pattern recognition. For example, in computer vision, plenty of filters are designed to alleviate the impact of noise, e.g., Gaussian filter, uniform filter. In the past decade with the rapid growth of machine learning, the robust model is an extremely studied topic, e.g., noise-insensitive clustering [1], robust feature selection [2], multi-view learning [3], noisy matrix completion [4], adversarial training [5].\nNevertheless, does the above assumption always holds? Or formally, the crucial question that this paper intends to answer is: is noise always harmful?\nThe question originates from some inspiring instances of noise. The first one is the traffic noises from the cars for Xuelong Li is with the School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University, Xi'an 710072, Shaanxi, P. R. China.\nThis work is supported by The National Natural Science Foundation of China (No. 61871470).\n©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. E-mail: li@nwpu.edu.cn acoustics tasks. In most scenes, the car noises should be regarded as useless information or disturbance due to the unsatisfied reception when collecting data. However, if the acoustics task is relevant to time, the car noise may offer extra information about time and enhance performance. Generally speaking, the intensities of car noises in the morning rush hour and midnight are clearly different, which can provide coarse information about time. Another inspiring instance is the gum example. For a clean wall, either gum or nail is a kind of noise for the wall. It means that the gum stuck on the wall and embedded nail are both unexpected. However, a piece of gum may help to remove the embedded nail with the help of its adhesive ability. Although there will be a hole, the gum, a kind of noise, is used to rectify another noise. Or similarly, the gum may help to remove a broken key stuck in a lock, while both the gum and the broken key are noise for the lock. Inspired by the above instances, a question comes: do noises really mislead the target task in all cases? A topic related to the question is stochastic resonance [6], which employs random noises to enhance the detection of weak signals. However, SR fails to completely answer the above question since it only focuses on the scenes about weak signal detection.\nThe crucial factor causing doubt about noises is the loose definition of noise. To rigorously answer the question, the complexity (or equivalently difficulty, uncertainty) of the given task plays an important role. With the definition of task entropy, the conventionally defined noise can be classified into 2 categories. One is the noise decreasing the complexity of the task, namely Positive-incentive noise (Pi-noise or π-noise). Another is the useless noise for the task, namely pure noise. With the proper definition of task entropy, stochastic resonance is a specific case of π-noise. Several subfields of pattern recognition (e.g., multi-task learning, adversarial training) are also connected with π-noise. It should be emphasized that superfluous π-noises also result in negative impact, which is the reason why it is still named \"noise\". For instance, car noises will also disturb time-related acoustics recognition if the noises are too strong.\nIn the following part, Section II introduces the mathematical notations appearing in this paper. Section III elaborates on the theoretical motivation and definition of π-noise. In Section IV, two applicable topics of π-noise are discussed and sufficient experiments also verify the existence and effectiveness of π-noise. The experimental results provide a counterintuitive conclusion: Even a simple random noise may simplify the task with the proper setting.\n\nII. PRELIMINARY\nIn this paper, matrices and vectors are denoted by uppercase and lowercase letters in boldface, respectively.  Fig. 1: Comparison between sophisticated image and simple image. The left one is an aerial image and the right one is COIL20 [9]. For single-label classification, the left one can be labeled as \"plane\", \"building\", or \"tree\". The uncertainty increases with growth of label space Y due to the abundant information in the image. entropy [7] of a random variable x is denoted by And the mutual information of two discrete random variables is computed by [8] MI(x, y) = x,y where the conditional entropy is defined as The above definition can be easily extended to continuous variables by replacing the sum operator with the integral symbol. δ(x) is the Dirac delta function. sgn(x) returns x/|x| if x = 0 and 0 otherwise. The noise is denoted by if without any specific statement.\n\nIII. POSITIVE-INCENTIVE NOISE\nIn this section, the motivation and the formal definition of π-noise are introduced first. Then, the relations between some existing fields and π-noise are elaborated.\n\nA. Motivation from Information Theory\nThe behind philosophy of π-noise is that the same noise may play different roles in diverse tasks. The inspiring example is also about the car noise. For most acoustics recognition tasks, car noise is the unexpected additive signal caused by unsatisfied reception. For time-relevant tasks, however, the car noise provides the extra beneficial information.\nAccordingly, it implies that the rigorous discussion of noise should be based on tasks. Before discussing the relationship between task and noise, how to mathematically measure a task T is the first crucial question. With the help of information theory, the entropy of T can be defined to indicate the complexity of T . Formally speaking, the smaller H(T ) means the easier task. Clearly, how to compute H(T ) is the key problem. In the following part, the rationality of H(T ) and how to compute it are shown with the help of a general classification task.\nIf the entropy of task T can be formulated, it is natural to define the mutual information of task T and noise , In the context of the conventional discussion of noise, the strict definition of unexpected and harmful noise should satisfy MI(T , ) = 0. However, as theoretically and empirically shown in this paper, even the simple random noise (e.g., Gaussian noise) may lead to positive mutual information. That is an interesting phenomenon since it implies that finding completely unrelated random noise may be also difficult. Formally, the definition of π-noise is given as follows: Definition 1. Formally, define the noise satisfying the following condition, as the π-noise. The above inequality is also equivalent to which indicates that simplifies the original task. On the contrary, the noise satisfying MI(T , ) = 0 is named the negative noise or pure noise.\nFurthermore, more strict π-noise can be defined by introducing a threshold: Similarly, the noise satisfying MI(T , ) ≤ α is named as αstrong negative/pure noise.\nIt should be emphasized that π-noise can be viewed as a kind of information gain brought by . One may argue why not to define π-noise via the information gain, which is widely used in machine learning. The mutual information is preferable due to that it directly shows the essence of π-noise. In other words, the random noise component contains useful information for T . Various measurements of the information gain could be used to estimate MI(T , ) and help to distinguish the π-noise.\nRemark 1 (Moderate π-Noise Assumption). The existence of π-noise does not indicate that there exists a random variable so that T can be persistently enhanced with the increase of the π-noise. Even for the π-noise, the conventional consensus of noise does not change: superfluous π-noise will cause degeneration. In other words, = 0 holds in most cases. This is the reason why π-noise is still named \"noise\".\nAs shown in the following subsections, some existing relevant topics can be viewed as special cases of the π-noise framework.\n\nB. Explanation of Single-Label Classification\nFor a fundamental single-label classification problem, the dataset (X, Y ) can be regarded as samplings from D X ,Y [10] where D X ,Y is the underlying joint distribution of data points and labels from feasible space X and Y, i.e., (X, Y ) ∼ D X ,Y . Therefore, given a set of data points X, the label set can be regarded as sampling from Y ∼ D Y|X and the \"complexity\" (equivalently difficulty or uncertainty) of T on X is formulated as To better understand p(Y |X), Fig. 1 shows two image datasets for the classification tasks. The left one is an aerial image where the label space is subjected to Y = {plane, building, tree, . . .}. It may be therefore tagged as \"plane\", \"building\", or \"tree\". which is also the intention of label smoothing [11]. The uncertain label increases the complexity of T . On the contrary, classification on the pure images of objects without complicated background, e.g., COIL20 [9] shown in Fig. 1 In this case, the task is apparently the simplest since all semantic ambiguity does not exist. Furthermore, the expected entropy of task T (i.e., independent of some specific dataset X) can be defined as Note that the expected task entropy H(T ) is actually the conditional entropy H(T |X). To keep simplicity and generality, H(T ) is instead used. In the following part, both the specific task entropy and the expected entropy are denoted by H(T ) if unnecessary. Another interesting corollary is that the task entropy can measure the quantity of information under the context of the classification task to some extent.\n\nC. Explanation of Stochastic Resonance\nStochastic resonance (SR) [6], which is known as a kind of noise benefit, is firstly discussed to provide a specific instance. For signal y t = f (t) ∈ Θ, the assumption of SR is the weak stimuli and f (t) < θ holds in most cases where θ represents the minimum threshold that sensors can detect. The goal of signal detection is to detect the weak signal as much as possible. The unseen stimuli imply that any value in the feasible domain may be possible which leads to strong randomness. Formally speaking, define p(y t ) as where θ 0 = inf yt∈Θ y t , S f = {t|y t ≥ θ}, and f o (t) is the observed quantity. Accordingly, the task entropy can be formulated as Clearly, if S f = ∅, p(y t ) is a uniform distribution and H(T SR ) achieves the maximum of the entropy. Provided that t ∼ N (0, σ 2 ), the joint probability and conditional probability can be formulated as and where S f + = {t|y t + > θ}. Accordingly, the conditional entropy is formulated as Consider an extreme example when S f = ∅ and the following inequality, easily holds if σ is appropriate. On the contrary, when The analysis of SR shows that the random noise may be π-noise on some data but be pure noise in other cases. It may implies that there exist no noise being π-noise or pure noise on every dataset for task T .\n\nD. Explanation of Multi-Task Learning\nMulti-task learning [12] could be regarded as a special case of π-noise. If represents one or some tasks, i.e., = (T 1 , T 2 , . . . , T k ). Suppose that all tasks related to T are denoted by G and other irrelevant tasks are denoted by G. The low-rank multi-task models [12] intend to employ {T } ∪ G and eliminate G, since G is the π-noise and G is pure noise. In other words, H(T |G) < H(T ) is the reason why the multitask learning outperforms the original task.\nE. Relationship Between π-Noise and Adversarial Training π-noise also offers a new perspective for the adversarial training, which seems relevant to π-noise framework. The adversarial training [5] is usually formulated as where (·) represents some loss function, θ is the learning parameters of the model and C is a constant. The goal of adversarial training is to enhance the robustness of the model f θ via introducing the adversarial perturbation . In other words, the underlying assumption is: f θ achieves satisfying performance on X but obtains unexpected generalization performance. In the π-noise framework, is used to reduce the complexity of the task, instead of aiming at any specific models. More precisely, the purpose of introducing π-noise is to decrease the difficulty of training any models. Large H(T ) usually implies that a model probably learns imprecise semantic information, which may provide new perspectives to understand why some models are not stable on complicated datasets. For instance, the classification models may over-evaluate those points with uncertain labels.\n\nIV. APPLICATIONS OF π-NOISE\nAfter theoretically discussing the definition of π-noise (and pure noise), two possible applications of π-noise are provided in this section. Some experiments are also conducted to show the universal existence of π-noise.\n\nA. Enhanced π-Noise\nThe first application is to use π-noise to enhance the performance which is direct from the definition and corresponds to multi-task learning. Rigorously speaking, the enhancement of performance is based on decreasing the complexity of tasks via π-noise. This part of the experiment is also a direct answer to the question proposed in the title: Even for the simple random noise, the impact is not always negative.\n1) Datasets Setting: The experiments of enhanced π-noise are conducted on the image classification task. The real image dataset STL-10 [13] is chosen as the benchmark dataset. This dataset has 10 class samples. Each class has 500 training images and 800 testing images. Suppose that the original image is noiseless and three categories of noise (including multiplicative noise, Gaussian noise, and uniform noise) are  added to the data before training. For the dimension noise, five UCI benchmark are selected and the details are listed in Table  I. 50% of the sampled points from each class are employed as the training data and the rest are acted as the test data. Meanwhile, LeNet [14] is chosen as the baseline method to extract the deep feature and output the predicted classification. This network is trained by the stochastic gradient descent to minimize the cross-entropy loss. Besides, the batch size is 200 and the epoch is 50. The learning rate is 0.01. Furthermore, SVM [15], Lasso [16], and DLSR [17] are employed as the classifier to evaluate the performance. Among them, the regularization parameter of Lasso and DLSR is set to 0.01 and 1, respectively. The classification accuracy (ACC) metric is employed to evaluate the performance of the network.\n2) Details of Generated Noise: To be more persuasive, four categories of noises are applied to the original training set. In particular, the first three kinds of noise (multiplicative noise, Gaussian noise, and uniform noise) are generated according to the different proportion p = N N , where N is the number of noisy training samples and N is the total number of training samples. The detailed settings of the noises are listed as follows: • Multiplicative Noise: This noise generally is generated by the change of channel and can be represented as u = u × where u is the original signal. Meanwhile, the salt-and-pepper noise is common multiplicative noise for images. Specifically, due to the signal disturbed by the sudden strong interference or bit transmission error, the image generates unnatural changes such as the black pixels in bright areas or white pixels in dark areas. As shown in Fig. 2 The same operation is adopted to add uniform noise. As shown in Fig. 2(i)-2(l), the noisy images are listed according to different a and b. • Dimension Noise: This type of noise is obtained by random linear transformation and nonlinear activation of the original data, which is cascaded behind the original data subsequently. It can be written as u = u sgn(P u), where is the concat operation and P is a linear transformation. In the experiment, the multiplicative noise with degree=3, additive noise with µ = 0.5, σ = 0.5, and uniform noise with a = 0, b = 1 are chosen as the enhanced noise. For dimension noise, p is generated from a random uniform distribution and sgn(·) is a sign function.\n3) Experiment Results: To show the effect of different noises on the model, the experiments with the different noisy proportion, p, from {0.0, 0.05, . . . , 0.95} are conducted. The results are shown in Fig. 3. From the three figures, a counterintuitive conclusion is obtained: Data with a little simple random noise enhance the model, compared with the \"noiseless\" data. The curve is an inverted U-shape curve, which indicates that proper noise is beneficial. From the visualization in Fig. 2, it is easy to find that the proper random noise blurs the background and remains the necessary feature of the airplane, leading to a decreasing complexity. Besides, the enhancement of dimension noise is shown in Table II. Among them, m is the dimension of the added noise. The classification performance of original data is substantially improved by adding noise. The experimental results verify the existence of π-noise and support the guess about the amount of π-noises.\n\nB. Rectified π-Noise\nAnother application is to use the π-noise to neutralize the negative effect of the pure noise. Instead of detect and eliminate the pure noise in data points, another scheme is to add some π-noises to rectify the data distribution. It is particularly preferable for incremental learning systems. The core assumption of incremental systems is the expensive retraining. When a batch of data points with some noisy points come, the system suffers from the irreversible damage and the idea to add π-noise provides a cheap scheme. It corresponds to the gum-nail instance proposed in Section I. Before the details of experiments, it should be emphasized that the noise added in this subsection is actually noisy instances, rather than the additive or multiplicative noise acting on the original data instances.\n\n1) Datasets Setting:\nThe experiments are conducted on three tasks, including classification, clustering, dimensionality reduction. For classification and clustering, totally 3 datasets are utilized to investigate the performance of rectified π-noise, including a synthetic dataset and 2 UCI [18] datasets. For each dataset, each class has the same number of samples. The details of these datasets are reported in Table III 2) Classification (Support Vector Machine [15]): In the experiments, the classical support vector machine (SVM) is utilized as the classifier. Meanwhile, the One-versus-Rest strategy is equipped for SVM to handle the dataset with multiple classes. Firstly, SVM is run on the original benchmark dataset. Secondly, to show the degradation  Fig. 6. 5) Experimental Results: From three types of learning models, it is easy to conclude that there exists π-noise eliminate the negative effect of pure noise and rectifying the learning systems. It enlightens us that adding some proper random noisy points, instead of detecting the existing pure noise and removing it, may also help to improve the performance, which offers a new scheme for investigation of robust models.\n\nV. FUTURE WORKS\nThe discussions in this paper are elementary and instructive. More detailed analysis and investigations deserve further attention in the future. For instance, there are several attractive topics listed as follows: The rows represent the clustering results on Toy, Iris, and Wine datasets, respectively. The first column is the clustering result of the original dataset. The second column is the result with Gaussian random noise. The third column shows the proper number of rectified π-noises introduced to rectify the performance. The last column indicates that too many rectified π-noises can also degrade the clustering.\n• Although the π-noise widely exists in different fields, there is a crucial question: What property will the (αstrong) π-noise have? For example, it is promising to study which kind of random noise (e.g., uniform noise, Gaussian noise) is more likely to be π-noise in diverse scenes. It will be a core in the future investigations. • As highlighted in the preceding sections, although a little π-noise enhances the performance, too much πnoise would lead to degeneration as well. What is the relationship between the quantity of π-noise and inflection point of performance? In other words, what is the upper bound of the quantity of π-noise that maximizes MI(T , )? For multivariate Gaussian noise, the problem is equivalent to find a rigorous upper-bound of the covariance matrix regarding certain norm. • Although the existence of π-noise has been verified in some cases (e.g., classification, stochastic resonance), how to prove the existence of π-noise under general settings is still an attractive problem. • As shown in Section III-B, the computation of task entropy offers a new way to measure the complexity of datasets. Therefore, it is attractive to study whether the measurement induced by π-noise could provide a novel and practical framework of learning theory like the Rademacher complexity [10]. It may also show how to measure the ability to provide information per unit data size, namely information capacity. • Although the rectification ability of π-noise is sufficiently shown, how to find the rectified π-noise is an urgent problem. One way that may work is to find the desirable distribution via variational methods. • π-noise could be a new principle for designing models.\nFor instance, adversarial training can be more efficient if the optimization of aims at finding π-noise. A simple loss incorporating π-noise is Compared with the heuristic search of , the above principle may be more reliable and stable. In object detection, π-noise could provide a reliable principle to expand the bounding box to promote the detection by incorporating positive background information. • The clear difference between π-noise and pure noise also inspires us to rethink the data preprocessing. The existence of π-noise and its definition based on tasks imply that the denoising scheme should be designed for specific tasks since some noises may be beneficial. • π-noise will be the core of Vicinagearth Security [22].\nFor example, in the field of non-line-of-sight imaging and underwater imaging, the theory of π-noise may provide a new perspective to view received signals and help to  Fig. (a) visualizes the result of LDA on original data. Fig. (b) shows that the noises disturb the results and Fig. (c) indicates that the π-noise can rectify the projection direction.\ndesign a stronger imaging system. π-noise also plays an important role in UAV (unmanned aerial vehicle) applications. How to apply the theory of π-noise to Vicinagearth Security will be an emphasis of future works. In sum, it requires more systematic and rigorous investigations of π-noise in the future.\n\nVI. CONCLUSION\nThis paper rethinks whether the noise always results in a negative impact. The doubt comes from the loose definition of noise. Through modeling the mutual information of task T and noise , the traditional \"noise\" can be classified into two categories, π-noise and pure noise. In brief, π-noise is the random signal that can simplify the target task. By conducting some convincing experiments and showing that some existing topics (e.g., stochastic resonance, multi-task learning, adversarial training) can be explained as special cases, we empirically and theoretically conclude that π-noise is ubiquitous in diverse fields. There are still plenty of attractive problems that deserves more investigations, including but not limited to the general property of π-noise, the upper-bound of quantity of π-noise, the existence of π-noise under general settings, the new principle for designing models regarding πnoise, etc. Importantly, π-noise is also related to the study of information capacity. Both of them will be theoretical bases of Vicinagearth Security, which is the core of my future works."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 95319,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254563860"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning\n\nCompositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We introduce two key modifications to this model which encourage more disentangled representations and improve its compute and memory efficiency, allowing us to tackle compositional generalization in a more realistic setting. Specifically, instead of adaptively re-encoding source keys and values at each time step, we disentangle their representations and only re-encode keys periodically, at some interval. Our new architecture leads to better generalization performance across existing tasks and datasets, and a new machine translation benchmark which we create by detecting naturally occurring compositional patterns in relation to a training set. We show this methodology better emulates real-world requirements than artificial challenges.\n\n\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) and variants thereof have become ubiquitous in natural language processing. Despite widespread adoption, there is mounting evidence that Transformers as sequence transduction models struggle with compositional generalization (Kim and Linzen, 2020;Keysers et al., 2020;Li et al., 2021). It is basically the ability to produce and understand a potentially infinite number of novel linguistic expressions by systematically combining known atomic components (Chomsky, 2014;Montague, 1970). Attempts to overcome this limitation have explored various ways to explicitly inject compositional bias through data augmentation (Jia and Liang, 2016;Akyürek et al., 2021;Andreas, 2020; 1 Our code and dataset will be available at xxx.yyy.zzz.  or new training objectives (Conklin et al., 2021;Oren et al., 2020;. The majority of existing approaches have been designed with semantic parsing in mind, and as a result adopt domain-and task-specific grammars or rules which do not extend to other tasks (e.g., machine translation).\nIn this work we aim to improve generalization via general architectural modifications which are applicable to a wide range of tasks. Our starting point are Zheng and Lapata (2022) who unveil that one of the reasons hindering compositional generalization in Transformers relates to their representations being entangled. They introduce Dangle, a sequence-to-sequence model, which learns more Disentangled representations by adaptively re-encoding (at each time step) the source input. For each decoding step, Dangle learns specialized source encodings by conditioning on the newly decoded target which leads to better compositional generalization compared to vanilla Transformers where source encodings are shared throughout decoding. Although promising, their results are based on synthetic datasets, leaving open the question of whether Dangle is effective in real-world settings involving both complex natural language and compositional generalization.\nWe present two key modifications to Dangle which encourage learning more disentangled representations more efficiently. The need to perform re-encoding at each time step substantially affects Dangle's training time and memory footprint. It becomes prohibitively expensive on datasets with long target sequences, e.g., programs with 400+ tokens in datasets like SMCalFlow . To alleviate this problem, instead of adaptively re-encoding at each time step, we only reencode periodically, at some interval. Our decoder is no different from a vanilla Transformer decoder except that it just re-encodes once in a while in order to update its history information. Our second modification concerns disentangling the representations of source keys and values, based on which the encoder in Dangle (and also in Transformers) passes source information to the decoder. Instead of computing keys and values using shared source encodings, we disassociate their representations: we encode source values once and re-encode keys periodically.\nWe evaluate the proposed model on existing benchmarks Li et al., 2021) and a new dataset which we create to better emulate a real-world setting. We develop a new methodology for detecting examples representative of compositional generalization in naturally occurring text. Given a training and test set: (a) we discard examples from the test set that contain out-of-vocabulary (OOV) or rare words (in relation to training) to exclude novel atoms which are out of scope for compositional generalization; (b) we then measure how compositional a certain test example is with respect to the training corpus; we introduce a metric which allows us to identify a candidate pool of highly compositional examples; (c) using uncertainty estimation, we further select examples from the pool that are both compositional in terms of surface form and challenging in terms of generalization difficulty. Following these three steps, we create a machine translation benchmark using the IWSLT 2014 German-English dataset as our training corpus and the WMT 2014 German-English shared task as our test corpus.\nExperimental results demonstrate that our new architecture achieves better generalization performance across tasks and datasets and is adept at handling real-world challenges. Machine translation experiments on a diverse corpus of 1.3M WMT examples show it is particularly effective for long-tail compositional patterns.\n\nBackground: The Dangle Model\nWe first describe Dangle, the Disentangled Transformer model introduced in Zheng and Lapata (2022) focusing on their encoder-decoder architecture which they show delivers better performance on complex tasks like machine translation.\nLet X = [x 1 , x 2 , ..., x n ] denote a source sequence; let f Encoder and f Decoder denote a Transformer encoder and decoder, respectively. X is first encoded into a sequence of contextualized representations N : which are then used to decode target tokens [y 1 , y 2 , ..., y m ] one by one. At the t-th decoding step, the Transformer takes y t as input, reusing the source encodings N and target memory M t−1 which contains the history hidden states of all decoder layers corresponding to past tokens [y 1 , y 2 , ..., y t−1 ]: This step not only generates a new token y t+1 , but also updates the internal target memory M t by concatenating M t−1 with the newly calculated hidden states corresponding to y t . Dangle differs from vanilla Transformers in that it concatenates the source input with the previously decoded target to construct target-dependent input for adaptive decoding: The adaptive encoder consists of two components. C t is first fed to k 1 Transformer encoder layers to fuse the target information: whereH t is a sequence of contextualized representations [h t,1 ,h t,2 , ...,h t,n ,h t,n+1 , ...,h t,n+t ]. Then, the first n vectors corresponding to source tokens are extracted and fed to another k 2 Transformer encoder layers for further processing: Finally, the adaptive source encodings H t together with the target context [y 1 , y 2 , ..., y t ] are fed to a Transformer decoder to predict y t+1 : In a departure from vanilla Transformers, Dangle does not reuse the target memory from previous steps, but instead re-computes all target-side hidden states based on new source encodings H t . Similarly to Transformers, Dangle accesses source information at each decoding step via encoder-decoder attention layers where the same encodings H t are used to compute both keys K t and values V t : where key and value projections W K and W V are parameter matrices; and Q t , K t , V t , and O t are respectively query, key, value, and output matrices, at time step t.\n\nThe R-Dangle Model\nIn this section, we describe the proposed model, which we call R-Dangle as a shorthand for Realworld Disentangled Transformer.\n\nRe-encoding at Intervals\nThe need to perform re-encoding (and also redecoding) at each time step substantially increases Dangle's training cost and memory footprint, so that it becomes computationally infeasible for realworld language tasks with very long target sequences (e.g., in the region of hundreds of tokens). Adaptively re-encoding at every time step essentially means separating out relevant source concepts for each prediction. However, the Transformer is largely capable of encoding source phrases and decoding corresponding target phrases (or logical form fragments in semantic parsing), as evidenced by its remarkable success in many machine translation and semantic parsing benchmarks (Vaswani et al., 2017;Keysers et al., 2020;Zheng and Lapata, 2021). This entails that the entanglement problem (i.e., not being able to disassociate the representations of different concepts for a sequence of predictions) does not occur very frequently. We therefore relax the strict constraint of re-encoding at every step in favor of the more flexible strategy of re-encoding at intervals. Given source sequence X = [x 1 , x 2 , ..., x n ], we specify P = [t 1 , t 2 , ..., t l ](t i+1 − t i = o) in advance, i.e., a sequence of re-encoding points with interval o. Then, during decoding, when reaching a re-encoding point t(t = t i ), we update source encodings H t and target memory M t : where f Adaptive_Encoder denotes the adaptive encoder described in Section 2. For the next time step t(t i < t < t i+1 ), we fall back to the vanilla Transformer decoder using the source encodings H t i computed at time step t i : Note that we always set t 1 to 1 to perform adaptive encoding at the first time step.\n\nDisentangling Keys and Values\nDuring decoding, Dangle accesses source information via cross-attention (also known as encoderdecoder attention) layers where the same source encodings are used to compute both keys and values. The core design principle underlying Dangle is that learning specialized representations for different purposes will encourage the model to zero in on relevant concepts, thereby disentangling their representations. Based on the same philosophy, we assume that source keys and values encapsulate different aspects of source information, and that learning more specialized representations for them would further improve disentanglement, through the separation of the concepts involved.\nA straightforward way to implement this idea is using two separately parameterized encoders to calculate two groups of source encodings (i.e., corresponding to keys and values, respectively) during re-encoding. However, in our preliminary experiments, we observed this leads to serious overfitting and performance degradation. Instead, we propose to encode values once and only update keys during adaptive encoding. We compute source values via the standard Transformer encoder: and adaptively re-encode source keys at an interval: where f KV_Decoder denotes a slightly modified Transformer decoder where source keys and values in each cross-attention layer are calculated based on different source encodings: At time step t (where t i < t < t i+1 ), we perform vanilla Transformer decoding: Note that fully sharing values could potentially cause some entanglement, however, we we did not observe this in practice. We also experimented with a variant where keys are shared and values are repeatedly re-computed but empirically observed\n\nSelected Examples\nCompositional Degree Uncertainty but what can we do about this ? 2 / 8 = 0.25 please report all changes here . 5 / 6 = 0.83 0.054 you have disabled your javascript ! 5 / 6 = 0.83 0.274 Table 1: Candidate examples from the WMT corpus. Different n-grams previously seen in the IWSTL training corpus are highlighted in color. The first example is composed of two n-grams (but and what can we do about this? ) with a compositional degree 0.25, and is discarded in the second stage. The second example has a high compositional degree but receives a low uncertainty score, and is thus filtered in the third stage. The third example is high in terms of both compositional degree and uncertainty, and is included in the compositional test set.\nit obtains significantly worse generalization performance than the value-sharing architecture described above. This indicates that entanglement is more likely to occur when sharing keys.\n\nA Real-world Compositional Generalization Challenge\nModels of compositional generalization are as good as the benchmarks they are evaluated on. A few existing benchmarks are made of artificially synthesized examples using a grammar or rules to systematically control for different types of generalization (Lake and Baroni, 2018a; Kim and Linzen, 2020; Keysers et al., 2020;Li et al., 2021). Unfortunately, synthetic datasets lack the complexity of real natural language and may lead to simplistic modeling solutions that do not generalize to real world settings (Dankers et al., 2022). Other benchmarks focus on naturally occurring examples but have artificial train-test splits (Finegan-Dollak et al., 2018), based on heuristics (e.g., query patterns). Again, the types of compositional generalization attested therein may not reflect real-world occurrence. It is fair to assume that a SOTA model deployed in the wild, e.g., a Transformer-based translation system, will be constantly presented with new test examples. Many of them could be similar to seen training instances or compositionally different but in a way that does not pose serious generalization challenges. An ideal benchmark for evaluating compositional generalization should therefore consist of phenomena that are of practical interest while challenging for SOTA models. To this end, we create ReaCT, a new REAl-world dataset for Compositional generalization in machine Translation. Our key idea is to obtain a generalization test set by detecting compositional patterns in relation to an existing training set from a large and diverse pool of candidates. Specifically, we use the IWSLT 2014 German → English dataset as our training corpus and the WMT 2014 German → English shared task as our test corpus (see Section 5 for details) and detect from the pool of WMT instances those that exemplify compositional generalization with respect to IWSLT. This procedure identifies naturally occurring compositional patterns which we hope better represent practical generalization requirements than artificially constructed challenges.\nIn the following, we describe how we identify examples that demand compositional generalization. While we create our new benchmark with machine translation in mind, our methodology is general and applicable to other settings such as semantic parsing. For instance, we could take a relatively small set of annotated user queries as our training set and create a generalization challenge from a large pool of unlabeled user queries.\nFiltering Out-of-Vocabulary Atoms Compositional generalization involves generalizing to new compositions of known atoms. The WMT corpus includes many new semantic and syntactic atoms that are not attested in IWSLT. A large number of these are out-of-vocabulary (OOV) words which are by definition unknown and out of scope for compositional generalization. We thus discard WMT examples with words occurring less than 3 times in the IWSLT training set which gives us approximately a pool of 1.3M examples. For simplicity, we do not consider any other types of new atoms such as unseen word senses or syntactic patterns.\nMeasuring Compositionality How to define the notion of compositional generalization is a central question in creating a benchmark. Previous definitions have mostly centered around linguistic notions such as constituent or context-free grammars (Kim and Linzen, 2020;Keysers et al., 2020;Li et al., 2021). Since we do not wish to synthesize artificial examples but rather detect them in real-world utterances, relying on the notion of constituent might be problematic. Sentences in the wild are often noisy and ungrammatical and it is far from trivial to analyze their syntactic structure so as to reliably identify new compositions of known constituents. We overcome this problem by devising a metric based on n-gram matching which assesses how compositional a certain example is with respect to a training corpus.\nSpecifically, we first create a lookup dictionary of atomic units by extracting all n-grams that occur more than 3 times in the training corpus. Given a candidate sentence, we search the dictionary for the minimum number of n-grams that can be composed to form the sentence. For example, for sentence \"x 1 x 2 x 3 x 4 x 5 \" and dictio- . A sentence's compositional degree with respect to the training corpus is defined as the ratio of the minimum number of n-grams to its length (e.g., 2/5 = 0.4 for the above example). We select the top 60,000 non-overlapping examples with the highest compositional degree as our candidate pool. As we discuss in Section 6, compositional degree further allows us to examine at a finer level of granularity how model performance changes as test examples become increasingly compositional.\nEstimating Uncertainty Examples with the same compositional degree could pose more or less difficulty to neural sequence models (see last two utterances in Table 1). Ideally, we would like to identify instances that are compositional in terms of surface form and hard in terms of the underlying generalization (see third example in Table 1). We detect such examples using a metric based on uncertainty estimation and orthogonal to compositional degree. We quantify predictive uncertainty based on model ensembles, a method which has been successfully applied to detecting misclassifications and out-of-distribution examples (Lakshminarayanan et al., 2017;Malinin and Gales, 2021).\nWe follow the uncertainty estimation framework introduced in Malinin and Gales (2021) for sequence prediction tasks. Specifically, we train 10 Transformer models with different random initializations on IWSLT (our training corpus), and run inference over the candidate pool created in the previous stage; for each example in this pool, we measure the disagreement between ensemble models using reverse mutual information, a novel measure (Malinin, 2019;Malinin and Gales, 2021) which quantifies knowledge uncertainty, i.e., a model's uncertainty in its prediction due to lack  of understanding of the data rather than any intrinsic uncertainty associated with the task (e.g., a word could have multiple correct translations). We use the token-level approximation of knowledge uncertainty. We empirically find that the most uncertain examples are extremely noisy and barely legible (e.g., they include abbreviations, typos, and nonstandard spelling). We thus therefore throw away the top 2,000 uncertain examples and randomly sample 3,000 instances from the next 18,000 most uncertain examples in an attempt to create a generalization test set with diverse language patterns and different levels of uncertainty.\nAnalysis We analyze the compositional nature of ReaCT by comparing it to several popular benchmarks. Specifically, for all datasets, we count the number of novel test set n-grams that have not been seen in the training. We extract n-grams over words and parts of speech (POS); word-based n-grams represent more superficial lexical composition while n-grams based on POS tags reflect more of syntactic composition.\nAs shown in Table 2, despite being considerably smaller compared to other benchmarks (see # examples column), ReaCT presents substantially more diverse patterns in terms of lexical and syntactic composition. It displays a much bigger number of novel word n-grams, which is perhaps not surprising. Being a real-world dataset, it has a larger vocabulary and more linguistic variation. While our dataset creation process does not explicitly target novel syntactic patterns (approximated by POS n-grams), ReaCT still includes substantially more compared to other benchmarks. This suggests that it captures the complexity of real-world compositional generalization to a greater extent than what is achieved when examples are synthesized artificially. We show ReaCT examples with novel POS n-gram compositions in Appendix A (Table 6).\n\nExperimental Setup\nDatasets We evaluated R-Dangle on two machine translation datasets and one semantic parsing benchmark which we selected to maximally reflect natural language variations and real-world generalization challenges. These include: (a) ReaCT, the machine translation benchmark developed in this paper; we used the IWSLT 2014 De→En test set as the in-domain test set and create an out-ofdistribution test set from the WMT'14 De→En training corpus; (b) CoGnition (Li et al., 2021) is a semi-natural machine translation benchmark focusing on English-Chinese sentence pairs; source sentences were taken from the Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016(Mostafazadeh et al., , 2017 and target sentences were constructed by post-editing the output of a machine translation engine; (c) SMCalFlow-CS (Andreas et al., 2020) is a semantic parsing dataset for task-oriented dialogue, featuring real-world human-generated utterances about calendar management; following previous work Qiu et al., 2022), we report experiments on the compositional skills split, considering a few-shot learning scenario (with 6, 16, and 32 training examples). See Appendix B for more detail on datasets.\nModels On machine translation, our experiments evaluated two variants of R-Dangle depending on whether keys and values are shared (R-Dangle shr ) or separate (R-Dangle sep ). We implemented all machine translation models with fairseq (Ott et al., 2019). We compared R-Dangle against a vanilla Transformer (Vaswani et al., 2017) and the original Dangle model (Zheng and Lapata, 2022) which use the popular fairseq configuration transformer_iwslt_de_en. We also implemented bigger variants of these models using 12 encoder layers and 12 decoder layers which empirically led to better performance. R-Dangle shr and R-Dangle sep also use a 12-layer decoder. We tuned the number of layers of the adaptive components (k 1 = 2 and k 2 = 10) on the development set. For R-Dangle sep , we adopted a 10-layer value encoder and a 10-layer key encoder (k 1 = 2 and k 2 = 8), with the top 8 layers in the two encoders being shared. This configuration produced 12 differently parametrized transformer encoder layers, maintaining identical model size to comparison systems.\nPrevious work (Qiu et al., 2022) has shown the advantage of pre-trained models on the 8 R-Dangle shr 11.8 11.9 11.8 11.6 R-Dangle sep 12.3 12.2 11.9 11.7 Table 3: BLEU score for R-Dangle variants (with different re-encoding intervals) on CoGnition and Re-aCT compositional generalization test sets. Note that R-Dangle shr with interval 1 is Dangle.\nSMCalFlow-CS dataset. For our semantic parsing experiments, we therefore built R-Dangle on top of BART-large (Lewis et al., 2020). We only report results with R-Dangle shr as the R-Dangle sep architecture is not compatible with BART. We again set k 1 = 2 and k 2 = 10. We provide more detail on model configurations in Appendix C.\n\nResults\nDisentangling Keys and Values Improves Generalization Table 3 reports the BLEU score (Papineni et al., 2002) achieved by the two R-Dangle variants on ReaCT and CoGnition, across different re-encoding intervals. R-Dangle sep is consistently better than R-Dangle shr which confirms that representing keys and values separately is beneficial. We also observe that smaller intervals lead to better performance (we discuss this further later). Table 4 compares R-Dangle sep (with interval 1) against baseline models. In addition to BLUE, we report novel compound translation error rate, a metric introduced in Li et al. (2021) to quantify the extent to which novel compounds are mistranslated. We compute error rate over instances and an aggregate score over contexts. R-Dangle sep delivers compositional generalization gains over Dangle and vanilla Transformer models (both in terms of BLEU and compound translation error rate), even though their performance improves when adopting a larger 12-layer network. R-Dangle sep achieves a new state of the art on CoGnition (a gain of 0.9 BLEU points over Dangle and 1.5 BLEU points over the Transformer baseline). R-Dangle sep fares similarly on ReaCT; it is significantly superior to the Transformer model by 0.9 BLEU points, and Dangle by 0.5 BLEU points. Moreover, improvements on compositional generalisation are not at the expense of in-domain performance (R-Dangle obtains similar performance to the Transformer and Dangle on the IWSLT2014 in-domain test set).\n\nR-Dangle Can Handle Long-tail Compositional\nPatterns Bettter We next examine model performance on real-world examples with diverse language and different levels of composition. Specifically, we train R-Dangle sep (interval=1) and a Transformer on the IWSTL14 corpus and test on the pool of 1.3M WMT examples obtained after filtering OOV words. Figure 1a plots the difference in BLEU between the two models against compositional degree. This fine-grained evaluation reveals that they perform similarly on the majority of less compositional examples (BLUE difference is around zero), however, the performance gap becomes larger with more compositional examples (higher difference means higher BLEU for R-Dangle sep ). This indicates that R-Dangle is particularly effective for handling long-tail compositional patterns.\n\nR-Dangle Boosts the Performance of Pretrained Models\nThe \"pre-train and fine-tune\" paradigm (Peters et al., 2018;Devlin et Raffel et al., 2020;Lewis et al., 2020) has been widely adopted in NLP, and semantic parsing is no exception (Shin et al., 2021;Qiu et al., 2022). We further investigate R-Dangle's performance when combined with a pre-trained model on the SMCalFlow-CS dataset (across the three crossdomain settings). Table 5 shows that R-Dangle shr boosts the performance of BART-large, which suggests that generalization improvements brought by R-Dangle are complementary to generalization benefits afforded by large-scale pre-training (see Zheng and Lapata 2022 for a similar conclusion). The proposed model effectively marries pretraining with disentangled representation learning to achieve better generalization.\nIn Table 5, we also compare R-Dangle with other top-performing models on SMCalFlow-CS. These include: (a) a sequence-to-sequence model with a BERT encoder and an LSTM decoder using a copy mechanism (BERT2SEQ; ); (b) the coarse-to-fine model of Dong and Lapata (2018) which uses a BERT encoder and a structured decoder that factorizes the generation of a program into sketch and value predictions; (c) and combinations of these two models with span-supervised attention (+SS; . We also include a T5 model and variant thereof trained on additional data using a model called Compositional Structure Learner (CSL) to generate examples for data augmentation (T5+CSL; Qiu et al. 2022). R-Dangle with BART performs best among models that do not use data augmentation across compositional settings. Note that our proposal is orthogonal to CSL and could also benefit from data augmentation.\nLarger Re-encoding Intervals Reduce Training Cost The results in Table 3 indicate that reencoding correlates with R-Dangle's generalization ability, at least for machine translation. Both model variants experience a drop in BLEU points when increasing the re-encoding interval to 8. We hypothesize that this sensitivity to interval length is taskrelated; target sequences in machine translation are relatively short and representative of real language, whereas in SMCalFlow-CS, the average length of target sequences (in formal language) is 99.5 and the maximum length is 411. It is computationally infeasible to train R-Dangle with small intervals on this dataset, however, larger intervals still produce significant performance gains. Figure 1b shows how accuracy and training time vary with interval length on SMCalFlow-CS with the 16-C setting. Larger intervals substantially reduce training cost with an optimal speed-accuracy trade off in between 10 and 50. For instance, interval 40 yields a 4x speed-up compared to interval 10 while achieving 50.3% accuracy. Finding a tradeoff between generalization and efficiency is an open research problem which we leave to future work.\n\nRelated Work\nThe realization that neural sequence-to-sequence models struggle with compositional generalization has led to numerous research efforts aiming to precisely define this problem and explore possible solutions to it. A line of research focuses on benchmarks which capture different aspects of compositional generalization. Finegan-Dollak et al. (2018) repurpose existing semantic parsing benchmarks for compositional generalization by creating more challenging splits based on logical form patterns. In SCAN (Lake and Baroni, 2018b) compositional generalization is represented by unseen combina-tions of seen actions (e.g., JUMP LTURN). Keysers et al. (2020) define compositional generalization as generalizing to examples with maximum compound divergence (e.g., combinations of entities and relations) while guaranteeing similar atom distribution to the training set. Kim and Linzen (2020) design five linguistic types of compositional generalization such as generalizing phrase nesting to unseen depths. In ReaCT, our definition of compositional generalization is dependent on the data distribution of the candidate corpus, which determines what compositional patterns are of practical interest and how frequently they occur.\nAnother line of work focuses on modeling solutions, mostly ways to explicitly instil compositional bias into neural models. This can be achieved by adopting a more conventional grammar-based approach (Herzig and Berant, 2021) or incorporating a lexicon or lexicon-style alignments into sequence models (Akyurek and Andreas, 2021;Zheng and Lapata, 2021). Other work employs heuristics, grammars, and generative models to synthesize examples for data augmentation (Jia and Liang, 2016;Akyürek et al., 2021;Andreas, 2020;Qiu et al., 2022) or augments standard training objectives with new supervision signals like attention supervision or meta-learning (Oren et al., 2020;Conklin et al., 2021;. Our work builds on Dangle (Zheng and Lapata, 2022), a disentangled sequence-to-sequence model, which tries to tackle compositional generalization with architectural innovations. While Dangle is conceptually general, our proposal is tailored to the Transformer and features two key modifications to encourage more disentangled representations and better computational efficiency.\n\nConclusions\nIn this paper we focused on two issues related to compositional generalization. Firstly, we improve upon Dangle, an existing sequence-to-sequence architecture which generalizes to unseen compositions by learning specialized encodings for each decoding step. We show that re-encoding keys periodically, at some interval, improves both efficiency and accuracy. Secondly, we propose a methodology for identifying compositional patterns in real-world data and create a new dataset which better represents practical generalization requirements. Experimental results show that our modifications improve generalization across tasks, metrics, and datasets and our new benchmark provides a challenging testbed for evaluating new modeling efforts.\n\nLimitations\nOn machine translation, the optimal generalization performance requires using small interval values. However, R-dangle with small intervals still runs much slower than an equivalent Transformer model. In this paper, we only explore a simple periodic reencoding strategy. However, more complex and flexible ways of re-encoding could be used to further narrow the gap. For instance, we could adopt a dynamic strategy which learns when re-encoding is necessary. Table 6 showcases examples from the ReaCT test set. These are novel syntactic patterns approximated by POS n-grams. As mentioned in Section 4, ReaCT is created by detecting compositional patterns in relation to an existing training set from a diverse pool of candidates.\n\nB Dataset Details\nWe evaluated our model on two machine translation datasets, and one semantic parsing benchmark which we selected to maximally reflect natural language variations and real-world generalization challenges. We describe these in detail below.\nReaCT is the real-world machine translation benchmark developed in this paper for compositional generalization. The IWSLT 2014 De→En dataset consists of approximately 170K sequence pairs.\nWe used the fairseq script prepare-iwslt14.sh to randomly sample approximately 4% of this dataset as validation set and kept the rest as training set. Following standard practice, we created an in-domain test set, the concatenation of files dev2010, dev2012, tst2010, tst2011, and tst2012. We created an outof-distribution test sets from the WMT'14 De→En training corpus following the uncertainty selection method based on sequences.\nCoGnition is another machine translation benchmark targeting compositional generalization (Li et al., 2021). It also contains a synthetic test set to quantify and analyze compositional generalization of neural MT models. This test set was constructed by embedding synthesized novel compounds into training sentence templates. Each compound was combined with 5 different sentence templates, so that every compound can be evaluated under 5 different contexts. A major difference between RE-ACT and CoGnition is the fact that test sentences for the latter are not naturally occurring. Despite being somewhat artificial, CoGnition overall constitutes a realistic benchmark which can help distinguish subtle model differences compared to purely synthetic benchmarks. For example, Zheng and Lapata (2022) showed that their encoder-only Dangle variant performed badly on this dataset in spite of impressive performance on synthetic semantic parsing benchmarks (Kim and Linzen, 2020;Keysers et al., 2020). the account data is provided to you directly via e-mail .\n• a couple of hours ( DT NN IN NNS ) later , the sun will shine on the next magnifying glass .\n• but this could also be used for good . (   SMCalFlow-CS  is a largescale semantic parsing dataset for task-oriented dialogue, featuring real-world human-generated utterances about calendar management.  proposed a compositional skills split of SMCalFlow (SMCalFlow-CS) that contains singleturn sentences from one of two domains related to creating calendar events (e.g., Set up a meeting with Adam) or querying an org chart (e.g., Who are in Adam's team? ), paired with LISP programs. The training set S consists of samples from single domains while the test set C contains compositions thereof (e.g., create a meeting with Adam and his team). Since zero-shot compositional generalization is highly non-trivial due to novel language patterns and program structures, we follow previous work Qiu et al., 2022) and consider a few-shot learning scenario, where a small number of cross-domain examples are included in the training set. We report experiments with 6, 16, and 32 examples.\n\nC Implementation Details\nMachine Translation Models We implemented all translation models with fairseq (Ott et al., 2019). Following previous work (Li et al., 2021;Zheng and Lapata, 2022), we compared with the baseline machine translation models Dangle and Transformer using the popular fairseq configuration transformer_iwslt_de_en. We also implemented a bigger variant of these models using a new configuration, which empirically obtained better performance. We used 12 encoder layers and 12 decoder layers. We set the dropout to 0.3 for attention weights and 0.4 after activations in the feedforward network. We also used pre-normalization (i.e., we added layer normalization before each block) to ease optimization. Following Zheng and Lapata (2022), we used relative position embeddings (Shaw et al., 2018;Huang et al., 2020) which have demonstrated better generalization performance.\nHyperparameters for R-Dangle were tuned on the respective validation sets of CoGnition and Re-aCT. Both R-Dangle shr and R-Dangle sep used a 12-layer decoder. For R-Dangle shr , we tuned the number of layers of the two adaptive components k 1 and k 2 , and set k 1 and k 2 to 2 and 10, respectively. For R-Dangle sep , we shared some layers of parameters between the value encoder and the adaptive key decoder and experimented with different sharing strategies. Finally, we adopted a 10-layer value encoder and a 10-layer key encoder (k 1 = 2 and k 2 = 8). The top 8 layers in the two encoders were shared. This configuration produced 12 differently parametrized transformer encoder layers, thus maintaining identical model size to the baseline. Qiu et al. (2022) showed the advantage of pre-trained sequence-tosequence models on SMCalFlow-CS. We therefore built R-Dangle on top of BART-large (Lewis et al., 2020), which is well supported by fairseq. We used BART's encoder and decoder to instantiate the adaptive encoder and decoder in our model. For compatibility, we only employ the R-Dangle shr architecture. We also set k 1 and k 2 to 2 and 10, respectively."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-12"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 100518,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254493065"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Wittgenstein on Criteria and Practices\n\nIn the interpretive literature from the 1950's through the 1970's the term 'criterion' was thought to be a central key to the understanding of Wittgenstein's later philosophy. Later on, it was relegated from this place of honour to being one of a variety of expressions used by Wittgenstein in dealing with philosophical questions. This Element tries to account for the shifting fate of this concept. It discusses the various occurrences of the word “criteria” in the Philosophical Investigations, argues that the post-Wittgensteinian debate about criteria was put on the wrong track by a problematic passage in Wittgenstein's early Blue Book, and finally gives an overview of the main contributions to this debate, trying to achieve a reconciliation between the rival conceptions."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-08"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 104652,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254904055"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Critique and Speculation: Reconsidering Hegel's Early Dialectical Logic\n\n\n The aim of this article is to clarify the critical role of Hegel's early logic, through an assessment of the dialectical process of sublation [Aufhebung] of the determinations of finite thinking at stake within its exposition. I want to show that the dialectical-critical work of logic has a speculative meaning for Hegel, thereby displaying the inward correspondence between critical and speculative aspects of philosophical activity. By pointing out the evidence from fragmentary texts on logic relating to Hegel's teaching activity in 1801–1802, I will first put into question the idea of an introductory role of logic. In so doing I challenge a widespread reading which argues for the presence of a sharp separation between critical logic and speculative metaphysics. I will then focus on the texts on logic in the 1804–1805 Reinschrift, reading them as the worksite wherein the dawning form of a full-fledged dialectical logic is first prepared and elaborated. More generally, if this paves the way for establishing a continuity between Hegel's early and mature logic and his concept of dialectic, it is also paramount for understanding how the activity of systematic philosophy in the mature version of the system essentially constitutes an ongoing work on the forms of the finite."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 105105,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255042618"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Is music just an outward form? Some philosophical issues of aesthetics in G. Shpets’s works\n\n. In the article the aspect of perception of philosophy of art is represented aesthetically from the point of view of a national philosopher, psychologist and art theorist, founder of Russian hermeneutics G. Shpet. His conception of questions of correlation between philosophy and art, art and life are analyzed. Reviewed in the article are the questions of correlation between art and beauty, beauty and meaning; the author attempts to de-fine the role and function of decoration in art. The article covers a question of reality and copyism in the aspect of creativity as well as the role of an artist in the creative process and differential characteristics of artistic sight. Philosophical categories of the form and nature in the context of art, as well as conception of categories of outer and inner subject of aesthetics are considered. The author also provides a general outline of musical art from the point of view of G. Shpet. In accordance with such understanding, an attempt is made of working out the problem of form and nature of music, its definition of outer and inner form and relevant meanings of expression, correlation between music and other forms of art. It is suggested that music belongs to temporal and plastic forms of art, due to its specificity of forms of existence and means of expressing reality as well as its appropriated language."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-20"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 106097,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255330610"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Dostoevsky on the Metaphysical Mystery of the Unhappy Consciousness\n\n\nWe argue that such trends of modern philosophy as philosophical anthropology, hermeneutics and phenomenology in their ideological origins were born in Russian philosophizing, so that the ideas and images of Dostoevsky’s works have been revealed as their harbinger. More specifically, he supplemented the categorical apparatus of philosophy with the concept of mystery, foreseeing the specifics of descriptive language, which has become relevant to the specifics of modern philosophical and anthropological discoveries and socio-ontological constructions. In addition, we consider the phenomenon of unhappy consciousness found in Notes from the Underground and make an assumption about intersubjective (pluralistic) idealism – an original metaphysical conception, within the framework of which the definition of man as a mystery can be explained."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-22"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 110251,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254620011"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Implicitness of Logos and Explicitness of Logics in Ancient Philosophy\n\nWe consider semantic and syntactic transformations of the concept of \"the logical\" in the ancient philosophy in the form of crypto-logos, para-logismos, dia-logos, and syl-logismos.  We interpret Heraclitus' concept of Logos as a cryptologos through which intuitive insight (epístasthai gnóomen) reveals hidden or implicit harmony (harmoníe aphanés) in nature (phýsis) as a conceptual unity of ontic opposites (tà enantía). In Pramenides' paraconsistent concept of the identity of Being and thought, we point to para-logical hypotheses about the One that are carried out through antithetical deductions of thought and which maintain the dynamics of the ontic determinations of being (ón) in the statics of the conceptual determinations of Being (tò eînai). As the beginning of the explicative granulation of ''the logical'' we consider Plato's concept of the dialectical skill (dialektikè tékhne) of dividing concepts of genus into species and sub-species that logically represent ontic opposites in problem-formulated questions. Finally Aristotle's concept of lógos as a statement-making sentence / proposition (lógos apophantikós) made explicit the Being (tò eînai), or the Being as Being (tò ón hê ón), in semantic and syntactic figures and modes of syllogistic inferences in which ontological (eînai), ontic (ón), conceptual (logikôos) and linguistic (légomenon) correspondence is shown. We conclude that with these changes in the concept of lógos, the path has been taken from the hidden or implicit Truth of the phenomena of nature and the world (pân) to explicit truthfulness of propositions as the unhiddeness (alétheia) of Being trough the semantical and syntactical visibility of the logical structures of being, thought and language in scientific knowledge based on demonstration (apoódeiksis)."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-12"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 112380,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "248022259"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Cascade versus Mechanism: The Diversity of Causal Structure in Science\n\nAccording to mainstream philosophical views causal explanation in biology and neuroscience is mechanistic. As the term “mechanism” gets regular use in these ﬁelds it is unsurprising that philosophers consider it important to scientiﬁc explanation. What is surprising is that they consider it the only causal term of importance. This paper provides an analysis of a new causal concept–it examines the cascade concept in science and the causal structure it refers to. I argue that this concept is importantly diﬀerent from the notion of mechanism and that this diﬀerence matters for our understanding of causation and explanation in science. This paper provides an analysis of the cascade concept in science and the causal structure it refers to. 2 I examine the main features of this causal structure, analogies it is associated with, and strategies used to study it. While scientiﬁc work supports distinguishing the cascade and mechanism concepts, this analysis is not merely descriptive. Instead, it provides a theoretical framework for how these concepts should be understood. This framework matters for our assessment of the causal structure of the world, how we study this structure, use it to produce particular outcomes, and communicate about it to others. Before proceeding with this analysis, two clariﬁcations are in order. First, I do not suggest that scientists always use these causal terms in the distinct ways indicated in this analysis, but that they often do and should use them in this way. This reveals normative features of this work and an important way that philosophy can contribute to science, namely, by making suggestions for how these concepts should be understood and used. Second, my analysis of these concepts articulates clear ways in which they diﬀer, but leaves space for some structures in science to be borderline. The presence of such cases should not prevent us from articulating useful categories that distinguish causal structures in the majority of cases, even if the distinction can (in rare cases) be blurred."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-05"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 116752,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255116071"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The Dialectic Moments of Wissenschaftslehre\n\nFichte’s Science of Knowledge (Wissenschaftslehre) creates the basis of his own philosophical system. In this work, it is possible to see the main lines of the \"philosophy of action\", which has become a kind of equivalent of the \"Fichte Philosophy\" in the literature. For Fichte, who identifies his philosophy with his own life, a whole philosophical adventure is to complete the moments which the Ego will create with action/posit. As a matter of fact, we can easily see that Fichte was constantly in action in his life. The Science of Knowledge (Wissenschaftslehre), which came into existence in such an adventure of life, will show that all the actions of humanity have a final telos. Science of Knowledge is tasked with positing the principles of the independent sciences. Because, according to Fichte, another science from which each science will take its principles is almost compulsory. Since sciences cannot produce their own principles, such an occupation is the subject of another science that has not made the particular field an occupation. The science tasked with positing these principles to other sciences is the 'Science of Knowledge (Wissenschaftslehre)'. Also, the Wissenschaftslehre itself is an independent science. Just as the other sciences that construct their own understanding of science with the principles laid down by the Science of Knowledge (Wissenschaftslehre) are independent and have issues and issues within themselves. As a matter of fact, this field of science, like other sciences, has an object that it examines, which is human. Science of Knowledge shows that as a starting point, following the preparation process that a person creates with false assumptions (sense certainty) before becoming aware of his own reflection ability, and then becoming aware of his own reflection ability. At this stage, the subject is in a state of contentment that is not yet real. After these stages, a follow-up of the Ego process, in which this awareness (reflection ability) is achieved, and afterwards, is carried out in the Science of Knowledge. The basic question of the Science of Knowledge is to describe how the ego can know something, and through which processes this act of knowing takes place In this sense, Science of Knowledge is concerned with following human reasoning and processes regarding knowledge. In this way, what is done is to reveal a philosophical anthropology of humanity. No action in the past can be snatch off the individual existence of the subject. Because each subject is the Absolute Ego who walking around the world. Especially the revision of the Science of Knowledge in 1810 and the text titled Basic Characteristics of Our Age give a different meaning to historical actions However, it should not be forgotten that the philosophical heritage of humanity is on the verge of great crisis of thought, especially in the period when Fichte lived. Science of Knowledge, which is the product of such a painful process, inherited some philosophical legacies before it in some aspects. It is among the main aims of Science of Knowledge, especially to evaluate the actions of human beings in the context of a final telos. Following the philosophical legacy and establishment upon which the Science of Knowledge, which has acquired such a task, is built, will strengthen the understanding of all the assignments that the Science of Knowledge has undertaken. However, one of the objectives of this research is to understand how Schelling and Hegel, who came after Fichte and created a new threshold in the literature in different ways with their History and Systematic Philosophies, were influenced by Fichte. Shortly in this study, the philosophical traditions, the initial principles, the dialectical moments and the freedom that will be seen at the end will be followed depictly."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-25"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 117968,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254712305"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "M. Bakhtin and G. Shpet in the context of Hermeneutics and Communication Theory: legacy and research prospects\n\nM. M. Bakhtin and G. G. Shpet play an important role in the history of the humanities in Russia. Their philosophical views correspond to the time of their lives and generally reflect the tendencies in the development of the humanities in the first half of the twentieth century. The authors of the article demonstrate a close connection and a certain continuity of philosophical views and principled positions in the theoretical concepts of G. G. Shpet and M. M. Bakhtin. This concerns not only common scientific origins and common preferences in their contemporary scientific research, but, above all, conceptual convergence in a number of fundamental issues that unite both scientists by the framework of hermeneutics and, more specifically, the hermeneutical model of communication. This concerns common views on the methodology of humanitarian knowledge, the methodology of cognition, the intersubjectivity of understanding, an interest in the dynamics of development, co-being and deed, an attitude to the word as a sign, to contextualization as a condition of finding meaning, an attitude to speech as a deed, its belonging to the author, in general - an interest in the practice of social interaction between people, etc. The theories of G.G. Shpet and M.M. Bakhtin, each in its own way, are valuable, multifaceted and not fully exhausted in terms of scientific potential, projection and influence on modern humanitarian science. The authors insist on the need to consider the hermeneutic component of the works of M.M. Bakhtin and G.G. Shpet in tandem, using the methodology of comparative and comparative analysis. Particular emphasis is placed on the way in which M.M. Bakhtin developed and used in his works the principles of H.G. Shpet's hermeneutics, in particular his hermeneutic model of communication, extending it with the cultural component and extending \"to all the sign phenomena of intellectual life\". Of great scientific interest is the comprehensive (comparative) study of all the scientific works of M.M. Bakhtin and G.G. Shpet as an aggregate phenomenon in the history of Russian (Soviet) philosophy, including its relationship to communicativism. Such a study, in the opinion of the authors, is extremely needed and could yield useful results right now. The key concepts of G.G. Shpet and M.M. Bakhtin's theories - personality, uniqueness, identity, individuality, word, value, deed, responsibility, understanding, dialogue - become more than relevant scientific categories and targets of the dynamics of world and society development from the perspective of all forms and aspects of globalization we are experiencing, in conditions of radical nationalism and populism, with regard to modern trends in international relations and social interaction."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-14"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 120716,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254993948"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The Tractatus Logico-Philosophicus : a centenary disease\n\nAlbert Maslow points out that Wittgenstein dedicated a copy of the Tractatus to Morris Schlick with the following sentence: “ Jeder disese Sätze ist der Ausdruck einer Krankheit ” (Each of th ese propositions is the manifestation of a disease). We will try to see some of the treatments to see if the remedy is not, in many cases, worse than the disease. We will point out that it’s a crisis text, one that displays not just his o wn pathology, but that of the whole western philosophy.\n\n\nIntroduction\nAlbert Maslow points out that Wittgenstein dedicated a copy of the Tractatus to Morris Schlick with the following sentence: \"Jeder disese Sätze ist der Ausdruck einer Krankheit\" (Each of these propositions is the manifestation of a disease). We will try to see some of the treatments to see if the remedy is not, in many cases, worse than the disease.\nFew philosophical texts have so many materials surrounding it as the Tractatus, but and at the same time do maintain such a vast spectrum of interpretations. To the point that one can expect every now and then, exegetical revolutions. Like the political ones, they usually burry the past in its way. An example: the appearance in the 1980s of Wittgenstein's Vienna was a fundamental milestone that showed that it is impossible to dissociate the author from the Viennese intellectual environment. However, attempts have been made to present this new approach as the refutation of the rest, falsely equating the Tractatus with continental philosophy in a book that is really weak in terms of elements of symbolic logic and without any expertise in regard to the philosophy of logic of Frege and Russell, which are essential to understand the context of the concepts in the text of the Tractatus itself and the records of the time.\nA similar case is the new interpretation that was awarded some ten years later by the North American school: they rightly called attention to the neglect of the essential paradox of the Tractatus of having intended to say what is shown. However, the value of this perspective is lost when the book is treated as the work of a postmodern nihilist, an antimetaphysical dialectician. The same can be said of the interpretation of Raymond Bradley, whose praiseworthy efforts to highlight the Tractarian modal 6 GARMENDIA, S.\nTo show an overview, I would broadly follow the outline of Cerezo (1998), adding and extending her taxonomy a bit. I think Cerezo's outline is accurate and simple. Cerezo does not take into account contextualist readings or, in any case, it isn't her concern in her Lenguaje y Lógica in Wittgenstein's Tractatus. Let (Stern, 1996)  Hacker's, with a logically based idea of metaphysics; 4. The irrationalist reading (irrationalist reading) that Toulmin and Janik maintain, for example, or Isidro Reguera, where there is pre-eminence of the religious ethic elements; and 5. therapeutic reading (therapeutic reading), which poses it as a kind of infinity joke. We are going to add to the list a reading, which we will call a pathological reading.\n\nSome notes on biographical fascination\nOf course, the problem is very abstract and universal, but I think important to open this article with this fundamental theme for Wittgenstein: the relationship between the life and work of authors. How crucial is it to study the circumstances surrounding a philosophical work? James Conant, in his article \"Philosophy and Biography\" (Conant, 2001) typifies two extremes that are illuminating: the Reductivist and Compartmentalist views regarding the relationship between biography and work. The first can become a kind of epistemic expressionism that reduces the work to its author or his biography, with The compartimentalists are, on the contrary, those who deny the link and, therefore, the value of the biography to understand the philosophical thought of an author. Samuel Schkolnik, a philosopher from Tucuman, used to say in his classes that we are contemporaries of all those who explored the region of which the question in question is the name and emblem. It would be the Borgesian idea that when we ask ourselves the question about supersensible reality, like Plato, at that moment we are Plato. But, of course, we are not Plato.\nLet's try to contribute to get out of this crossroads a little. Perhaps it can be found, both in the compartmentalists and in the reductivists, that bias, the one that Roland Barthes calls the empire of the author. In his essay \"The death of the author\" he presents some notes on the turn of modern literature, marked by the hegemony of the author's first person and the magic of the signature: The author is a modern character, undoubtedly produced by our society, to the extent that it […] discovers the prestige of the individual or, put more noblely, of the \"human person\". The author still reigns in literary history manuals, bibliographies of writers, interviews in magazines, and even in the very conscience of writers, who take good care to unite their person with their work thanks to their intimate diary […] as if, through the more or less transparent allegory of fiction, it was, ultimately, always, the voice of one and the same person, the author, who would be delivering his \"confidence\" (BARTHES, 1994).\nBourdieu will make a similar point by relating the magic of the word to Pret a porter and high culture (Bourdieu, 1975). The model that Barthes discusses, and to which we are equally opposed, is that of a writer who wants to say something that he has in his mind, that pre-exists writing. The image, is that with his idea in the mind or at the tip of his tongue, he sits down to write and it will be his expressive capacity what determines the effectiveness of the message. So to speak, his words and other 8 GARMENDIA, S.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 04-16, out./dez. 2022 expressive repertoire are the instrument for the intentional meaning. The critic will be the one to judge the ideas and whether or not he has had the ability to put the message on paper. The interpretation of the text is surrounded, like a television program, by biographical data, possible explanations and clues that the author may have given during his life, as well as personal records, diaries, friendships, courtships, uncles and cousins. The author himself is a prisoner of his charm, when, as Barthes says, they take care in many cases to keep an intimate diary that everyone has access to and that allows their message to be better deciphered. No one can ignore that such a thing does not happen only in the broad literary domains, but also in the philosophical genre. And very especially in Wittgenstein, whose published private work is overwhelming with respect to the lets say \"exoteric\" -just the Tractatus and a paper-, with blame for the editors, but also responding to the illuminating clues that he left in his diaries and annotations.\nIn the author's empire, the message, the meaning, is platonized in the way of pre-existing ideas to writing and probably to its intellection. The interpretation revolves around the actions of the author, with which there is a methodological reductionism, we look back to the context to reconstruct what he really meant. What Barthes proposes is, on the contrary, to emphasize the figure of the reader and his necessarily active position in the hermeneutics of the text. Meaning is a construction between individuals, but also between times and social classes. As Ricoeur says, even the copyist's task leads to a hermeneutic. And the most important thing that I want to rescue from the Barthesian idea: there is no end. Only the platonic empire of author can give such an end.\nTo give a text an Author is to impose an insurance, to provide it with an ultimate meaning, to close the writing. This conception suits critics very well, which then intends to dedicate itself to the important task of discovering the Author (or his hypostases: society, history, the psyche, freedom) under the work: once the Author has been found, the text is \"explained\", the critic has achieved victory (BARTHES, 1994).\nIs this the reason why there are so many interpretations of Wittgenstein? Do they demonstrate the mega-concept of no author no end? Or is it an infinite task because we have only a compartimentalist approach?\n\nThat man will be revolutionary who can revolutionize himself.\nThis is why Wittgenstein himself opens the door for the biographical passion that he has aroused: the conversion of himself into a saint who gives testimony of right, a Tolstoy who lives his words (or lack of words). Professor Tomasini Bassols, both Russell and Wittgenstein expert, told me an anecdote that I wasn't able to find.\nHe told me that when Wittgenstein read Moore's autobiography, he said, \"Remind him that a shoemaker also had a childhood.\". But he did not mean disparagingly that there is nothing extraordinary in Moore's life (or anyone else's), but that Moore presented his childhood with an extraordinary halo that carried a genius. This puts Wittgenstein in tune with the old skeptical idea of philosophy. There is no good philosophy if there is no work on oneself. The worst fallacy, for him, is that inauthenticity between life and philosophy. Hence that disease called Tractatus.\n\nThe disease called Tractatus\nWittgenstein's life has been scrutinized by great philosophers. I am referring to the fact that it has not only been the object of attention of writers who have seen in it a good story -there is one-, but also many valuable thinkers, such as Secondly, other peculiarities cannot be ignored: he is a writer of great beauty, depth and self-reflection on his own style. Both the Tractatus and the Investigations are great literary experiments; not to mention the loose phrases and the annotations where he was carried away by passions, insults and ridicule. More than once he seems to be haranguing himself to have strength and courage, or he exclaims some conceptual banner so as not to get lost in his ideas. For example:\n\nMy style is like bad musical composition. Don't apologize for anything, don't obscure anything, look & tell how it really is--but you must see something that sheds a new light on the facts\"\n(MS 123 112: 1.6.1941)  Added to this is the fact that his thinking has undergone enormous changes.\nOne of them, the change in the conception of meaning, sharply divides the waters of his philosophy. But there is also an enormous number of oscillations and transformations, which, to make matters worse, have been buried because they were not published in a timely manner. They require an elucidation work that is, we could say, by geological sediments. We find in his writings an enormous variety of layers where what seem to be small nuances, almost imperceptible differences in the way a concept is presented, are often the mark on the rock of an enormous intellectual activity, of true philosophical storms. This is the reason why Anthony Kenny, for the notebooks and the works after the conceptual turn of 1930. In the case of the notebooks, the difficulty is that every day he changes his mind and, therefore, is very difficult to collect a thesis that covers them from end to end. Of course, they are diaries. Paradigmatically, we can mention the issue of objects and their simplicity: there are enormous changes between the entries for the month of June 1915 in this regard.\nIn Philosophical Investigations, in On Certainty and in his class notes the challenge is, instead, that they have been written as dialogues between opposing thoughts, and it is therefore very difficult to identify which of them is truly Wittgenstein. Let us note, as an example, the number of layers present in the following paragraph: What does it mean to say that we cannot attribute being or non-being to the elements? -one could say: if everything we call \"being\" and \"not being\" consists in the existence and non-existence of relations between elements, then it makes no sense to speak of the being (non-being) of an element... But it would mean: Being cannot be attributed to the element (Wittgenstein, 1999, IF, 50).\nThen the appeals continue in the paragraph: \"let's imagine such a thing\", \"we could express this idea in this way\", \"then say such a thing\", etc. They seem, if you allow me, the draft of a platonic dialogue. It is known that Wittgenstein failed when trying to give them a systematicity that would satisfy their publication and that the reason for this failure goes beyond a mere esthetic or stylistic issue: form and content have always been related in Wittgenstein's writing. Following Mijaíl Bajtín's thesis on the origin of the novel (BAJTÍN, 1989), and continuing our brief analogy, if Plato is an antecedent of the modern novel due to his polyphonic dialectic, where opposing voices refuse to close the reading, Wittgenstein is, without a doubt, one of its highest points.\nThe case of the Tractatus -and this applies to all the writings of the first decade as well -is different. The point is that its reading is impossible without appealing to a host of arguments that hide behind each aphorism. The Tractatus is an enormous system -not as coherent and finished as one would like, because its in movement, we will remark this later-, from which only what can be considered the apex of the iceberg of each philosophical thesis emerges. With a bit of irony we can say that what is not written really matters a lot. Several such arguments or motifs are found in Wittgenstein's personal letters and diaries of the time.\nTo summarize: there is a strong biographical pressure, of unusual vigor, exerted largely by the author's stylistic manias and reluctance to print. But there is also, as we will point out, an attempt to fix the text, because it is thought that there is no tension in its content. The message of the author is not said. The text is voluble. This is precisely what we are trying to deny. You cannot fix it. Its breaking, it is meant to brake.\n\nSecond Navigation\nWittgenstein tried in the Tractatus to make a steady, rigid picture of the world impulsed first by his fascination with mathematical logic. But then while writing, while fighting, made a second navigation into the mystical, as a corollary of logic. This twofold nature, this two powerful sources of philosophy melt down in the Tractatus.\nThat is why, I think, the Tractatus colapses, breaks without a syntesis.\n\nLet me say some words to support that Wittgenstein wasn't a Jung Wien in\nCambridge, perhaps happened the contrary, but not our quest for the moment.\nWhoever immerses himself in the material of the first years, in the in the diaries, notes and correspondences of the time, finds a constant concern in all the writings, and by this I mean the ones before and after to the first war, for sharply distinguishing the logical plane from the empirical, the philosophical from the scientific. It can be seen that two impulses coexist, which chronologically follow one another before and the other after the trench: the first points out that philosophy is logic, then it consists decidedly in the scrutiny of the structure of language, from which They will establish the principles of language and denounce abuses and confusion.\nThere is a normative effort -this is ethical on their part -to unveil the structure of the language that emerges from that time of the war. P. Shields has convincingly shown the intimate link between logical duty and ethical duty (Shields, 1997). To give a notable example: in his first published text, the review of the book The science of logic, by P. Coffey, writing in 1912, the confidence of Wittgenstein more auspicious manifest advances of the mathematical logic of the time than what the one the Coffey commentary. A quote will suffice for illustration: Aristotle, whose name is brought up in vain by our logicians, would twist in his grave to learn that so many logicians know no more about logic today than he did 2,000 years ago. The author (that is to say P. Coffey) has not accused the slightest news of the great work of modern mathematical logicians, a work that has brought with it an advance in logic only comparable to that achieved by astronomy from astrology, from the chemistry from alchemy. (Wittgenstein, 1997, p. 23).\nThen, especially after 1914, where the logic/philosophy separation is no longer one of levels, philosophy does not get rid of logic as a science, but it no longer believes that there is such a formal science. We found this second navigation from 1914-15 to 1919, the year of the preliminary versions of the Tractatus. The reason for this turn seems to be, as we pointed out, the experience of war.\nThe comparison between the silence of proposition 7 of the Tractatus and what Walter Benjamin points out in his brilliant essay \"The Narrator\" about the fact that soldiers return from modern war not richer in experiences, but impoverished, is very fruitful: \"With the world war began to manifest a movement that has never stopped until now. Wasn't it noticed, during the war, that people returned mute from the battlefield? Not richer in transmittable experiences, but poorer\" (Benjamin, 1980, 116 Ausdruck einer Krankheit\" (Each of these propositions is the manifestation of a disease).\nWhat disease? The explanation lies in this tension between scientific optimism and existentialist humanism that fight in the book. In terms of CP Snow: the two cultures (the scientific and the humanistic) lodge without synthesis in Wittgenstein by the time of the Tractatus, where \"one hears not only the clear voices of Frege and Russell, but also the echoes of Kant, Schopenhauer, Plato and even Saint Augustine\" (Maslow, 1961, X). Let us then try to listen to Wittgenstein and, as far as possible, to the buried voices of those who speak behind him. We will hear noises, disputes, but perhaps we will also perceive that the whole forms, deep down, a secret symphony. What is the name of that symphony? The inseparability.\n\nSketch of a pathological reading\nIf there is an idea common to all readings, it is that the Tractatus thematizes the limit, or rather the inseparability of language-subject-world. We can refer to this with the denomination of a tractarian project. The second thing is that any proposal about the subject, language or world, that dispenses with the others, falls into the absurd.\nSo it is with the idea of solipsism that merges with realism, of ontology that ends up being logical, and of linguistic universalism that ultimately ends up in the ineffability of semantics. On another level, the philosophy of language that wants to deal with this triad is doomed to self-checkmate of 7.\nWhat happens in the Tractatus is that it is frequently forgotten that it is a symptomatic book of an illness, that it is pathological because it is a crisis book.\nWittgenstein does not have the language to say what he wants, he has not managed to detach himself entirely from the dichotomies inherited by Fregean-Russellian analytical philosophy. We refer to the fact that, to a certain extent, he is still captive to traditional categories against which he seems to be fighting with enormous will, but in an unequal battle. We point out then that the Tractatus is a crisis text in the Gramscian sense; this display of inseparability is still embedded in a certain metaphysical vocabulary. The Sardinian points out that a crisis breaks out when \"The old is dead and the new cannot be born. In this interregnum, the most varied morbid phenomena take place\". What has died, what is pushing to existence? What is dead is the old way of doing philosophy, uncritical about language, that thinks that can thematize reality and thoughts without considering languge. His pathological, unstable condition becomes apparent when his ontology, his psychology and his logic are developed, which, in indeterminacy, total ineffability and subject-world identification. The Tractatus is not more of the same, but the latest of the same, the best of the same that has not yet managed to be the first of the new.\nFor this reason, to conclude, we leave raised two metaphilosophical movements that are related to inseparability. In the first place, we have the denial of traditional philosophy for not having understood the deep logic of language (4.003). But Wittgenstein is not simply trying to change one piece (ontology, epistemology) for another, he is looking to kick the board. Immediately we must warn, due to what is developed in this article, that there is no more Tractarian moral than the impossibility of the philosophy of language, if it intends to study language without the subject and the world intervening. Perhaps as a synthesis of this double negation, the challenge of accepting the conditioning of and to language remains, turning its elucidation into the very task of philosophy. Not to heal us, but to know we are sick."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 127134,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255041939"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Philosophical wisdom and bioethical expertise: a wittgensteinian approach\n\nThis paper explores the interconnections between philosophical wisdom and bioethical expertise, arguing that the latter can be enlightened by the former. First, it analyses the arguments of those who think that philosophy has nothing to say to bioethics, and afterwards, it shows that philosophical wisdom has an irreducible ethical component, which in turn helps bioethics to reach its aims. Thus, this paper shows that some Wittgensteinian commentators (e.g\n\n\nIntroduction\nIn his chapter \"Bioethics, Wisdom, and Expertise\" of the book edited by Carl Elliott, Slow Cures and Bad Philosophers: Essays on Wittgenstein, Medicine, and Bioethics, Paul Johnston argues that since there is no independent adjudication of conflicting moral claims, there can be no experts in bioethics. In this paper, I reject Johnston's skeptical views. In doing so, I show that philosophy-particularly a Wittgensteinian approach that is based on wisdom-may in fact lead to some kind of bioethical expertise. In turn, this expertise can help researchers/doctors (in their dual role) and research participants/patients to take decisions when faced with tensions and apparent dilemmas in their professional lives.\nTo achieve this goal, I will divide this paper into three parts: in the first, I will reconstruct Johnston's arguments in detail and examine his main assumptions and conclusions; in the second, I will defend a \"return to Wittgenstein,\" critically examining Johnston's views; in the third, I will illustrate how a Wittgensteinian approach to philosophy may help bioethicists to achieve their objectives by providing theoretical instruments for researchers/doctors and participants/patients to reach better decisions.\nMy aims in this work are modest: namely, I intend to make a small contribution to the centenary of Wittgenstein's seminal work Tractatus Logico-philosophicus by showing that its main conclusion \"Wovon man nicht sprechen kann, darüber muss Philosophical wisdom and bioethical expertise 53 Rev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 51-67, out./dez. 2022 man schweigen,\" (normally translated as \"What we cannot speak about we must pass over in silence\") can be read as an expression of Socratic wisdom, which is of crucial importance for current bioethical debates.\n\nWisdom and Expertise in Bioethics\nPaul Johnston has made significant contributions both to Wittgensteinian scholarship (JOHNSTON, 1989;1993) and to our understanding of moral philosophy (JOHNSTON, 1999). Without wishing to oversimplify, I would argue that Johnston's main work in ethics amounts to the meta-ethical view according to which there is no way of resolving conflicting moral claims since judgments of human action simply reflect the different dispositions or preferences of the people who make them. This is clearly an epistemologically non-cognitivist and ontologically antirealist metaethical view. However, some problems emerge in both Johnston's interpretation of Wittgenstein's late work and his conception of what modern morality amounts to. In this paper, I will mainly focus on Johnston's skepticism regarding philosophy's possible contributions to bioethics.\nTo assess the work \"Bioethics, Wisdom, and Expertise,\" it is crucial to first make a detailed reconstruction of Johnston's main arguments. Before doing that, however, I would like to stress that I do agree with Johnston's reading of Wittgenstein as someone who, throughout his life, held that ethics cannot be a science. More specifically, I agree that \"The essence of [normative] ethics is the claim that there are ways of acting that everyone should recognize as right and wrong.\" (JOHNSTON, 2001, p. 150). However, pace Johnston, from the fact that moral claims cannot be derived from logic or be supported by empirical evidence, it does not follow that ethics cannot be an autonomous field of enquiry with specific methods and its own standards of justification. And it does not follow that moral judgements cannot be seen as objective and thereby apply to every person. I would like here to recall briefly the Tractatus to illustrate this point: both logic and ethics are said to be transcendental even though there are no ethical (and logical) propositions, that is, pictures of states of affairs. Ethics does not deal with facts but with (absolute) values-which can be shown, although not propositionally. Moreover, To start assessing Johnston's arguments against bioethical expertise, it is essential to quote his own definition of expertise: \"…an expert is someone who claims authority on the basis of having mastered a body of knowledge.\" (JOHNSTON, 2001, p. 151). To show that there are no experts in bioethics, Johnston refers to some conflicting views on contraception, abortion, and infanticide. He argues that since people strongly disagree, for instance on abortion, there is no place for an expert to say: \"I have studied these matters deeply, and these are the conclusions I have reached,\" since people would continue to perform abortions or decline to have abortions irrespective of the expert's views. According to Johnston (2001, p. 152), this makes any talk of expertise problematic and, consequently, \"the bioethicist cannot claim to be the purveyor of a body of knowledge arrived at by an approach or method accepted by everyone, not can she demonstrate the correctness of what she says.\" As we can see, Johnston's first argument relies on the well-known problem of moral disagreement. And I will return to this later.\nJohnston's second argument recognizes that valid medical moral problems exist and need to be solved, but it denies that an expert in bioethics could help in any abound, and bioethics itself was born as an independent field in the 1970s −separately from medical ethics− exactly because of a medical scandal (the infamous Tukesgee case, to which I will return soon). If I asked Johnston why bioethicists should not decide, he would simply reply by questioning what proper training might be in this area. To make his point, he asks (2001, p.154): \"does not twenty years of studying the arguments surrounding euthanasia or a lifetime or working with the terminally ill give someone a special qualification for speaking about this issue?\" And his response is simply that the bioethicist's judgments would have no special status-without further debate. Again, his reason is that there is no body of moral knowledge to be mastered by the expert; therefore, she has no moral authority. Johnston's meta-ethical view, then, amounts to holding a non-cognitivist moral epistemology, which I will scrutinize further in sections two and three.\nThe third argument relates to how philosophy (assuming here a distinction between knowledge and wisdom: the sciences and philosophy) could contribute to helping bioethicists. To mark the difference, Johnston calls a \"moralist\" the person who holds that ethics (and bioethics as its subfield) deals -as the above definition made clear-with the correct way of acting and can thereby differentiate the wise (ethical) decision from the foolish one. Thus, according to the moralist, not all moral judgements are merely a matter of opinion or simply express the preferences of the person who makes them. A moralist is, meta-ethically speaking, a cognitivist (epistemically) and realist (ontologically). Johnston, however, argues that while the bioethicist could claim to have wisdom rather than expertise, this assertion is not one people in our society are encouraged to make, or -at least -there are no universally accepted wise persons since there is no single morality.\nFinally, and closely related to the third argument, Johnston explores the idea that bioethics can be seen as involving not wisdom, but the mastery of a technique. That is, the bioethicist would be an expert in helping people to develop their powers of moral reflection. For this argument, he suggested the example of a bioethicist saying that prescribing drugs that have been tested on animals is something a doctor needs to think deeply about, but that prescribing drugs produced from genetically modified animals is not (JOHNSTON, 2001, p. 157). Again, Johnston rejects this kind of 56 DALL'AGNOL, D.\nBefore assessing Johnston's arguments, I would like to point out that he in fact believes that there is a modest place in our society for a bioethicist (obviously, not someone claiming moral expertise) in some institutions including those which debate medical ethical issues. The two main tasks would be: (i) contributing to the debate by arguing in favor of a particular moral approach or by criticizing other approaches, and (ii) recording what the debate is all about and the different views that are being put forward (JOHNSTON, 2001, p. 159). These functions are restricted because our society has no single or dominant moral code-so no one can be seen as having the role of solving difficult moral issues.\nTo finish this section, I would like to say that the two limited tasks Johnston reserved for bioethicists (a role that anyone could really perform without any expertise!) contrasts with what scientists expect from philosophy and with the birth of bioethics as a discipline. It is worth recalling that one of the first authors to use the word 'bioethics' explicitly related it to wisdom (von POTTER, 1971, p. 1). The American oncologist held that we need a new kind of wisdom, namely, \"'the knowledge of how to use knowledge' for man's survival and for the improvement in the quality of life.\" Potter's main idea was -in a context were life itself was in danger of disappearing because of pollution, nuclear threat, and so on-to reunite facts and values: the knowledge of sciences and the values of the humanities. There is no doubt that a doctor needs scientific training, but neither is there much doubt that she needs education in the humanities, including in philosophy. The question then turns out to be: what kind of contribution can philosophy make to helping bioethics achieve its aims? I believe Wittgenstein can help us here.\n\nBack to Wittgenstein\nIn this section, I would like to return to Wittgenstein in order to briefly discuss some of Johnston's (mis)appropriation of his ethics both from the Tractatus and the Philosophical Investigations. The intention is to show that a real Wittgensteinian approach to philosophy can help bioethics to reach its aims by supporting people to take wise First, I will discuss the ethical sense of Wittgenstein's first book -and perhaps of his entire work. The positivistic reading was that the Tractatus allowed only for meaningful discourse in the natural sciences by showing that all the rest was nonsensical. As I have argued elsewhere (XXX), this is mistaken since Wittgenstein indeed held that there are no propositions in ethics (…keine Sätze der Ethik …, 6.42), but not that moral judgements are unspeakable. Understanding the difference is of paramount importance. Thus, consider metaphysical attempts to ground ethics either by postulating some kind of deity (and holding some kind of divine command metaethical theory) or by holding a naturalistic approach to 'good,' 'right,' and so on (and trying to reduce their normative meanings to the descriptive ones of the natural sciences).\nAccording to Wittgenstein's Tractatus, both are trying to say more than can be said.\nIt is exactly against these metaphysical attempts to ground morality that Wittgenstein directs the last remark of the Tractatus. It does not follow, however, that ordinary moral judgements cannot be uttered. To illustrate this point, it is sufficient to give an example from the Lecture of Ethics. Johnston (strangely enough) is well aware of this text, and indeed the lecture presents similarities with some remarks in the Tractatus. I quote Wittgenstein (1965, p. 5\n\n):\nSupposing that I could play tennis and one of you saw me playing and said \"Well, you play pretty badly\" and suppose I answered \"I know, I'm playing pretty badly but I don't want to play any better,\" all the other man could say would be \"Ah, then that's all right.\" But suppose I had told one of you a preposterous lie and he came up to me and said, \"You're behaving like a beast\" and then I were to say \"I know I behave badly, but then I don't want to behave any better,\" could he then say \"Ah, then that's all right\"? Certainly not; he would say \"Well, you ought to want to behave better.\" Here you have an absolute judgment of value, whereas the first instance was one of relative judgment.\nThe same view clearly appears in the Tractatus: a moral judgment of the form \"thou shalt … [e.g., not lie]\" is not a proposition in the technical sense. Rather, it is an absolute judgment of value showing that there must be some kind of ethical reward and punishment and that \"they must reside in the action itself\" (… in der Handlung selbst liegen, 6.422). Ethical judgements that try to ground morality metaphysically go against the limits of sense (they are unsinnig), but day to day moral judgments are simply categorical ones (sinnloss only in the way of logical tautologies) showing what to 58 DALL'AGNOL, D.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 51-67, out./dez. 2022 do. Consequently, from the fact that there is no metaphysical foundation for ethics, it does not follow that in our daily lives we cannot express a moral judgement.\nIf this interpretation is right, then Johnston's insistence that there is no way of solving conflicting moral claims since judgments of human action simply reflect the different dispositions or preferences of the people who make them is mistaken. It conflates, using Wittgenstein's distinction, judgements of relative value with judgements of absolute value. As Wittgenstein illustrates in his Lecture, if a person lies and is reprehended, she \"certainly\" (to stress Wittgenstein's original word) cannot just say: \"I know I behave badly, but then I don't want to behave any better.\" Therefore, there are no substantial, deep moral agreements but only disagreements about whether one should play tennis well or not. To practice a specific sport or to subscribe a particular metaphysics is a matter of taste, not morality: no one may choose to invent a preposterous lie about another person. According to Wittgenstein, then, metaphysics is ungrounded. Thus, if a person justifies her moral judgements by appealing to a deity or to nature, she is trying to say more than needs to be spoken since morality only shows itself. I will explore the bioethical implications of this agnostic stance in metaphysics below, arguing that it provides sound reasons to respect the bioethical principle of respect for autonomy.\nBefore doing that, I would like to better specify the ethical sense of the Tractatus. There is, as Laraway showed (1992), a close similarity between a Socratic approach to wisdom and Wittgenstein's late philosophy. I believe, however, that we can also read Socratically the Tractatus' main (ethical) conclusion \"what we cannot speak about we must pass over in silence\" as a moral imperative not to say more than we know. To illustrate this, it is sufficient to recall Socrates' conclusion regarding his own wisdom: \"I am wiser than this man; it is likely that neither of us knows anything worthwhile, but he thinks he knows something when he does not, whereas when I do not know, neither do I think I know, so I am likely to be wiser than he to this small extent, that I do not think I know what I do not know.\" (PLATO,Apology,21d) According to Socrates, there are things he knows well and there are things he does not know; to be wise is to recognize the difference between them and to be honest about it. Thus, for both Socrates and Wittgenstein, it is a moral imperative \"to say no more than we know.\" As the author of the Das blaue Buch wrote: \"In der Philosophie liegt die Schewierigkeit darin, nich mehr zu sagen, als was wir wissen.\" (WITTGENSTEIN, 1989, p. 75) This (to say no more than we know) shows that wisdom has an irreducible ethical ingredient as well as an epistemic one.\nOne bioethical implication I would like to draw here-which relates to another of Johnston's confusions, this time regarding modern moral philosophy-concerns the special status of the principle of respect for autonomy. It is exactly because many metaphysical attempts to ground morality exist (and all fail) that we must assume a common principle: to respect a person's choice of whether or not to participate in scientific research, or to refuse an experimental medical treatment to keep oneself alive at all costs, and so on. Since metaphysics is irrelevant to ethics, it is not only logic that must, according to Wittgenstein, take care of itself (\"Die Logik muss für sich selber sorgen.\"), but ethics too. That is, ethics is an autonomous field of knowledge.\nAs Johnston correctly saw, ethics cannot be derived from logic, empirical sciences or metaphysics, but that does not matter since values show themselves. This implies that respecting a person's decision is the only ethical way of pursing scientific and technological knowledge. Recognizing this point is a piece of philosophical wisdom, which leads to an important consequence for understanding that a bioethicist also has expertise: she knows what can be scientifically known and what cannot and, therefore, it must be left to the persons involved to decide. Consequently, there is no longer any place for an autocratic and paternalistic view in modern bioethics.\nThe challenge, then, is to understand that despite metaphysics (and indeed because of metaphysics' failure), we are still capable of justifying common (bio)ethical principles. There is nothing paradoxical in denying knowledge in metaphysics while also arguing that there is moral knowing-how (e.g., acting virtuously). In this sense, Laraway (1992) is right in pointing to a Socratic and Wittgensteinian conception of the moral life as a kind of techne.\nThis point requires further exploration. In his paper \"Wittgenstein's Method and Socrates's Craft: The Moral Life as a Techne,\" Laraway writes (1992, p. 6): Without offering an extended defense of the claim, I would like to suggest that elenchus activity is a sort of craft for Socrates, and insofar as he possesses this sort of practical knowledge, we may take him to be something of a \"craftsman of virtue.\" By knowing how, through his execution of the elenchus to remind himself and his interlocutors of the limits of 60 DALL'AGNOL, D.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 51-67, out./dez. 2022 theirs own knowledge, he is continually able to reclaim the only piece of knowledge that he is willing to explicitly profess, \"human\" wisdom (Ap,, which is precisely the recognition of one's own ignorance. Laraway then goes on to show that Socrates' elenchus is a meta-ethical cognitivist approach based on knowing how or being capable of showing how to be moral. I would like to add that having moral expertise is knowing-how to act (e.g., how to take care of a patient respectfully). This, however, shows that Johnston is just mistaken in using Wittgenstein's late philosophy as if the latter held a non-cognitivist perspective.\nThe clearest evidence is that Wittgenstein has some practical aims his work; that is, he wants to get rid of metaphysical confusions to reach a \"sound human understanding.\" Therefore, his method in the Philosophical Investigations is a kind of Socratic' elenchus which aims at providing a therapy with an ethical aim, namely knowing how to live better. This shows, again, that Johnston' non-cognitivist approach is mistaken and that Wittgenstein's late work can be reconstructed in cognitivist (and universalist) meta-ethical terms.\nI would here like to develop further this interpretation in order to later extract some bioethical implications. Wittgenstein's notion of 'form-of-life' (Lebensform) may seem ambiguous, but it receives-in the first part the Philosophical Investigations-a meaning very close to that of 'species,' when he mentions dogs, lions, humans, and so on. Now, the predominant use is the one related to humans. As he points out in paragraph 241: \"it is what human beings (Menschen) say that is true and false; and they agree in the language they use. That is not agreement in opinions but in form of life (Lebensform).\" Humans have propositional language; other animals may have a kind of Augustinian language-game to communicate with each other, but if a lion could speak, we would not understand him because we do not share his form-of-life.\nWe can now recognize why Johnston's non-cognitivist reading of Wittgenstein's late philosophy is ungrounded. It is not only the case that we can, based upon the notion of Lebensform, speak of \"the common behavior of mankind\" (Die gemeinsame menschliche Handlungsweise… § 206), but we can also point to some moral behaviors that nobody would deny. For instance, we can state that caring is essential in medical contexts, that pursing the truth (e.g., by not falsifying data) is paramount in scientific research, that patients and doctors must respect one other, and so on. The common behavior of mankind is a reasonable ground for the objectivity of moral judgements. More specially, however, it opens the door to arguing for a special kind of cognitivist approach to morality. To give another trivial example: any normal human being would feel pain if subjected to surgery without anesthetic.\nConsequently, doctors must know the proper means not to let patients suffer, but they must also know-how not only to relieve pain and, more importantly in moral terms, to know-how to take care of their patients. To do otherwise is to infringe the most basic principles of biomedical ethics.\nAlthough it is not the goal of this paper to enter into meta-ethical discussions, it is worth quoting a passage from the Philosophical Investigations to illustrate the above point. In paragraph 151, Wittgenstein wrote: \"The grammar of the word 'knows' is evidently closely related to that of 'can', 'is able to'. But also closely related to that of 'understands'. ('Mastery' of a technique.)\" I have developed this idea by showing that moral knowledge involves not only knowing-that, for instance, cancer is a disease that requires treatment, but also knowing-how to do it: first by examining the patient, then treating the disease or performing a surgery, and above all, knowing-how to take care of the patient all the way though, for instance, respectfully. I have called this approach Practical Cognitivism. Unfortunately, I cannot pursue this approach here (see: XXX for further details), but it fits well with Laraway's characterization of Socrates' views.\nWe can, then, conclude that the late Wittgenstein gave us valuable philosophical methods, including conceptual analysis, which can be applied in bioethics. In the next section, I will illustrate this by means of some real applications of how philosophical wisdom may inform bioethical expertise.\n\nCan Philosophy help Bioethics?\nPracticing bioethicists (in hospital committees, institutional review boards Senior's \"early treatment\" etc.). Bioethics was born to make them accountable.\nConsequently, Johnston's optimism regarding scientists' morality and his pessimism concerning philosophers' contributions seem to be based on bias.\nAfter the publication of the Belmont Report, Beauchamp and his colleague Childress wrote the book that is now the classical approach to biomedical ethics. In their book, they proposed the principles of respect for autonomy, non-maleficence, beneficence, and justice-which are now the basis of Brazilian legislation in the area (Res. 466/12). Since the concept of 'person' may have a metaphysical assumption, they prefer to use respect for the agent's autonomous choices. This work made significant contributions to bioethics, some of which will be illustrated below. As we can see, philosophers may help to format the basic principles for specific subfields of ethics, not only bioethics, but also ecoethics, zooethics, roboethics, and so on.\n\nFinal Remarks\nWittgenstein would not, pace Johnston's views, prevent philosophers concerned with bioethical issues from sitting on a committee (including IBRs) that aims to contribute to reaching wise decisions. On the contrary, he would be sympathetic to such a practical engagement. Of course, bioethicists with a philosophical background must also learn legislation; they must learn to see things from an interdisciplinary perspective; and so on. But they can make significant contributions in defending objective principles such as respect for autonomy by explaining that the failure of metaphysics requires equal respect among all humans (doctors and patients alike), that what counts as benefits/harms needs to take into consideration the person's will, and therefore, that a doctor or scientist must not act in a paternalistic or autocratic way.\n\"Wittgensteinians\" sometimes turn against philosophy and hold a pessimistic stance, but Wittgenstein was only against metaphysical views (e.g., those involving category mistakes, conceptual confusions, etc.). He reserved a positive therapeutic task for real philosophers with clear ethical aims-as he himself was and had. He even built up a conceptual framework (e.g., language-games, family-resemblances, etc.) to criticize the metaphysician's errors (e.g., the assumption of private language), and he gave us a method requiring particular skills so we could continue philosophizing on many topics, including bioethical issues.\nTo finish this paper, without defending a conclusive view, I would like to point out that between the Tractatus and the Investigations there is a kind of Socratic continuity. That is, not only can the last remark of the first book can be read as showing that one must not say more than one knows, but also the later work can be seen as an example of epistemic humility in showing that one must not pretend to know something one does not know. This piece of philosophical wisdom needs to be"
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 128455,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254854025"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications\n\nScholarly text is often laden with jargon, or specialized language that divides disciplines. We extend past work that characterizes science at the level of word types, by using BERT-based word sense induction to find additional words that are widespread but overloaded with different uses across fields. We define scholarly jargon as discipline-specific word types and senses, and estimate its prevalence across hundreds of fields using interpretable, information-theoretic metrics. We demonstrate the utility of our approach for science of science and computational sociolinguistics by highlighting two key social implications. First, we measure audience design, and find that most fields reduce jargon when publishing in general-purpose journals, but some do so more than others. Second, though jargon has varying correlation with articles' citation rates within fields, it nearly always impedes interdisciplinary impact. Broadly, our measurements can inform ways in which language could be revised to serve as a bridge rather than a barrier in science.\n\n\nIntroduction\nSpecialized terminology, or jargon, naturally evolves in communities as members communicate to convey meaning succinctly. It is especially prevalent in scholarly writing, where researchers use a rich repertoire of lexical choices to communicate. However, niche vocabularies can become a barrier between fields (Vilhena et al., 2014;Martínez and Mammola, 2021;Freeling et al., 2019), and between scientists and the general public (Liu et al., 2022;August et al., 2020a;Cervetti et al., 2015;Freeling et al., 2021). Identifying scholarly jargon is an initial step for designing resources and tools that can increase the readability and reach of science (August et al., 2022a;Plavén-Sigray et al., 2017;Rakedzon et al., 2017).\nResearch on scholarly language typically focuses on the relative prevalence of word types ... the I-V characteristics are asymmetric with respect to zero bias as in a junction diode ... ... work with not-for-profit organisations are free to choose the best medical therapy, with no economic bias. Figure 1: Examples of two abstract excerpts from papers showing two different senses of bias. The first describes electronic currents (Satishkumar et al., 2000), and the second refers to a social phenomenon (Wizemann, 2000). (McKeown et al., 2016;Prabhakaran et al., 2016;Sim et al., 2012;Rakedzon et al., 2017). However, the same word can be overloaded with multiple meanings, such as bias referring to electric currents and social perceptions ( Figure 1). We use BERT-based word sense induction to disentangle these, and show that discipline-specific senses can be as specialized as unique word types. Thus, we operationalize scholarly \"jargon\" as both disciplinespecific words and senses. We measure jargon in English abstracts across three hundred fields of study, drawn from one of the largest datasets of scholarly documents: the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020).\nOur findings are valuable for several groups that partake in science: readers, authors, and science of science researchers.\nDue to scholarly language's gatekeeping effect, some areas of NLP has developed tools to support readers, such as methods for simplifying or defining terminology (Kim et al., 2016;Vadapalli et al., 2018;August et al., 2022a;Head et al., 2021;August et al., 2022b;Murthy et al., 2022). When deciding what constitutes jargon, studies may rely on vocabulary lists based on word frequency, often collapsing all of science into one homogeneous language variety (August et al., 2020b;Rakedzon et al., 2017;Plavén-Sigray et al., 2017). Our ap-proach identifies language associated with individual subfields and proposes a bottom-up, data-driven process for creating these vocabularies ( §3 and 4).\nSecondly, measuring levels of discipline-specific language in abstracts can inform authors who wish to communicate ideas to a wider audience or enter a new field. Our analysis of language norms across disciplines at scale shows that while some subfields use highly specialized word types, others use highly specialized senses ( §5). In addition, we provide evidence for audience design in scholarly discourse ( §6.1), following a sociolinguistic framework that describes how people adjust language depending on the scope of their audience (Bell, 1984).\nFinally, our language-centered approach contrasts the typical paradigm in science of science research, where citation behavior often defines relationships among articles, venues, and fields (e.g. Boyack et al., 2005;Rosvall and Bergstrom, 2008;Peng et al., 2021). Citation count is a common measurement of \"success\", and the mechanisms behind it form a core research area (Wang and Barabási, 2021;Foster et al., 2015;Fortunato et al., 2018). On the other hand, interdisciplinarity is increasingly valued, but does not always lead to short-term citation gains (Van Noorden, 2015;Larivière and Gingras, 2010;Okamura, 2019;Chen et al., 2022). We run regression analyses to examine the relationship between discipline-specific senses and types and these two distinct measures of success ( §6.2).\nTo summarize, we contribute the following: • Methods. We propose a new measure of scholarly jargon by using information-theoretic metrics and BERT-based word sense induction to identify discipline-specific word types and senses ( §3). We validate our approach for measuring senses by showing it recalls more overloaded words in Wiktionary compared to word types alone ( Figure 3).\n• Social implications. We illustrate the utility of our measurements of jargon for computational social science by analyzing audience design and articles' success ( §6). We find that some fields reduce jargon more than others when authors publish in general-purpose journals ( Figure 5), and that jargon has varying relationship with citations, but nearly always a negative relationship with interdisciplinary impact (Table 4).\nWe hope computational social scientists use our measure of scholarly jargon to better quantify language barriers in science and their implications. We plan to release our code and scored lists of words and senses for each subfield at https://github.com/lucy3/words_as_gatekeepers.\n\nData\nOur work involves several datasets: scholarly abstracts, Wikipedia, and Wiktionary. We use abstracts to calculate the association of words with disciplines and Wikipedia to supplement our calculation of background word probabilities. Later, in §4, we introduce and describe how we use Wiktionary to validate our approach.\n\nContemporary S2ORC\nOur dataset of academic articles, CONTEMPORARY S2ORC, contains 12.0 million abstracts and 2.0 billion words 1 that span a mix of scholarly fields ( Figure 2, Appendix A).\nTo create CONTEMPORARY S2ORC, we draw from the July 2020 release of S2ORC (Lo et al., 2020). S2ORC is a general purpose corpus that contains metadata for 136 million scholarly articles, including 380.5 million citation links (Lo et al., 2020). These articles originate from Semantic Scholar, which obtains data directly from publishers, the Microsoft Academic Graph (MAG), arXiv, PubMed, and the open internet. Metadata, such as titles, authors, publication years, journals/venues, and abstracts are extracted from PDFs and LaTeX sources or provided by the publisher. Though extensive, S2ORC contains some amount of noisy or missing metadata. We remove non-English articles and those with missing metadata, consolidate journals and venues into a single venue label, and limit the dataset to the years 2000-2019 (Appendix B).\nS2ORC links articles to paper IDs in the Microsoft Academic Graph (MAG) (Sinha et al., 2015;Wang et al., 2019), so we match S2ORC abstracts to MAG fields of study (FOS). S2ORC originally contains top-level MAG FOS (level 0), e.g. biology, but we also join abstracts with second level MAG FOS (level 1), e.g. immunology, for more granularity. 2 In this present paper, we refer to level 0 FOS as fields, and level 1 FOS as subfields. We take an approximately uniform sample of 50k articles per subfield, resulting in a total of 293 subfields that fall under 19 fields (Appendix A).\n\nWikipedia\nWe include Wikipedia article content to counterbalance CONTEMPORARY S2ORC's STEM-heavy focus for our estimation of words' typical prevalence. Wikipedia is a popular information-gathering resource (Reagle and Koerner, 2020), and we use an Oct 1, 2022 dump of its articles. It offers complementary topical coverage that is collectively curated and driven by public interest, and includes biographies, culture, and arts (Mesgari et al., 2015). We remove Wiki formatting using Attardi (2015)'s text extractor, and discard all lines, or paragraphs, that are less than 10 white-spaced tokens long. We sample twice as many Wikipedia paragraphs as the number of CONTEMPORARY S2ORC abstracts, so that each is similar in size despite differences in document length. In total our Wikipedia dataset, WIKISAMPLE, contains 24.0 million paragraphs and 1.5 billion tokens.\n\nMethods\nLanguage differences among subsets of data can be measured to a variety of approaches, from geometric to information theoretic (Ramesh Kashyap et al., 2021;Vilhena et al., 2014;Aharoni and Goldberg, 2020). We calculate the association of a word's type or sense to subfields using normalized pointwise mutual information (NPMI). We choose NPMI over similar metrics (e.g. tf-idf, divergence, z-score) because of the nature of language difference it emphasizes: higher NPMI scores reflect lan-guage that is not only commonly used in a community, but also highly specific to it (Lucy and Bamman, 2021;Gardner et al., 2021). NPMI offers an interpretable metric of association, where a score of 1 indicates perfect association, 0 indicates independence, and -1 indicates no association. We follow Lucy and Bamman (2021)'s framework of calculating NPMI separately for word types and senses, which they originally used to identify communityspecific language on social media. We update their approach with a more recent word sense induction (WSI) method, and use a different interpretation of type and sense NPMI scores.\n\nDomain-specific words\nWe calculate NPMI for word types, or type NPMI, as the following measure:: Here, P (t | f ) is the probability of a word t occurring given a set of abstracts f in a field, P (t, f ) is their joint probability, and P (t) is the probability of the word overall (Lucy and Bamman, 2021;Zhang et al., 2017). \"Overall\" refers to the combined background dataset of CONTEMPORARY S2ORC and WIKISAMPLE. We only calculate T f (t) for words that appear at least 20 times in each field. 3 As illustrative examples of outputs, Table 1 shows words with the highest T f (t) in several fields.\n\nDomain-specific senses\nWidely disseminated words can be overloaded with domain-specific meanings or use. For example, bias could refer to a type of voltage applied to an electronic system, social prejudice, or statistical misestimation. Thus, we include word senses as a complement to word types for characterizing domain-specific language. We use senses to refer to different meanings or uses of the same word induced by word sense induction (WSI).\n\nWord sense induction\nTo partition occurrences of words into senses, we adapt Eyal et al. (2022)'s WSI pipeline with minimal modifications. WSI is an unsupervised task NLP Chemical Engineering Immunology Communication International Trade Epistemology  Table 1: Top five words that are highly specialized to different disciplines. These have the highest type NPMI (T f (t)) scores in their respective subfields. As examples, treg in immunology stands for \"regulatory T cells\", and antidumping in international trade places high taxes on imports.\nwhere occurrences of words are split into senses.\nIn this pipeline, a language model is used to predict substitutes for a word, and these substitutes are clustered to represent different senses of the target word. Eyal et al. (2022)'s approach is designed for large-scale datasets, and their process follows roughly two steps: one where a sample of occurrences is used to induce senses, and another where remaining occurrences are assigned to senses. We carry out their method on a case-insensitive target vocabulary of 6,497 common and \"widely used\" words, those that appear in the top 98th percentile by frequency and in at least 50% of venues, and does not include stopwords and words split into wordpieces. 4 We lemmatize and lowercase target words and substitutes, following Eyal et al. (2022)'s implementation, because otherwise the most common substitutes representing a sense may be different lemmas of the same word. This processing step reduces the target vocabulary into 4,407 lemmas.\nWe sample 1000 instances of each vocabulary lemma, and use ScholarBERT to predict each instance's top 5 substitutes (Hong et al., 2022). 5 We truncate each abstract to this model's maximum input length. Then, a target lemma's predicted substitutes are clustered into senses using Louvain community detection (Blondel et al., 2008). Eyal et al. (2022) uses a few heuristics for determining clusters of representatives that are big enough to recognize as senses: each cluster needs to have at least two substitutes, and the second most frequent substitute needs to appear at least 10 times in training data. If no clusters are big enough, we add a fallback case, where we place all occurrences of a word to a single sense. 4 We avoid wordpieces since Eyal et al. (2022)'s pipeline masks and predicts words at the token-level. 5 We pick ScholarBERT over similar transformer models trained on scholarly language (e.g. SciBERT), because it is trained on a wider breadth of disciplines, splits fewer potential vocab words into wordpieces (15 versus 193), and uses RoBERTa-style training (Hong et al., 2022;Beltagy et al., 2019). Eyal et al. (2022) assigns remaining occurrences of the target word type to senses based on their substitutes' Jaccard similarity with pre-computed clusters. We also add a fallback case here: if the Jaccard similarity of a remaining occurrence with all clusters is zero, we assign that occurrence to an extra cluster representing previously unseen senses.\n\nSense NPMI\nOnce each occurrence of a widely-used word is labeled with a sense, their frequencies can be used to calculate sense NPMI. Sense NPMI uses the same formula as type NPMI, except it is calculated at the sense-level rather than the word-level (Lucy and Bamman, 2021). That is, counts of a word t are replace with counts of a sense, t i : 4 Validation\n\nWiktionary\nWe perform in-domain validation of the unsupervised sense pipeline using Wiktionary. Words marked as associated with a subfield in this online dictionary should also be highly scored by our metrics. Wiktionary is collaboratively maintained and includes common words listed with definitions that may be labeled as having \"restricted usage\" to a topic or context. 6 For example, the word ensemble has the labels machine learning, fashion, and music. 7 We map Wiktionary labels in English definitions of target words using exact string matching to fields and subfields. If an NPMI score threshold were used to determine whether a token should be considered discipline-specific or not, we expect sense NPMI to recall more words labeled by Wikitionary than type NPMI does. We do not calculate precision, because Wiktionary is not necessarily comprehensive for all subfields.\nWe obtain Wiktionary entries for 94.94% of the common, widely used words that were inputs in the WSI pipeline. We filter out words where all definitions are labeled with only one field, and allow subfields to inherit the words labeled with their parent field. In total, we have 11,548 vocabulary word and subfield pairs to recall across 83 subfields. Since recall is calculated at the word-level and sense NPMI is at the sense-level, we use a word t's most frequent sense t i 's S f (t i ) in a subfield to represent word-level sense NPMI S f (t).\nIn Eyal et al. (2022)'s WSI pipeline, the resolution parameter γ in Louvain community detection calibrates the number of senses induced per word. Increasing resolution leads to more fine-grained word senses and higher recall, but potentially spurious senses ( Figure 3). Rather than using Eyal et al. (2022)'s default resolution value of 1, we use a dynamic formula for resolution (Newman, 2016): where ω in is the probability of an edge between two nodes in the same community, and ω out is the probability of an edge between two nodes in different communities. This formula follows the intuition that nodes within communities should be more connected than nodes between them. To calculate dynamic resolution, we follow Newman (2016)'s algorithm, initializing γ = 1 and iterating for each target lemma at most 10 times. In each iteration, we run Louvain community detection and recalculate γ using the edge probabilities in the current clustering. We stop early if γ converges within 0.01 of its previous value. Sense NPMI with dynamic resolution recalls more discipline-specific Wiktionary words than type NPMI at the same score cutoff (Figure 3). In addition, the sense NPMI of a word in a subfield labeled by Wiktionary is higher than the score of the same word in a random field (paired t-test, p < 0.001, Appendix C).\n\nExamples and interpretation\nExamples of semantically overloaded words between fields can also qualitatively validate our results (Table 2). Returning back to the example introduced at the beginning, bias is indeed very overloaded. It has distinct senses with high NPMI (> 0.2) across multiple fields, including NPMI metric AUC, recall Figure 3: Recall and area under the curve (AUC) of 11,548 Wiktionary words with discipline-specific definitions. Higher resolution (γ), which leads to more senses per word, increases recall, and sense NPMI recalls more semantically overloaded words than type NPMI at the same score threshold. statistics (skew), 8 optoelectronics (charge), cognitive psychology (preference), and climatology (error). These examples suggest that future work could examine how our approach could provide potential candidates for updating dictionaries or glossaries. Table 3 shows examples of words whose scores increase from type NPMI to sense NPMI despite having counts split across senses. Lucy and Bamman (2021) interpret sense and type NPMI similarly in their downstream analyses, based on the magnitude of their values, but this does not account for how type and sense NPMI scores are related. In the boundary case where a word t only has a single sense t 0 , S f (t 0 ) = T f (t). This leads to a strong correlation between the two metrics, especially when a sense scored as highly associated with a field is also the dominant sense of that word in general. Thus, to narrow in on what we gain from WSI, we examine not only senses that are highly associated with a field, but have sense NPMI scores higher than their words' type NPMI scores (Table 3). Therefore, we count a token with a labeled sense as a discipline-specific\n\nLanguage norms across fields\nThe linguistic insularity of science varies across fields. For example, Vilhena et al. (2014) found that phrase-level jargon separates biological sciences more so than behavioral and social sciences. In addition, articles written by social scientists are sense t1    Table 3: Top five words that have senses associated with each field (S f (t) > 0.1), ordered by the difference ∆ between word-level sense and type NPMI. These are words that are highly specific to subfields based on their sense, rather than their type. more accessible to biological scientists than vice versa (Vilhena et al., 2014). We extend these science of science analyses with the novel addition of word senses.\nTo summarize the distinctiveness of word types in a field, we calculate the mean type NPMI score of unique words in a field. Before taking the mean, however, we adjust scores by zeroing negative values, since we are more interested in words associated with a field rather than those that are not. This zeroing practice is typically used in studies where PMI measures word relatedness (Levy et al., 2015;Dagan et al., 1993;Bullinaria and Levy, 2007).\nSimilar to Vilhena et al. (2014), we also find that fields such as biology and medicine have very distinctive word types (Figure 4). However, there is a considerable amount of overlap in word type distinctiveness across fields. Similar to how natural sciences name molecules and chemicals, the arts and humanities name canons of writers, philosophers, and artists.\nWe also examine what fields gain the most in NPMI scores when common words are broken into their senses. We recalculate subfields' average adjusted NPMI, but use max(T f (t), S f (t)) instead of T f (t) for words that have induced senses. Based on their relative increases in average adjusted NPMI, subfields in math/technology, physics, and economics often use common words in specialized Figure 4: The left subplot shows the distinctiveness of subfields' word types, while the right shows the increase in distinctiveness when we take the max of words' type and sense NPMI instead of only their type NPMI. Each point is a subfield, such as organic chemistry in chemistry, and fields are colored using larger disciplinary categories for interpretation clarity. contexts (Table 3, Figure 4). There is no significant correlation across subfields between the distinctiveness of their word types and that of their senses. Thus, identifying senses provides a very different perspective on language norms in some fields and suggests an additional route through which gatekeeping may occur.\n\nSocial implications\nIn this section, we examine two social implications of our metrics: audience design and scholarly success. We limit these experiments to articles in CONTEMPORARY S2ORC that are published among 11,047 venues in the top 95th percentile by abstract count (at least 800 each in S2ORC), to ensure solid estimation of venue-level information, such as their disciplinary focus and average citations per article.\nFor example, on Twitter, when writers target smaller or more geographically proximate audiences, their use of nonstandard language increases (Pavalanathan and Eisenstein, 2015). Here, we investigate how broad and narrow audiences relate to authors' use of specialized language. We hypothesize that for abstracts that fall under the same subfield, ones published in general-purpose venues use less discipline-specific language than those published in discipline-focused ones.\nTo address this hypothesis, we first collect sets of 6 general-purpose and 149 discipline-specific venues. We use general-purpose venues that appear in both our dataset and Wikipedia's list of general and multidisciplinary journals: 9 Nature, Nature Communications, PLOS One, Science, Science Advances, and scholarly Reports. Discipline-focused venues are those where 80% of articles fall under a single subfield. We chose this percentage cutoff because these venues' titles often mentioned the subfield, e.g. Agronomy Journal (80% of titles). Among these two sets of venues, we examine abstracts labeled with only one subfield.\nWe then calculate the fraction of jargon over all words in each abstract, by counting tokens t that are either discipline-specific senses or types, where c = 0.1. In other words, we count t if max(T f (t), S f (t)) > 0.1 in the abstract's subfield f . We find that almost all fields adjust their rate of jargon based on audience ( Figure 5). Interestingly, medicine, physics, biology, and psychology are notable exceptions that have similarly high amounts of jargon in both sets of venues. This actually aligns with our hypothesis, because generalpurpose venues have a history of being led and dominated by biological sciences, and in some, by physical sciences as well (de Carli and Pereira, 2017;Koopman, 2011;Varmus et al., 2000). Thus, jargon-laden fields further from these areas adjust their writing the most when publishing in these venues.\nA limitation of this approach for quantifying the amount of jargon in an abstract is that it relies on choosing c. We also obtain similar results with c = 0.2 and justify our choice of c in Appendix D.1. An alternative perspective mimics how soon a reader may encounter highly specialized language in an abstract. In this approach, we calculate the maximum over an abstract's type or sense NPMI scores within the first m tokens of 9 https://en.wikipedia.org/wiki/List_of_scholarly_journals. Figure 5: Abstracts in the same field typically contain less jargon when they appear general-purpose venues (e.g. Nature) than when they are in discipline-focused ones (e.g. Genetics). Error bars are 95% CI. Figure 6: The expected maximum type and sense NPMI of tokens in abstracts, where the x-axis indicates token position and shaded areas are 95% CI. Computer science and engineering have wider gaps in scholarly jargon use between general-purpose and disciplinefocused venues than medicine and biology do. the abstract. These results provide another view of our previous finding: fields such as computer science and engineering adjust their language for general-purpose venues more so than those in the biological sciences ( Figure 6). This indicates that though most \"general-purpose\" venues intend to be for all of science, 10 some fields are expected to adapt their language more so than others.\n\nScholarly success\nWe hypothesize that jargon plays different roles in the success of a paper depending on how \"success\" is defined. In particular, since jargon gatekeeps outsiders from a discipline, we expect it to negatively affect interdisciplinary impact. To test this hypothesis, we run two sets of regressions to measure the relationship between abstracts' use of jargon and citation behavior within five years after publication. The first set of regressions predicts short-term citation counts, while the second predicts interdisciplinary impact. We run separate regression models for each field to compare heterogeneity across fields. Each unit of analysis is an abstract published in 2000-2014 labeled with only one or two subfields.\nTwo key independent variables are the fractions of discipline-specific words and senses in an abstract, with c = 0.1. For abstracts that have two subfields in the same analyzed parent field, we sum their type and sense jargon counts. Additional independent variables include time (three evenly-sized time bins within 2000-2014), length of abstract in tokens, number of authors, number of references in the article, number of subfields (one or two), and the venue's average citations per article.\nCitation count is an over-dispersed count variable, so we run a negative binomial regression to predict this outcome (Hilbe, 2011). We find that in some cases, the use of jargon has a significant positive relationship with citations, but the direction of this relationship differs across fields (Table 4, Appendix D.2).\nAlternatively, interdisciplinary impact considers the subfield composition of articles citing a target abstract. We use Leydesdorff et al. (2019)'s established formula, which they call DIV: where C is the set of subfields citing the abstract, n = |C|, N is the total number of subfields, and where v are subfields vectorized using overall cross-subfield citation counts (Appendix D.2). The first component measures the fraction of citing subfields, the second uses the Gini coefficient to calculate balance of citation counts among C, and the third incorporates subfield similarity (Leydesdorff et al., 2019;Chen et al., 2022;Stirling, 1998). We run ordinary least squares regression on abstracts that are cited by at least two subfields, with DIV as the dependent variable. Discipline-specific words and senses have a negative relationship across fields that have highly distinctive language norms (Table 4). Thus, though jargon has varying relationship with citation counts, it generally impedes the forging of interdisciplinary connections.\n\nRelated Work\nComputational sociolinguistics often examines language varieties and norms on social media (Nguyen  et Danescu-Niculescu-Mizil et al., 2013;Lucy and Bamman, 2021;Zhang et al., 2017;Nguyen and P. Rosé, 2011;Pavalanathan and Eisenstein, 2015). There has been less attention on situation-dependent language varieties, or registers, in scholarly communities (Agha, 2005). Here, language differences can indicate different factions of authors and disciplinary approaches (Ngai et al., 2018;West and Portenoy, 2016;Sim et al., 2012). Other NLP studies of science have predicted responses to articles (Yogatama et al., 2011), measured impact and innovation (Gerow et al., 2018;Hofstra et al., 2020;McKeown et al., 2016), and classified topics' rhetorical functions (Prabhakaran et al., 2016). A few studies have examined word meaning or use, such as semantic influence or novelty (Soni et al., 2021(Soni et al., , 2022 and semantic uncertainty (McMahan and Evans, 2018). Research on lexical ambiguity in science also appears in education, with an emphasis on how to improve the teaching of overloaded terminology (Ryan, 1985;Cervetti et al., 2015).\n\nConclusion\nWe use data-driven, interpretable methods to identify jargon, defined as discipline-specific word types and senses, across science at scale. By iden-tifying senses, we are able to recall more words labeled as associated with a field in Wiktionary than with word types alone. We then map language norms across subfields, showing that fields with distinctive word types differ from those with distinctive word senses. Finally, we find that some fields do not reduce their use of jargon when publishing in general-purpose venues, but this practice may inhibit interdisciplinary impact. This suggests a potential opportunity for the reconsideration of abstract writing norms, especially for venues that intend to bridge disciplines.   (FOS) in S2ORC, sorted from biggest to smallest, before subsampling. Medicine is the most frequent field, and is almost twice as large as the next largest field, which is biology. Figure 7 shows the number of valid abstracts in each top-level MAG field of study before subsampling a similar number of abstracts from each subfield. This figure can be compared to Figure 2 to show how the distribution of fields changed after sampling. The following lists the subfields, or level 1 MAG FOS, used in our study. The same subfield may fall under multiple fields.\n• Computer Science (32 children): natural language processing, software engineering, theoretical computer science, embedded system, computer security, programming language, data science, computer vision, computer network, human-computer interaction, world wide web, information retrieval, parallel computing, operating system, computer hardware, multimedia, computer graphics (images), library science, real-time computing, artificial intelligence, database, distributed computing, simulation, telecommunications, internet privacy, pattern recognition, machine learning, knowledge management, data mining, speech recognition, algorithm, computer science (other).\n\nB Dataset filtering\nWe perform the following preprocessing steps of S2ORC to create CONTEMPORARY S2ORC: • Venue. We consolidate the venue and journal keys of each article's metadata. We use whichever label is non-empty, and only a small fraction (0.08%) of articles with valid abstracts have venue and journal that differ, in which case we use use the article's journal. We handle venue names case insensitively, and also remove tokens in their names that contain numbers to consolidate years and editions. Figure 8: Words labeled as belonging to a subfield by Wiktionary have higher S f (t) in that subfield than in a random one (paired t-test, p < 0.001).\n• Time. Our study focuses on contemporary science, which are abstracts published during 2000-2019. S2ORC contains some abstracts from 2020 and onwards, but dates past 2020 are likely metadata processing errors. We remove 47.6 million articles outside of this time range.\n• Valid metadata. We remove 42.5 million articles with missing abstracts, titles, or journal and venue labels.\n• Language. We remove 77,133 articles from 925 non-English journals or venues, which are those that have less than 80% of their articles in English, using Lui and Baldwin (2012)'s language classifier.\n• Field of study. Medicine fields dominate S2ORC abstracts. We balance the dataset by taking a sample of 50k articles per subfield. For subfields that are too small to sample or articles that have field-level but no subfieldlevel labels, we categorize these in an OTHER subfield under their parent field. Since articles can be labeled with multiple FOS, our sample is not perfectly stratified, but prevents large subfields from dominating calculations of the general prevalence of words in English. In total we identify specialized language across 293 subfields that fall under 19 fields (listed in Appendix A). Figure 8 shows the distribution of sense NPMI scores for words in Wiktionary-labeled fields versus random ones. Figure 9: Abstracts' average fraction of disciplinespecific words and senses in fields that appear in both general-purpose and discipline-focused venues (95% CI). This plot uses an NPMI cutoff of 0.2. Figure 10: Sometimes, NPMI score distributions for subfields are bimodal with a second peak among positive values, especially when a subfield contains large amounts of jargon. The left shows the distribution for pure mathematics, while the right shows particle physics.\n\nD Additional experimental details D.1 Cutoff decision\nWe generated Figure 5 with additional values of the NPMI cutoff c, such as c = 0.2, and achieve similar conclusions (Figure 9). That is, these results are similar when it comes to which fields tend to adjust their language between general-purpose and discipline-focused venues. In the main text, we usually use c = 0.1, as positive NPMI values indicate association, but NPMI values too close to 0 would instead lean towards independence. Though NPMI ranges from -1 to 1, the outputted scores for various subfields tended to range from -0.5 to 0.5, and some include bimodal behavior where the latter peak of the distribution usually occurs after c = 0.2 ( Figure 10). We assume that this latter peak is indicative of jargon. Thus, we experimented with cutoffs that would separate the initial peak around 0 and a secondary peak in the positive NPMI value range, if any.\n\nD.2 Scholarly success D.2.1 Subfield similarity\nTo calculate subfield similarity, we first create a (N + 1) × (N + 1) citation matrix, where N is the total number of subfields, and the additional row and column represents papers in unknown subfields. Rows in this matrix represent subfields that are cited, and columns are citing subfields. This matrix is generated using all papers published in S2ORC within the years 2000 and 2019 that have inbound citations. For subfield similarity calculations, we use the rows to represent each subfield. For example, the nearest neighbors via cosine similarity of the row representing chemical engineering include polymer chemistry, polymer science, and inorganic chemistry.\n\nD.2.2 Regressions\nWe ran a few statistical tests to determine what regressions to use. Citation counts. We run both Poisson regressions and negative binomial regressions on citation count data, as these generalized linear models are typically used to model count data. Negative binomial regression is used for data that shows overdispersion, when the variance of the dependent variable exceeds the mean. We calculate the overdispersion ratio φ of Poisson regressions for each field: φ = Pearson's χ 2 residual degrees of freedom .\nSince it exceeds 1 for each field's regression, there is overdispersion in our data, and thus we use negative binomial regressions for citation counts. Negative binomial regressions require choosing a constant α which is used to express the variance in terms of the mean. We determine α by inputting the fitted rate vector from the Poisson regression into an auxiliary OLS regression without a constant (Cameron and Trivedi, 2013). The α we obtain from this for each regression is significant for all fields except for Art and Philosophy (p < 0.01, right-tailed t-test). Interdisciplinary impact. We run ordinary least squares (OLS) regressions for this dependent variable. OLS involves several assumptions: randomly sampled data, linearity, exogeneity, noncollinearity, and homoskedasticity. We check for linearity and exogeneity by comparing residuals and fitted values, non-collinearity by checking that the variance inflation factors of covariates do not exceed 5, and homoskedasticity by running a Breusch-Pagan test (Breusch and Pagan, 1979). We find that we satisfy all assumptions except homoskedasticity. Due to to this, we also run a weighted least squares regression to check the robustness of our OLS results, and achieve similar coefficients."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-19"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 130758,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254686058"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments\n\nThe spread of misinformation, propaganda, and flawed argumentation has been amplified in the Internet era. Given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. In this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. We adapt existing evaluation datasets for each stage of the evaluation. We devise three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. The methods are designed to combine language models with background knowledge and explainable mechanisms. Moreover, we address data sparsity with strategies for data augmentation and curriculum learning. Our three-stage framework natively consolidates prior datasets and methods from existing tasks, like propaganda detection, serving as an overarching evaluation testbed. We extensively evaluate these methods on our datasets, focusing on their robustness and explainability. Our results provide insight into the strengths and weaknesses of the methods on different components and fallacy classes, indicating that fallacy identification is a challenging task that may require specialized forms of reasoning to capture various classes. We share our open-source code and data on GitHub to support further work on logical fallacy identification.\n\n\nIntroduction\nThe purpose of constructing an argument is to prove conclusions that are in some way unknown or doubtful or that have been challenged and called into question [8]. A logical fallacy is a logical mistake in the reasoning used to transition from one proposition to the next, which results in a faulty argument [3]. Logical fallacies form a broad category of violations of argumentation norms, including structure, consistency, clarity, order, relevance, and completeness. Detecting whether an argument is fallacious and the corresponding actual violation, is in practice a subtle task. Detecting one or more fallacies in an argument, however, does not prove its conclusion to be false -they merely detect a flaw in the reasoning that attempted to prove that the conclusion is true.\nLogical fallacies have been of interest to social science since the early days of mathematics and philosophy [4]. More recently, the societal relevance of logical fallacies has been greatly amplified due to the wide adoption of the World Wide Web, which enabled a free exchange of large amounts of information, including an easy spread of misinformation [118,122,2] and propaganda [29,10,48]. Misinformation and propaganda are thorny issues for social media platforms on the Web, and have been increasingly addressed through the growing teams of moderators [41,84], and are under the scrutiny of different organizations and governmental bodies, such as the UN [62]. Similarly, the EU plans to ratify addressing misinformation as part of the Digital Services Act [27], as the spread of harmful and incorrect arguments can sway the population and lead to political shifts and civil unrests [65].\nConsidering the subtlety and the volume of the fallacious arguments, manually checking each by a human has become impossible. Moreover, the very subjective nature of the tasks tends to open room for disagreement on the classification when multiple annotators or moderators are involved. This motivates the need for automated methods that can quickly process an argument, understand its intent, and detect possible flaws in the reasoning. The algorithms need to be robust, i.e., work well for an argument in an open domain, and explainable, i.e., provide an explicit trace of their reasoning for human collaborators like social media moderators. Prior work on taxonomizing logical fallacies [26,4,8] and the initial efforts to develop logical fallacy benchmarks [60] has set the ground for comprehensive and trustworthy logical fallacy methods. However, as these works have been attempted in isolation, comprehensive methods and tasks are lacking.\nBuilding a comprehensive evaluation setup and methods for logical fallacy identification has several key challenges. First, while prior work has provided a list of taxonomies for organizing logical fallacies, it is unclear how they can be organized and aligned with existing benchmarks. Second, logical fallacies require an abstraction from syntax to highlevel semantics revolving around structure and soft logic. This makes pure language model-based methods insufficient for fully solving the task. Third, arguments rely heavily on background factual and commonsense knowledge. A robust and explainable method needs mechanisms to make implicit (assumed) knowledge in fallacies explicit. Fourth, given the large set of fallacies and the relatively small amount of annotated examples for supervised learning, data sparsity is a serious issue. To build robust and explainable methods, it is essential to devise scalable mechanisms that can combat data sparsity.\nIn this paper, we consider the research question: How can we build methods for robust and explainable identification of logical fallacies in natural language arguments? We consolidate prior work on taxonomizing logical fallacies into a three-stage framework of logical fallacy identification tasks, ranging from deciding whether there is a logical fallacy in an argument (logical fallacy detection), performing classification in high-level classes (coarse-grained classification), and finally performing classification into a wider range of specific classes (fine-grained classification). To deal with the need for abstraction and to fill knowledge gaps, we experiment with three families of methods: prototypebased reasoning, instance-based reasoning, and knowledge injection. We combat data sparsity through suitable methods for data augmentation and curriculum learning.\nThe contributions of this paper are as follows: 1. We design a three-stage framework of logical fallacy identification tasks, inspired by fallacy classification theories. We map and enhance existing datasets into this pipeline to provide a well-motivated and representative evaluation set.\n2. We adopt and devise a wide range of methods with a focus on robustness and explainability: prototypebased reasoning, instance-based reasoning, and knowledge injection. We complement these methods with strategies for distant learning from more data based on data augmentation and curriculum learning.\n3. We conduct an extensive evaluation of these methods on our datasets, focusing on their robustness and explainability. Our results provide insight into the strengths and weaknesses of the methods on different components and fallacy classes, indicating that fallacy identification is a challenging task that may require specialized forms of reasoning to capture various classes.\n\nOrganizing Logical Fallacies\nDefinitions and categorizations of logical fallacies have been proposed since antique Greek philosophers such as Aristotle [4]. Aristotle's Sophistical Refutations [4] and John Locke's An Essay Concerning Human Understanding [71] can be considered as the cornerstones of works done on logical fallacies, while others such as Copi [26], Barker [8], and Watts [119] have made notable contributions too. We elaborate on each of the aforementioned philosophical theories in Section 2.1. Then, in Section 2.2, we devise our logical fallacy framework that is rooted in these philosophical theories, and it formalizes them into three stages: fallacy detection, coarse-grained classification, and fine-grained classification. We describe the coarse-and the fine-grained classes that constitute our taxonomy of logical fallacies.\n\nExisting Theories of Categorization for Logical Fallacies\nIn general, there are two broad categories of fallacies: formal, involving the error in the logical structure of the argument, and informal, mostly concerned with the content of the argument or the latent error in their expression of logic [44]. In this study we focus on the informal fallacies, whose name derives from the fact that they are drifting away from the formal and symbolic language present in formal fallacies. While informal fallacies can be investigated from a linguistic point of view and detailed assessment of the language that is used creating an argument, formal fallacies are far simpler to consider from a computational approach and mostly rely on understanding the structure of the argument rather than language comprehension [45,123].\nAristotle [4] distinguishes several kinds of deductions (syllogisms) in [4]. In terms of broad categorization, he puts the fallacies into two groups: the ones dependent on language (In Dictione) and the ones not dependent on language (Extra Dictionem). His categorization revolves around the premises discussed in the deductions as well as conditions required for arguments to prove them correct. According to him, an argument satisfies three conditions, and \"is based on certain statements made in such a way as necessarily to cause the assertion of things other than those statements and as a result of those statements.\" Thus an argument may fail to be a syllogism in three different ways: (1) the premises may fail to necessitate the conclusion, (2) the conclusion may be the same as one of the premises, and (3) the conclusion may not be caused by (or grounded in) the premises. Aristotle's fallacies are primarily fallacious deductions that appear to be correct deductions. There are six classes of fallacies dependent on language: Equivocation, Amphiboly, Combination of Words, Division of Words, Accent, and Form of Expression. Additionally, there are seven kinds of logical fallacies (sophistical refutation in Aristotle's words) that can occur in the category of fallacies not dependent on language: Accident, Secundum Quid, Consequent, Non-Cause, Begging the Question, Ignoratio Elenchi and Many Questions. In summary, Aristotle classifies fallacies into thirteen classes.\nBarker [8] classifies logical fallacies based on the validity of the assumptions made when transitioning from premises to conclusions, as well as the validity of the premise and the conclusion themselves. Barker defines validity as follows.\nFirst, a valid argument would comprise premises that are all true. Second, it would not need the conclusions to satisfy their validity. And finally, its conclusions can be directly derived from the premises. This view is closely similar to Aristotle's, as well as the requirements that [86] have analogously proposed. Neglect of the third requirement gives rise to the fallacies of Non Sequitur that are fallacies that have an insufficient link between premises and conclusions. Neglect of the second requirement gives rise to fallacies of Petitio Principii in which \"the premises are related to the conclusion in such an intimate way that the speaker and his hearers could not have less reason to doubt the premises than they have to doubt the conclusion\". Neglect of the first requirement gives rise to the remaining category of fallacies in which premises are present that are not necessarily true all at once, even if the link between premises and conclusions is as rigorous as can be. In summary, identifying fallacious arguments would boil down to analyzing the validity and soundness of the claims as well as the sufficiency and necessity of the premise of arguments to satisfy the needs of the conclusion to be true. There are three levels of classification proposed in [8] that, on the finest level, would sum up to twenty classes of fallacies, although his categorization allows for more as well and he does not argue for a bounded definition or particular number.\nLocke [71] can be credited with the contribution of Ad-Arguments, which are arguments \"that men, in their reasoning with others, do ordinarily make use of to prevail on their assent; or at least so to awe them as to silence their opposition.\" Locke discusses three kinds of such arguments: Ad Verecundiam, Ad Ignorantiam, and Ad Hominem. According to him, these are not fallacies, but have been developed beyond his conception and have been named as such [47]. Ad Verecundiam, or Appeal to Authority is a fallacy when it is either on the ground that authorities (experts) are fallible or for the reason that appealing to authority is an abandonment of an individual's epistemic responsibility [51]. Ad Ignorantiam, or Appeal to Ignorance, happens when one demands \"the adversary to admit what they allege as a proof, or to assign a better.\" In other words, the Ad Ignorantiam fallacy happens when the argument claims a proposition to be true because there is no evidence against it. According to Locke, Ad Hominem was a way \"to press a man with consequences drawn from his own principles or concessions.\" That is, to argue that an opponent's view is inconsistent, logically or pragmatically, with other things he has said or to which he is committed to [51].\nCopi [26] defines fallacies as \"a form of argument that seems to be correct but which proves, upon examination, not to be so.\" Copi discusses both deductive invalidities and inductive weaknesses as sufficient reasons for arguments to be fallacious. From the eighteen informal fallacies he categorizes, eleven are borrowed from [4] and the other seven can be traced back to [71]. He  We conclude that the described categorizations [4,26,8,71] mostly agree on the definition of fallacious arguments as well as the broad categorizations of fallacies. The main difference lies in the fine-grained categorizations: [4] discusses the thirteen ways arguments can be fallacious, while [26] proposes eighteen different fallacy groups. [8] categorizes fallacies into twenty classes although he does not delineate the exact categorization or the number of classes, and all presumably borrow Ad Fallacies from [71]. These discrepancies require computational approaches for logical fallacy identification to choose between the proposed theories. For our experimental work, we adopt the broad categorization of [26], and the fine-grained classification by [52] and [60]. We describe our categorization further in Section 2.2.\n\nLogical Fallacy Framework\nWe design a three-stage framework ( Figure 1) as an overarching testbed for prior research on logical fallacies. The first stage of the logical fallacy detection aims to identify whether a logical statement contains a logical fallacy or not. The detection is formalized as a binary classification task to identify the arguments that are logically fallacious in any sense. If a fallacy has been detected, the goal of the second stage is to categorize the fallacy into one of a few broad classes (e.g., Fallacy of Relevance). In the third stage, the aim is to further classify a fallacy into a fine-grained class (e.g., Ad Populum).\nFollowing [26], we consider the following four coarsegrained classes: Fallacy of Relevance, Fallacy of Defective Induction, Fallacy of Presumption, and Fallacy of Ambiguity. Figure 1 shows the sub-categorizations we make from these coarse-grained classes to fine-grained classes described in [60]. To perform the mapping we use the definitions of fine-and coarse-grained classes given in [26]. We next describe our fallacies in detail.\nFallacy of Relevance occurs for arguments with premises that are logically irrelevant to the conclusion. Fallacy of Relevance subsumes the fine-grained classes Ad Hominem, Ad Populum, Appeal to Emotion, Fallacy of Extension, Intentional Fallacy. All of these fallacy classes present different means for using peripheral premises as support for claims. Ad Hominem contains sentences where an attack over the subject acts as a premise for the claim made in those sentences, while Appeal to Emotion involves manipulating the recipient's emotions to prove a claim. Ad Populum involves affirming claims based on popular belief, and Fallacy of Extension uses exaggeration for affirming claims based on the Figure 1: Three-stage taxonomy of logical fallacy identification. Coarse-grained classes are shown in boldface, while regular font is used to show fine-grained classes. We use solid and dotted boundaries to distinguish between fine-grained classes that we include and exclude in our experimental study, respectively.\n\nTable 1\nExamples for fallacious arguments belonging to different coarse-grained and finegrained classes covered in our work.\n\nFallacy of Relevance Ad Hominem\nBoris is not qualified to make suggestions about our penal system. As an ex-convict, he would always take the criminals' side.\n\nAd Populum\nAliens must exist because most people believe in them.\n\nAppeal to Emotion\nLuke didn't want to eat his vegetables, but his father told him to think about the poor, starving children in a third world country who don't have anything to eat.\n\nFallacy of Extension\nIf you don't drive a car, you hate the Earth.\n\nFallacy of Relevance\nI know you want to imprison me for having murdered my parents, but judge, have mercy on me, I'm an orphan! Intentional A woman decides to visit a certain doctor after only asking advice on the best doctors from ONE friend.\n\nFalse Causality\nThe temperature has dropped this morning, and I also have a headache. The cold weather must be causing my headache.\n\nFalse Dilemma\nSubscribe to our streaming services, or get stuck with cable! Faulty Generalization My friend said her Math class was hard, and the one I'm in is hard, too. All Math classes must be hard!\n\nFallacy of Credibility\nMy uncle is a mechanic and he says you shouldn't spank children. He says it's ineffective.\n\nFallacy of Logic\nEmployees are like nails. Just as nails must be hit in the head in order to make them work, so must employees.\n\nFallacy of Presumption Circular Reasoning\nQuinoa is a delicious, plant-based source of protein because it tastes so darn good.\n\nFallacy of Ambiguity Equivocation\nThe officer told me to freeze but it was too hot out to be freezing, so I was justified in running away.\ncorresponding sentences. Intentional Fallacy is directed towards using subconscious choices to incorrectly support an argument.\nWithin the broad class of Fallacy of Defective Induction, the premises seemingly provide ground for the conclusion but upon analysis prove to be insufficient and weak for supporting the claim made. Fallacy of Defective Induction is specified via five fine-grained categories, namely False Causality, False Dilemma, Faulty Generalization, Fallacy of Logic, and Fallacy of Credibility. Arguments that jump to a conclusion without implying a causal relationship between the premise and the claim fall under False Causality. If the specific causal relationship between the premise and the claim is generalized to a wider category of subjects, the argument is categorized as Faulty Generalization. Arguments that cast doubt regarding the credibility of the subject making a claim constitute for Fallacy of Credibility. When an argument presents a premise that erroneously limits the options available, it constitutes a False Dilemma. When the logical construct of the argument is inaccurate and misleading, it constitutes a Fallacy of Logic.\nFallacy of Presumption takes place when the inference to the conclusion depends mistakenly on unwarranted assumptions. Fallacy of Presumption includes the following fine-grained classes. Circular Reasoning occurs for arguments that come back to the beginning without proving themselves. Other classes that fall within Fallacy of Presumption are: Begging the Question, where the conclusion is treated like an assumption from the premise of the statement; Complex Question, where the argument is framed as a loaded question that intends to prove another latent unproved assumption; and Accident, where generalization is applied to specific cases that are out of scope.\nFallacy of Ambiguity occurs when words or phrases are used in an equivocal way, thus causing ambiguity in the logic that connects the premise and the conclusion. The fallacy class Equivocation is a Fallacy of Ambiguity due to the presence of phrases in arguments that are used interchangeably in different parts of the sentence, leading to ambiguity in logic. Other classes in Fallacy of Ambiguity include Amphiboly, Accent, Composition, and Division. In the case of Amphiboly, the usage of words that could be used interchangeably leads to a false interpretation in the grammatical construction of the sentences. Accent fallacy is one, where a specific phrase or word carries a different contextual meaning in the premise and the conclusion. Mistaken inferences about parts of a whole argument for drawing inferences about attributes for that argument constitute the Composition fallacy. Division fallacy is the reverse of the Composition fallacy, where mistaken inferences about the whole argument are used for drawing inferences about attributes of parts of it.\nWe provide examples for each of the fine-grained and coarse-grained classes in Table 1. A simplifying assumption we make in this work is that each fallacious argument belongs to exactly one broad class and exactly one fine-grained class. Prior work [29,60] has shown that this assumption does not always hold, for example, \"Drivers in Richmond are terrible. Why does everyone in a big city drive like that?\" as cited in [60], is an example that belongs to Ad Hominem but does have flavors of Faulty Generalization as well. This gives room for arguments to be categorized into different fallacy classes simultaneously. Our simplifying assumption restricts our classification task to a multi-class task rather than a multi-label task.\n\nRelated Work\nIn this section, we review prior computational work on logical fallacy detection and the related task of propaganda detection. We also review related work that leverages the methods of case-based reasoning, knowledge injection, and curriculum learning.\nLogical Fallacy. Prior computational work aims to formalize arguments containing logical fallacies to make them suitable for ingestion by rule-based systems and theoretical frameworks. Gibson et al. [45] formalize and identifies formal logical fallacies using Argument Markup Language (AML) and discusses the theoretical questions that arise in the study of fallacy. Yaskorska et al. [123] adopt a structure-aware approach to identify, include, and eliminate formal fallacies in natural dialogues. Nakpih and Santini [86] present a model that discovers non sequitur fallacies in legal argumentation using Prolog language and check the validity, soundness, sufficiency, and necessity of argumentation using logical rules. These works mostly focus on formal fallacies, which are defined in terms of their structure. In our work, we focus on informal fallacies, whose detection and classification rely on linguistic and world knowledge.\nOne of the few studies done on informal fallacies [60] proposes the task of logical fallacy detection, where arguments are classified into thirteen fine-grained fallacies. This work evaluates the effect of using both large pretrained language models on two datasets, called LOGIC and LOGIC Climate. Apart from using large pretrained language models, Jin et al. [60] try to abstract away from the surface of the arguments by exploiting coreference resolution and entity linking, in order to identify logical fallacies that are structurally fallacious in their arguments. Similarly, Goffredo et al. [46] alongside presenting an annotated dataset of 31 political debates from the U.S. Presidential Campaigns, use transformer-based language models and process four parts of arguments, i.e., the dialogue context, argument components (premise and claim), fallacious argument snippet, argument relation (attack or support) separately, classify them, and train all the models jointly. They show that detecting argument components, relations, and context (see also [105]) in debates is a necessary step to improve the model's performance. The main difference between [46] and our study is the fact that we do not need and use any context to classify logical fallacies. Furthermore, our framework does not assume any specific structure for text, and hence can be more generalizable. In our work, we reuse the dataset from [60], and also extend its evaluation framework in three ways: by introducing a binary detection and coarse classification stage, by developing methods with robust and explainable properties, and by carrying out an extensive set of experiments and analyses.\nPropaganda Detection. Recent research has developed benchmarks and techniques for propaganda detection in natural language documents. A significant portion of these works focuses on extracting better features as well as novel methods that would help the model boost its performance [89,49,92,63,114]. There has also been a surge focusing on the interpretability of models in propaganda detection [125], [124], [39]. Dimitrov et al. [34] show that propaganda techniques function as shortcuts in the argumentation process that connect to the emotions of the audience and often include logical fallacies. In [50], logical fallacies are called \"hallmarks of propagandist messaging\", which implies that logical fallacies can be seen as components within the broader task of propaganda detection. However, as pointed out by Jin et al. [60], the two tasks overlap but are distinct, since propaganda detection focuses on arguments that aim to influence people's opinions often using misinformation as a tool [72,59], while logical fallacy detection aims to understand gaps in argumentation. There is also a practical difference between the formalization of these two tasks, as propaganda detection data has typically focused on longer input documents, while logical fallacy datasets have generally relied on focused and isolated text inputs. In our study, we utilize the overlap between some of the propaganda techniques and fallacy classes, by augmenting the training data for logical fallacy classification with a dataset gathered explicitly around propaganda detection [79].\nCase-Based Reasoning. The case-based reasoning framework has been used to learn from past experiences explicitly in medical applications [90,91] and mechanical engineering [7,94]. One of the most important aspects of casebased reasoning is its inherent interpretability. Walia et al. [115] use case-based reasoning as an interpretation model for Word Sense Disambiguation, while Brüninghaus and Ashley [17] apply case-based reasoning to predict the outcome of legal cases. Ford et al. [40] advocate for the increase in comprehension of the black-box models using examplebased explanations by the end-users. Most similar to our work, Spensberger et al. [111] explore the effect of casebased reasoning on the student social workers and their fallacy recognition abilities, and find that those who have access to worked examples perform better during the experiment. We use case-based reasoning both as a means to enhance the performance of our model and simultaneously as a proxy to explain the behavior of the model classifying logical fallacies.\nKnowledge Injection. The challenge of generalizability and transferability for logical fallacy classifiers has been discussed in [60], by testing the model on a dataset containing unseen domain-specific subjects. This motivates the need for injection of background knowledge. Injection of background, especially commonsense knowledge in language models has been proposed within tasks of multiple-choice question answering. Combining neural language models with commonsense knowledge graphs (KGs) like Concept-Net [110] or ATOMIC [106] can be done by lexicalizing knowledge into task-targetted evidence paths and combining them with the task input [74,82]. The idea in K-BERT [69] is similar -here a multi-head attention layer is used to combine evidence from background knowledge and the input task. Other forms of knowledge injection have been popular as well, such as using graph and relation networks [68,127], or introducing the entire KG at training time regardless of the task at hand [93,75]. Notably, prior work has shown that the impact of the injected knowledge strongly depends on the overlap between the knowledge in these graphs and the downstream question answering task [75,57]. Due to the nature of logical fallacies, they can cover daily-life matters and events spreading throughout social media, and this calls for domain-specific knowledge for comprehension of certain logical fallacies. However, to our knowledge, exploiting external knowledge has not yet been fully explored in logical fallacy detection. Trying to fill in the gap and utilize commonsense knowledge in the detection of logical fallacies, we use [69] to incorporate knowledge from arbitrary knowledge bases and benefit from potential enhancements.\nCurriculum Learning. Curriculum learning has been proposed in [12] and [38] from the computer science and psychology perspectives, respectively. The key idea of curriculum learning is that starting from simple examples and learning from examples in an organized and meaningful way can contribute positively to the learning process. Using pure language model-based methods does not suffice for a reliable classification of logical fallacies [60], owing to known issues of robustness and induction capabilities of vanilla language models on unseen data [116,76]. This motivates us to leverage continual curriculum learning to attempt to improve the convergence and robustness capabilities of models, an idea that has not yet been explored in logical fallacies. The application of curriculum learning to logical fallacies in our work is facilitated by the availability of datasets at different granularity levels.\n\nMethod\nDue to the difficulty, as well as the contention over the categorization and classification of logical fallacies [52], we use methods that humans usually adopt when faced with problems that require complex reasoning. According to [95], [14] and [103], people use similar or prototypical examples of a situation or problem to solve or approach a new one. The alluded similarity can be in the various levels, namely, coarse-grained features such as the whole argument or statements, but also in the more fine-grained features and in terms of the extra knowledge one might have about concepts or entities discussed in the sentences as discussed by [5]. Having in mind the simplicity as well as explainability of using similar examples or experiences to reason about and solve new problems or situations, we adapt methods for Instance-based Reasoning, Prototype Learning, and Knowl- The adapter encodes these two inputs and tries to adapt based on the new case . Finally, the classifier uses the rectified information from the adapter to classify the new case by outputting the probabilities corresponding to belonging to each class of fallacies (in the example shown above, = 1).\nedge Injection ( §4.1). Another approach that humans follow for learning how to solve problems is starting from easy or simpler tasks and gradually shifting to harder ones to learn [38], which has been shown to work even better than other learning strategies by Chen and S. Savage [20]. This has been shown to be the case for neural networks as well [36], not as a barrier, but as a way of training more robust models referred to as Curriculum learning ( §4.2). Finally, we devise data augmentation strategies to address data sparsity and improve the stability of our models [126] ( §4.3).\n\nInstance-Based Reasoning\nInstance-based reasoning (IBR) [30] is the process of solving new problems based on the solutions of similar past problems [78]. IBR is reported to resemble the way humans think and approach new problems to save time and effort instead of starting from scratch [95]. IBR is a formalization of the general idea of Case-based reasoning (CBR) [78]. Within CBR, rather than comparing new problem instances with instances seen before like in IBR, we use past similar problems and experiences, and attempt to perform explicit generalization or induction. 1 IBR starts with a set of cases or training examples; it forms generalizations of these examples, albeit implicit ones, by identifying commonalities between a retrieved case and the target case, and tries to approach the new case using known solutions to past cases. Our IBR formulation ( Figure  2) follows a three-stage pipeline consisting of: (1) Retriever -given a target problem, retrieve similar cases with known solutions from memory, (2) Adapter -adapt the retrieved similar cases to help the decision on the new case, and (3) Classifier -classify the new case based on the adapted exemplars. The last step in our pipeline corresponds to two steps in the formulation by [1]: classify the new case based on the previous examples, and retain the new problem alongside its adapted solution and resulting experience in memory for later use in a more explicit way. We next describe the design of the retriever, the adapter, and the classifier.\nRetriever is responsible for finding similar cases to the new case from a database and passing them to the adapter together with the new case ( = ⊕ < > extracting similar examples). The retriever uses language model encoders to get the feature vectors for each new case as well as all the previous cases in the retriever database and uses these features to compute their cosine similarity. 2 The retriever obtains the most similar examples from the database, which are then passed on to the adapter module.\nWe experiment with SimCSE [42], a Transformer-based retriever that is optimized for capturing overall sentence similarity using a contrastive loss function. We also include sentence encoders that are reportedly able to manipulate a wide range of concepts, by using Sentence-BERT [98] based on MiniLM [117]. We also include Transformer models that have been trained to distinguish emotional expressions, since it has been shown that emotions can be used to manipulate masses [13] and they are intuitively important to detect certain logical fallacies, such as Appeal to Emotion. To capture the usage of empathetic and emotional terminology, we use a RoBERTa model [70] fine-tuned on the WASSA 2022 Shared Task dataset [9].\nAdapter transforms the retrieved cases { 1 , 2 , ..., } together with the new case denoted as as well as the new case , and prioritizes earlier cases that are most helpful. The adapter consists of two parts: an encoder and an attention mechanism. As an encoder, we use a language model that takes as input and , and produces a set of raw hidden states and respectively without a head layer on top.\nThe attention mechanism selects the most important information to be considered from similar cases. Based on the second step of the pipeline by [1], after the similar cases are retrieved, some of these similar cases should be manipulated or adapted to help the classifier at the end of the pipeline, since not all similar cases will be equally helpful for the model. We formalize this step with an attention mechanism on top of the encoded cases ( and ) to filter the retrieved cases or shift the attention to where it helps the model best to reason about new cases. More concretely, we use a Multi-headed attention component [113] that fetches the new case embedding as the query and the combined embeddings as both keys and values. We include both the new case as well as similar cases in to avoid losing information from the new case. The output of this component, i.e., the attention output has the same shape as and , and is fed to the last step of IBR, i.e., the classifier. Classifier layer at the end of the pipeline is applied on top of the adapter output to predict the labels. As a classifier, we use a two-layer perceptron with a [54] activation function. Given a number of classes , we compute logits and their corresponding probabilities of belonging to each class . We use cross-entropy loss as our learning objective.\nOverall, our framework is similar to a language model with a classification head on top with an important distinction. By using a retriever\n\nPrototype-Based Reasoning\nPrototype theory [103] is a theory of categorization in psychology and cognitive linguistics, in which there is a graded degree of belonging to a conceptual category, and some members are more central than others. In prototype theory, any given concept in any given language has a realworld example that best represents this concept, i.e., its prototype. Like IBR, prototype-based reasoning (PBR) is also an instance of case-based reasoning, and there has been some controversy about the superiority of one over the other. There are both claims about the superiority of prototypical examples over normal examples [61], as well as their counterparts [80] who state that a context theory of classification, which derives concepts purely from exemplars works better than a class of theories that included prototype theory ( §6.4).\nWe build on the deep learning adaptation of the prototype theory by the Prototex [31] method. The architecture of Prototex is shown in Figure 3. Prototex is based on the Prototype Classification Network proposed in [67]. The Prototex architecture contains an encoder f and a special prototype layer p, where each unit of that layer stores a weight vector that resembles a prototypical example. The prototype layer includes both positive and negative prototypes, aiming to help the models distinguish between the presence and absence of features that support any given class. The input x is first encoded into a latent representation that is shared between the input data and the prototype layer p. This representation is used to calculate the euclidean distance with the prototype layer p, resulting in a distance vector . We mask the distance vector with a distance mask layer m. The role of the distance mask m is to make the model only optimize the proximity of input examples of a particular class to a fixed set of prototypes. In other words, the distance mask directs the prototypes to represent prototypical examples of a particular class instead of a mixture of arbitrary classes. The masked distance vectors between input examples and prototypes are further fed to a fully connected layer w followed by a softmax layer s to classify a particular data point. To have interpretable prototype vectors, the model is optimized with auxiliary loss terms that bring the embeddings of the training examples closer to the prototypes and also the embeddings of the prototypes closer to the input examples.\nThe Prototex method was originally designed for binary classification between propagandistic and non-propagandistic sentences. We modify the Prototex architecture to support a multi-class classification setup. Moreover, the original architecture uses a sequence-to-sequence model, BART [66]. For a fair comparison to our other methods and inspired by the best results on logical fallacy reported in [60], we replace the BART encoder model in Prototex with a selfsupervised language model, Electra [23]. We do not use the decoder network and instead focus on the learned prototypes and their explanations.\n\nKnowledge Injection\nMany fallacy classes rely on the ambiguous structure of the logical construct in sentences to introduce flaws in arguments. Let us consider the example sentence The police asked me to freeze, but it was a hot day. So I was justified in running away, which belongs to the fallacy class Equivocation ( Figure 4). Here, the word freeze is used in two contexts, one for where the police asked to freeze and another, where the antonym of freeze, i.e, hot is used in the sentence. Such sentences, with latent fallacies, illustrate the need for models to have access to commonsense knowledge.\nWe propose a knowledge injection (KI) formulation, where background commonsense knowledge is combined with the original input for the language model. We adopt a popular method for injecting background knowledge in language models, called K-BERT [69]. K-BERT introduces knowledge injection to a BERT [33] model by querying a struc-tured knowledge base. This knowledge base consists of a set of triples of the form (subject, predicate, object). In the first layer, i.e., the knowledge layer, triples from the knowledge base are connected along with the tokens of the sentences, forming a sentence tree, as illustrated in Figure 4. The embedding layer of K-BERT flattens out the sentence tree by retaining the structural information in the form of a visible matrix. As stated in [69], a crucial goal of K-BERT is to prevent false semantic changes to the original sentence due to the addition of sentence trees from the knowledge base. K-BERT functions similarly to BERT [33], but uses a masked self-attention mechanism. The masked self-attention mechanism takes the visible matrix calculated by the seeing layer and ensures that the knowledge branches are not isolated from the tokens they are associated with and do not change the context of the general sentence that they are connected to. The classification task in K-BERT uses the Masked Language Modeling objective.\nOur KI adaptation of the K-BERT method focuses on the input of the knowledge layer, as shown in Figure 5. We adapt K-BERT to leverage knowledge from the Commonsense Knowledge Graph (CSKG) [58], which consolidates commonly used public commonsense sources like Concept-Net [110], ATOMIC [106], and WordNet [81]. The information in CSKG is structured as (subject, relation, object) triples. To link to these triples, we extract all non-stopword tokens from the sentences as individual words and we match them with triples in CSKG where the words act as subjects.\nSince CSKG contains multiple relations associated with the same subject, a key question is how to prioritize or select relations (triples) that are most relevant and informative for the input sentence. Following [75], we only use the 14 highly semantic relations in CSKG, namely 'Causes', 'UsedFor', 'CapableOf', 'CausesDesire', 'IsA', 'SymbolOf', 'MadeOf', 'LocatedNear', 'Desires', 'AtLocation', 'HasProperty', 'PartOf', 'HasFirstSubevent', 'HasLastSubevent'. Furthermore, we add a Similarity Ranking component, which ranks the retrieved triples according to their relevance to the original sentence. To do so, we estimate the contextual similarity of the triple to the original sentence by using the cosine similarity of their BERT [113] embeddings as a proxy. The cosine similarity is directly used to order the triples in order of priority. The triples with the highest similarity are injected into the original sentence, thus enriching it with commonsense knowledge. In our experiments with KI, we investigate the impact of the width and depth of the knowledge retrieval procedure. For this purpose, we test different branching factors ( ), representing the maximum number of obtained relations per subject, and different numbers of hops, representing the path length between the subject and the subsequent relations discovered iteratively based on the entities of the previously discovered relation.\nReturning to our example in Figure 4, we see that the knowledge derived from CSKG helps in providing the context for the word freeze, as a synonym for arrest. Similarly, background knowledge tells us that the word hot is related to temperature. On the surface, the words freeze and hot seem to be used in the same context, but the background information from the knowledge base helps in indicating that they are based on two completely different contexts. The background knowledge for police also bolsters that the usage of the word freeze was intended for an arrest. This additional knowledge helps in identifying the ambiguous usage of words, and connects the terms based on making implicit knowledge explicit. As the additional context (arrest, law enforcement) is not directly connected, we rely on the ability of the BERT LM to estimate contextual similarity between these terms. Thus, the combination of CSKG and LMs would lead to the classification of the logical fallacy in the sentence as one of equivocation.\n\nCurriculum Learning with Language Models\nCurriculum learning (CL) [12] is a strategy that exploits the varying complexity across ordered tasks in a pipeline to increase performance. CL uses previously learned concepts in the task pipeline and applies this information to more complex tasks in the latter half of the pipeline. We follow prior work [101] to formulate two variants of CL (Figure 6). Our Forward Curriculum Learning (FCL) strategy exposes the model to increasingly demanding tasks, similar to how humans learn concepts. We also experiment with the inverse strategy of Reverse Curriculum Learning (RCL), which starts with a difficult task and gradually adapts the model for increasingly easy tasks.\nForward Curriculum Learning (FCL) For FCL, we primarily experiment with continuous training of Transformer language model variants. We try to induce fallacy knowledge in a discrete, three-stage curriculum pipeline, going from the simplest (binary fallacy detection) to the most complex (fine-grained classification) tasks. Through the binary classification stage, we aim to introduce the structural and topical knowledge required to identify fallacies in arguments. The model uses this information in the subsequent (coarsegrained) stage to learn about the broad categories of fallacies. These learned coarse representations are then transferred to and trained further on the fine-grained fallacy classification objective.\nReverse Curriculum Learning (RCL) Rohde and Plaut [101] discovered that learning from simple to complex examples is sometimes not as effective as learning complex patterns directly first. Although they revised their claims Table 2 Augmentation examples.\n\nOriginal Sentence Augmentation Method Augmented Sentence\nEven without watching the movie, I just know that it would not be as good as the book.\n\nWordNet\nYet without watching the picture show, I just make love that it would not be as good as the book.\n\nWord2Vec\nEven without watching the moive, I just know that it could not be as good regarded the book.\n\nRoBERTa\nEven without viewing the movie, you just knew that it would not be as good as the book. Backtranslation (DE-EN) Even without seeing the film, all I know is that it wouldn't be as good as the book.\nThe news is fake because so much of the news is fake.\n\nWordNet\nThe news be fake because so much of the word is fake.\n\nWord2Vec\nThe news becomes fake anyway so much of the news is bogus.\n\nRoBERTa\nThe data is fake because so much about the information is fake.\nThe messages are fake because so many messages are fake.\nin a subsequent paper [102], we explore the capabilities of the models trained with a reverse curriculum, i.e., moving inversely from complex to simple examples, which allows us to compare the different curriculum learning strategies for the task of logical fallacy identification. For RCL, we first train on the fine-grained classes and use these weights for the coarse-grained classification task. We ultimately test their applicability on the binary fallacy detection task.\n\nData Augmentation\nBesides curriculum learning, we experiment with using data augmentation for addressing data sparsity. We devise two data augmentation strategies: modifying the original task data and adapting related benchmarks.\nAugmentation by Modifying the Original Task Data. We apply commonly used text augmentation techniques for improving the performance and enhancing contextual understanding for the logical fallacy detection and classification. We begin with a basic WordNet [81] similarity-based augmentation. This involves using the synsets to substitute the words in the input with words that have the closest meaning according to the synset. Second, we evaluate word embedding substitution methods based on Word2Vec and transformer embeddings. These substitutions involve finding word vectors that are closest to the input word vector in the embedding space and replacing them. Lastly, we experiment with a more recent technique of backtranslation, popularized by [88] and originally proposed by [35]. This involves translating the input sentence into a language that is syntactically and morphologically dissimilar, and subsequently reverse-translating this translation back to the original language. To select languages, we follow the insights from prior work [88,35,108]. As the parental tree for a language must be analyzed, languages that have fewer cognates are preferred as they enhance variety. Additionally, the use of two translation models trained on different datasets has been found to usually work better and provide more diversity to the output sentence. The most popular choices for backtranslation model pairs are German ↔ English, Turkish ↔ English, and French ↔ English. Table 2 shows representative examples of the obtained augmentations for two input sentences. We observed that the WordNet and Word2Vec techniques introduced excessive noise in our trials, which ended up deteriorating the performance of our models. For the backtranslation, we experiment with German ↔ English translation models for the augmentation because of the syntactical dissimilarity between the two languages. Although the backtranslation method was able to broaden the variety of the sentence structure, it occasionally led to the rephrasing of the actual fallacious components of the sentences. Therefore, while we believe that backtranslation and transformer-based substitution together would work best with improved translation models, in this work, we focus on augmentation with RoBERTa embeddingbased synonym substitution (RESS).\nAugmentation by Adapting Related Benchmarks. We investigate the possibility of augmenting the training data with human-curated datasets created for the related task of propaganda detection. As discussed in §3, this task applies various logical fallacy techniques including Ad Hominem, Red Herring, Appeal to Emotion, and Irrelevant Authority. We adopt the Propaganda Techniques Corpus (PTC) [29], which includes techniques that can be found in journalistic articles and can be judged intrinsically, without the need to retrieve supporting information from external resources. The taxonomy of PTC is illustrated in Figure 7.\nThe PTC dataset consists of news articles, where each sentence can have between zero, one or more fallacy anno-\n\nTable 3\nTraining data augmentation statistics for PTC. tations. As such, we adapt the PTC dataset for augmentation as follows. If a sentence contains more than one propaganda technique, then that sentence is duplicated with all its respective labels. We also combine one previous sentence, as a context, with the original labeled sentence only if the previous sentence does not belong to another fallacy class. As some of the fine-grained classes of PTC differ from those of our logical fallacy framework, we use PTC for augmentation after mapping its 18 classes to coarse-grained classes. To do so, we map the fine-grained classes in the PTC dataset to their closest fine-grained class correspondents in the logical fallacy dataset using the class definitions and descriptions. We then simply apply the broad class mapping created for the logical fallacy dataset and map the PTC fine-grained classes to the logical fallacy coarse classes. As the goal of this merging is to use the PTC coarse-grained classes for augmentation, we only leverage the training set of PTC and discard its development and test sets. Since the imbalance of the dataset worsens after merging, we use the RESS-based augmentation to augment the three underrepresented classes in the merged training setup to a minimum of = 2000 samples. We cap the augmentation to this amount so as to avoid repetitions and noise in the augmented dataset, which become dominant in the case of augmenting until the number of samples in the largest class ( ≈ 4, 000). We refer the reader to Table 3 for augmentation statistics.\n\nEvaluation\nBinary Logical Fallacy Detection. BIG Bench [43] is a benchmarking dataset that is used for probing the representations of large language models to check their biases on various sub-tasks. BIG Bench includes two tasks for probing fallacies: the binary logical fallacy detection and the formal fallacy syllogism negation. We use the binary fallacy detection dataset for evaluating whether the methods can distinguish between normal and fallacious arguments. We do not use the formal fallacy syllogism negation dataset since its format and purpose involve the deduction of the validity of sentences on the basis of the two provided premises, which is not directly related to the objective of this paper.\nWe split the BIG Bench logical fallacy dataset into training, validation and testing sets, for which the distributions are shown in Figure 8a. The dataset is balanced and contributes 2,800 samples across all three splits.\nFine-Grained Classification. For the fine-grained classification evaluation, we use the LOGIC and LOGIC Climate datasets introduced in [60]. There are thirteen classes within the LOGIC and LOGIC Climate fallacy datasets as described in Table 1. The LOGIC dataset contains everyday fallacious arguments belonging to various topics. We use the cleaned and revised version of this dataset. 3 The LOGIC Climate dataset consists of climate change news articles and fallacious arguments detected in them. We use LOGIC Climate as an evaluation-only dataset. As observed in Figures  8d and 8e, the distributions between the two datasets are different, with Intentional being the largest class in the Climate dataset, whereas it is one of the under-represented classes in the LOGIC fine-grained dataset. The LOGIC Climate dataset is included to test the ability of our models to learn these under-represented classes.\nCoarse-Grained Classification. We evaluate the coarsegrained classification based on data inferred from the LOGIC and LOGIC Climate datasets. The coarse-grained datasets are curated by mapping fine-grained classes from these two datasets to the coarse-grained categories following Figure 1. In the mapping process, fine-grained classes with ≤ 20 samples were removed from their corresponding coarse class if this coarse class was not under-represented. For LOGIC, we left out the fine-grained classes Fallacy of Relevance, Fallacy of Logic and Intentional, as their mapping to coarsegrained classes was mostly ambiguous for the data examples. This resulted in a four-way coarse classification task for LOGIC and LOGIC Climate into: Fallacy of Relevance, Fallacy of Defective Induction, Fallacy of Ambiguity, and Fallacy of Presumption.\nThe coarse version of the LOGIC dataset shows a clear imbalance. A visual representation of the distribution is shown in Figure 8b. To ensure that the testing and validation splits are representative of this distribution, we sample all our splits using stratification. The splits for LOGIC Cli- mate [60] are created in a similar manner. Their distribution is shown in Figure 8c. Evaluation Regime and Metrics. We test the models on the BIG Bench and LOGIC datasets by fine-tuning and curriculum learning. We apply the models trained on the LOGIC dataset for fine-and coarse-grained classification in a zero-shot fashion to the corresponding LOGIC Climate data. We report the average model performance over three runs. We use weighted precision, recall, F1-score, and accuracy to characterize the performance of different models. Weighted measures are used to assess the per-class scores more accurately for the available unbalanced testing sets.\n\nImplementation Details\nBaselines. We experiment with six NLI/MNLI base version models: BERT [83], DeBERTa [96], DistilBERT [22], Electra [53] and RoBERTa [97]. We utilize NLI models because we find that they perform better on the tasks of logical fallacy identification. This can be expected given that they are trained on a larger variety of data than MLM or similar models. NLI models have also been shown to have a better grasp of concepts than their MLM counterparts and to produce embeddings with better semantic representations [25]. To contextualize the results, we also evaluate two simple baselines: a random baseline and a baseline that picks classes based on the relative frequency of classes in the training set.\nInstance-Based Reasoning. For all the experiments, we use a sweep over the hyperparameters such as weight decay (L2 regularization), learning rate, and feed-forward network dropout rate. Since we use a threshold to filter the fetched similar examples from the retriever, based on cosine similarity, we use a sweep over the used threshold as well. We then use the best combination on the development set and report the average performance on three runs using the best hyperparameters. We use the NLI-initialized Electra-base LM as the underlying encoder for generating input sentence embeddings and ten epochs for each experiment. As we observe that the 0.5 similarity filter for fetched similar cases from the retrievers yields the best results, we apply a similarity filter on top of the retrievers discarding any fetched case whose cosine similarity to the new case is below 0.5. We use multi-head attention with eight heads. The number of cases ( ) used in our experiments ranges from 1 to 10. We do not experiment with more cases due to the stable trend seen when increasing the number of cases.\nPrototype-Based Reasoning. We experiment with a different number of positive and negative prototypes and find that 49 positive prototypes and 1 negative prototype works best for the fine-grained classification task. We keep the same number of prototypes for the binary, coarse-, and fine-grained classification tasks. To train the negative prototype, we also include a \"None\" class, supported by the examples from the negative class in the binary classification task. We use the NLI-initialized Electra-base as the underlying encoder for generating input sentence embeddings and report the best metrics averaged over three runs. We monitor the validation loss to choose the best model and use early stopping ( = 10) to prevent overfitting. We also compute class weights to handle any imbalance in the training dataset.\nKnowledge Injection. For the experiments with K-BERT, we perform a grid search and report the results for the bestperforming set of parameters. We use grid search to find the optimal parameters: a learning rate of 2 × 10 −5 with a dropout of 0.5. We use the BERT-base model by injecting knowledge from CSKG, and fine-tune the KI model over the different datasets for five epochs. Curriculum Learning. For a fair comparison of the curriculum learning pipeline against the baseline model, we report scores on the default hyperparameters of the finetuned model, though we expect an overall increase for all metrics of at least 2-3% when these models are tuned. We train for 5, 8, and 10 epochs for each tuning stage respectively in the curriculum learning pipeline to avoid loss of knowledge across multiple fine-tuning stages. We fix the batch size to 32, the learning rate to 5×10 −5 , and we use the cosine learning rate scheduler, while keeping the remaining hyperparameters for our experiments unchanged.\nData Augmentation. We conduct experiments with different augmentation techniques for word-based and sentencebased augmentation using NLPAug [73]. We experiment with a range of augmentation probabilities and the number of suitable substitutions for RESS, discovering the best results with 5 substitutions, while over 10 substitutions leads to a decrease in performance. Similarly, we obtain the best results with the augmentation threshold set between 80 − 90%, and a maximum of three replacements per argument.\n\nResults\nWe run all our experiments using A100-PCIE-40GB GPUs. The runtime of our experiments depends on the family of methods used, the dataset, and the size of the model being fine-tuned. We report runtimes for our best models on the binary fallacy detection task in Table 4. The recorded times show that the runtime of the models is mostly within the same order of magnitude of tens of seconds for binary, around a hundred seconds for coarse-, and between one and two hundred seconds for fine-grained classification. The PBR model is exceptionally efficient to train -its runtime is lower or comparable to the baseline NLI model. IBR takes the longest to run, taking one order of magnitude longer than PBR for binary classification and twice as long for finegrained classification. As the encoding stage that is part of the retriever in the IBR framework is executed as a preprocessing step and is presented as a look-up table in the training stage, the time that is needed to encode all training examples with an encoder is excluded in this table.\n\nOverview of the Results\nTables 5, 6, and 7 show the obtained results for each method: NLI baseline, NLI with FCL, IBR, PBR, and KI on the tasks of logical fallacy detection, coarse-grained classification, and fine-grained classification. Here, we present the best result per method, indicating the corresponding model, and dive into each method in the subsequent sections. All presented results use augmentation data based on modifying the original task data (RESS). We observe that all methods besides KI can solve the logical fallacy detection task with nearly perfect accuracy, reaching 99.7% for the IBR method with an NLI-Electra language model. The results on the coarse-and fine-grained tasks show more intriguing patterns. IBR again obtains the best performance on the in-domain task (LOGIC dataset), however, the trends are more mixed when generalizing to the out-of-domain task of LOGIC Climate. The transfer learning accuracy of IBR falls behind the PBR model on the coarse-grained classification of the LOGIC Climate data, while the performance of the NLI method with curriculum learning performs on par with IBR for the LOGIC Climate fine-grained task. The performance of our methods is generally higher than the best baseline NLI model, however, the baseline NLI model is competitive to our methods on all tasks and datasets. Among the different language models, Electra with NLI initialization has been seen to perform best on these tasks with most of our methods.\nThese results provide insights into the overall trends between the method families, however, many questions remain open. We next investigate the following questions. Does augmentation help? ( §6.2) Does curriculum learning have a consistent impact across models? ( §6.3) Does commonsense knowledge and reasoning by cases have robust and notable effect on the model performance? ( §6.4) Do instances, prototypes, and commonsense knowledge provide intuitive explanatory mechanisms? ( §6.5) Which classes are helped by our methods, and which remain difficult to address? ( §6.6)\n\nEffect of Augmentation\nAs the LOGIC dataset is highly imbalanced, we hypothesize that data augmentation will help to address this gap, ultimately bringing better performance on this dataset. The challenge with standard augmentation techniques is that logically fallacious statements have a certain structure and arrangement, which we wish to retain even after applying the augmentation technique. We experiment with modifying the Table 6 Main results for the best models for each method family on the coarse-grained classification.  Table 7 Main results for the best models for each method family on the fine-grained classification. original dataset using our RESS method and including data from the neighboring propaganda dataset, PTC. The obtained results for our models using Forward Curriculum Learning are shown in Table 8. We observe that augmentation is overall helpful on the fine-grained task and harmful on the coarse-grained task. Within the fine-grained task, the RESS augmentation always outperforms the baseline which confirms our expectation that data sparsity is an important challenge and it can be addressed through RoBERTabased synonym substitution. The PTC augmentation is partially beneficial for some models, owing to the overlap between the propaganda and the logical fallacy data. However, the effect of augmentation with PTC is dominantly negative, signaling that despite the overlap, this dataset is prohibitively different from the logical fallacy data. On the coarse-grained data, we see that augmentation has a negative impact on five out of six models even for the RESS augmentation method. We investigate this further by monitoring the augmentation impact per class, and we observe no notable variance over classes. While augmentation does not increase performance on the coarse-grained task variant, its success on the fine-grained task motivates the need for further analysis and development of data augmentation methods.\n\nEffect of Curriculum Learning\nThe effect of curriculum learning for models trained on RESS-augmented data can be seen in Table 9. We see clear trends for all three tasks that are consistent across the NLI models. 4 We observe that curriculum learning is beneficial for the coarse-grained and the fine-grained tasks, whereas it is detrimental for the binary detection task.\nAmong the two CL variants tested on the coarse-grained task, we see that RCL performs better than FCL. With the reverse curriculum, we notice that using the fine-grained weights for coarse-grained classification improves scores considerably for all models, with DeBERTa performing the best with a 0.78 weighted F1. This means that all models learn more about the coarse-grained task from the fine-grained task compared to learning from the binary fallacy detection task (i.e., when we use the BIG Bench initialization weights instead of NLI). Three out of five models still improve their performance in the FCL setup. However, Electra and RoBERTa decrease their performance and increase their variance between runs in this setup, which can be attributed to their sensitivity to hyperparameter values.\nOn the other two tasks, we only compare a single CL variant to the baseline models. We do not test FCL on the binary task, as there is no task that is easier than the binary detection in our pipeline to initialize the weights on. Analogously, we do not test RCL on the fine-grained task, because our pipeline has no task that is more complex than the finegrained classification to initialize the model weights on. For the fine-grained evaluation, we see that the coarse-grained initialization performs better than the original NLI initial- Table 8 Data augmentation results on the LOGIC dataset: no data augmentation, augmentation with RESS, and augmentation with PTC. All the models in the table are trained using Forward Curriculum Learning framework -FCL.  Table 9 Curriculum learning results with different NLI and PBR models on Big Bench and the LOGIC coarse-and fine-grained datasets. All models use RESS augmentation. ization. We note that using a forward curriculum leads to an increase in at least 1% F1-scores throughout, with Electra performing the best in this category with 0.61 weighted F1. As described before, we expected the benefit of FCL on the fine-grained task, as the forward curriculum allows the model to learn in stages of increasing difficulty, which enhances model performance at each granularity. We also observe that increasing the number of epochs at each level of the pipeline helps to reduce the forgetting of knowledge during downstream, fine-grained tasks. Interestingly, we observe a negative impact when using RCL for binary fal-lacy detection, which indicates that this task does not benefit from the initialization of models on the fallacy classification tasks.\n\nBinary (BIG Bench) Coarse-grained\nOverall, our results reliably show that the curriculum learning pipeline is capable of improving performance for the logical reasoning task of fallacy detection and the coarse representations are effective in the final stage of tuning even though they do not always outperform the other initializers in the coarse stage.\n\nAnalysis of Method Sensitivity and Ablations\nNext, we perform ablations of the components of our methods and investigate key parameter settings.\n\nInstance-Based Reasoning\nWe observe that the IBR method performs the best among the methods across all datasets (cf. Table 5, 6, and 7). This indicates that the idea of using similar instances to solve a new problem is effective at various levels of granularity. Although considering the common belief about the trade-off between predictive ability and interpretability ( [64], [18], [21]), IBR models could have not behaved as well as other methods discussed, inline with [100] and [55], we observe that IBR models offer good accuracy, as well as potentials for explainability [99]. We investigate the effect of the optimal number of similar cases, and of the designs of the retriever and the adapter on the performance of the method. We do not investigate different design choices for the Classifier, which is currently a feed-forward neural network, and as such, a trivial step in our framework.\nOptimal number of similar cases. Considering the complexity of sentences containing a logical fallacy, as well as the wide range of subjects they cover and revolve around, it is most likely that for some sentences, there would be more than one already-seen sentence that would be useful or essential for the model's reasoning. It is worth mentioning that although similar cases can potentially help the model classify certain sentences better, due to the fact that retrievers are imperfect and also language models can only capture the surface meaning of the sentences (form in the language) and not necessarily understand the meaning [11], adding more similar cases to the model can be considered noise and not useful. On this ground, we check the effect of the different number of cases shown to the model and assess their impact on the model's performance in Figure 9. As can be observed, for the coarse-grained and fine-grained datasets, there is a soft transition between using fewer examples and more examples that shows using more similar cases does not help the model as much as it hinders the process. This pattern differs further between coarse-grained classes and fine-grained classes. In the coarse-grained classification, regardless of the number of cases, the performance of the IBR model is always superior to the baseline, while in the fine-grained classification, having more than five similar examples would hurt the performance and cause a drop even below the baseline. Considering the fact that fewer similar cases mean less noise and more similar cases mean better coverage in terms of the potential aid from similar cases, we conclude that higher coverage cannot compensate for the excess noise added to the model.\nDesign of Retriever. For sentences that contain logical fallacies, nuances in meaning are vital to distinguish the actual relevant similar sentences from the ones that are only revolving around the same subject. Building upon this idea, we investigate different pre-trained language models as our retriever's encoder ( §4.1.1). The comparison between these encoders is illustrated in Table 10. We observe the superior Figure 9: Comparing the performance of the model being exposed to different numbers of similar cases in our IBR framework.\n\nTable 10\nComparing the performance of the model using different retrievers to fetch similar cases. performance of SimCSE on all the datasets with different granularity levels. We attribute this to the contrastive learning objective used in SimCSE. MiniLM with six layers, an all-round model tuned for many use cases, comes in the second rank. Both SimCSE, as well as the all-MiniLM model trained on NLI, show the relevance and effectiveness of NLI for logical fallacy prediction. However, the paraphrase models, though trained on similar tasks such as AllNLI (concatenation of SNLI [15] and MultiNLI [121]) and sentence compression, come in the last rank. Design of Adapter. We compare our results on three datasets with and without using the attention mechanism in the third stage (adaptation). The results of this ablation study are presented in Table 11. Confirming our hypothesis, we note better performance in the presence of an attention mechanism to adjust the weights on similar cases when reasoning about the new case . This observation is consistent across all datasets, which means that attention is a robust adaptation mechanism that helps the model to attend to relevant cases regardless of the granularity of the task.\n\nPrototype Learning\nWe dive deeper into the connection between prototype and classes, and the sensitivity of our PBR model on the number of prototypes.\nPrototypes Characterizing Classes. We find the prototypes responsible for the classification of each training example and assign them to the respective labels. We observe that the masking mechanism, which we introduce to the PBR method, helps to associate certain prototypes to particular classes. While we expect to see a distinct set of prototypes representative of each class, we observe a mix of distinct and common prototypes representing a particular label. For example, for the class Fallacy of Logic, we get prototypes 6, 13, 38, and 7 as the strongest representatives. However, we observe prototype 38 to be a strong representative for five other class labels as well. We believe this is because of the nature of the overlap of fallacy classes, e.g., a fallacious sentence might have flavors of both Appeal to Emotion and Ad Populum, even if only one of them is annotated as the correct class. Further, we cluster the 50 prototype tensors used for the benchmarking of the fine-grained classification task and color code the prototypes based on their indices, as shown in Figure 10. Here, prototypes 1-10 have a light color and as we go to prototypes 40-50, the shades get darker. We observe a certain grouping of prototype tensors, which may indicate unique features captured by the prototypes per class.\nPrototypes Characterizing Classes. Figure 11 shows the trend of F1-score on the fine-grained classification task for a different number of prototypes. We assign 10% of the prototypes to the negative class for this specific benchmarking. We observe a high sensitivity of the Prototex model to the number of prototypes, where having too low or too high a number of prototypes yields suboptimal results. We find that having a total of 50 (5 negative) or 100 (10 negative) prototypes yields the best performance. The PBR method is highly sensitive to the number of prototypes, and, thus, it is important to tune this hyperparameter for new datasets.\n\nTable 12\nComparing the performance of the KI method with and without using similarity ranking of relations. Moreover, we investigate whether introducing negative prototypes is beneficial to the PBR model. Similar to [31], we find that including negative prototypes together with a \"None\" prediction class brings better performance on the logical fallacy coarse-and fine-grained classification tasks, though the performance gain in our case is more limited (2-3% increase in absolute F1-scores).\n\nKnowledge Injection\nWe assess the performance of K-BERT, [69] on identifying logical fallacies in terms of the decisions made when injecting knowledge from the external KG (namely, CSKG). The information gained from CSKG is used for forming sentence trees that are used as the primary points for knowledge injection. In the process of knowledge injection with CSKG, the tokens of the sentences are broken down and the triples containing the token are appended to the token to form a sentence tree. By default, the KI method creates a sentence tree by using a maximum of two such branches per token. The exploration depth of the relation is limited to a single hop. With regards to picking useful relations, the KI method uses a brute force method for choosing triples for tokens that have multiple relations present within the knowledge base. We investigate the effect of different knowledge selection strategies, numbers of branches, and hops.\nEffect of Similarity Ranking of Relations. While information is appended to the sentence tree, we hypothesize that it is more meaningful to have a selection strategy in effect to select relations that add relevant knowledge to the sentence, and this serves as a point for an ablation study. This similarity ranking strategy enhances the performance of the KI method consistently over three different tasks for the different datasets, as observed in Table 12. The performance gain is around six F1-score points for each of the three datasets, confirming our hypothesis that selecting knowledge based on relevance is important, as also shown in [75]. This result also motivates the need for more advanced methods for context-dependent knowledge selection.\nBranching Factor Size. In the knowledge layer of K-BERT, the default branching factor is 2. Here, we analyze the performance of the model for different branching factors chosen (with similarity ranking of relations). As observed from Figure 12, a branching factor of 5, gives better performance over the other branching factors. We take this branching factor to represent a sweet spot between providing K-BERT with too little additional knowledge ( < 5) and too much additional knowledge ( > 5).\nNumber of hops. The base KI model uses only 1 hop of knowledge. A single hop corresponds to discovering the first relation and entity connected with the token, while by using multiple hops, we discover subsequent depths of relations based on the entities associated with them. Our analysis shows that, in the multi-hop setup, the performance of K-BERT decreases by 3-4%. The drop in performance can be explained by the noise introduced by including multiple hops without careful filtering of the expansion. This finding is consistent with the finding of the best branching factor size that the KI model works better when presented with a smaller set of relevant relations. We look closer at the quality of the retrieved knowledge in the next section.\n\nQualitative Analysis\nWe analyze four cases for which the base model predicts an incorrect class, and our IBR and PBR methods change the prediction to the correct class. The KI method predicts the last two examples correctly as well.\nQuality of the Retrieved Cases. For these four exemplars, Table 13 shows the retrieved instances by IBR and prototypical examples by PBR. For PBR, we show the two nearest training examples to the nearest prototype for a given input. We note that 6 out of 8 examples for IBR and all 8 examples for PBR come from the same class, which indicates that the modified decision in these cases correlates with obtaining helpful (or even representative) examples from the same class. We note, however, that this is not always the case -the retrieved examples for IBR and PBR can also be from different classes. We observe that the corrected prediction of IBR and PBR is based on two scenarios. The first situation, shown with the first three examples for IBR in Table 13, is when the retrieved examples reflect surface similarity, which curiously still helps the model to change its decision. The second situation, observed for the last example of IBR and most PBR examples, is when the model captures the structural similarity and more abstract semantics. As we hypothesize that informal fallacies require a mixture of both aspects, observing that IBR and PBR capture them to different extents is encouraging for future work. At the same time, we also observe cases where the model correctly changes its prediction even though some of the retrieved cases belong to different classes and it is not clear how they help the model prediction. This shows the impact of the other components of our methods (the Adapter and Classifier components for IBR, or the rest of the neural architecture in PBR), but also motivates the need for future work on better models for retrieving semantically and pragmatically similar cases.\nQuality of the Retrieved Knowledge. Table 14 shows the commonsense triples retrieved by KI as background knowledge. As mentioned above, the KI method predicts the first two examples wrongly, and the last two examples correctly. In example 1, we see that while the retrieved triples focus on the word phone, it is the word everyone in the sentence that is the main clue that points to the fact that the sentence actually belongs to the class Ad Populum. The second example shows a case where the background knowledge misleads the model about the subject of the sentence, thus hindering it to perform a correct classification. In the third and fourth examples, the model is able to correctly classify the example as Faulty Generalization, and we believe that this correlates with the quality of the retrieved knowledge. For instance, in the last example, BERT receives relevant knowledge such as flossing being used for good oral hygiene and floss related to teeth, which may have helped the model to overturn the wrong prediction into a correct one.\n\nTable 13\nInput arguments with their fetched similar cases. We mark the exemplars from the same class as the input in bold.\n\nClass\nInput Sentence Similar Cases (IBR) Prototypical Cases (PBR) Ad Populum Everyone is going to get the new smart phone when it comes out this weekend. Why aren't you?\n(1) I'm gonna get an iPhone because everybody else has an iPhone and they're cool.\n(1) Everyone seems to support the changes in the vacation policy, and if everyone likes them, they must be good. (2) Everyone wants the iPhone 11 because it's the best phone on the market! (2) Everyone is buying the new iPhone that's coming out this weekend. You have to buy it too.\n\nFallacy of Logic\nsurgeons have X-rays to guide them during an operation, lawyers have briefs to guide them during a trial, carpenters have blueprints to guide them when they are building a house. Why, then, shouldn't students be allowed to look at their textbooks during an examination?\n(1) Doctors refer to medical books all the time when they are treating patients. In the same way, I should be allowed to use a textbook in my medical exam.\n(1) All Paul Newman movies are great. All great movies are Oscar winners. Therefore, all Oscar winners are Paul Newman movies.\n(2) If I say that a surgeon should be allowed to use a guidebook to carry out surgery like a student can use open notes on a test, I have made a ...\n(2) The lady in the pink dress is Julia Roberts. The reporter thinks Julia Roberts drives a Prius. Therefore, the reporter thinks the lady in the pink dress drives a Prius.\n\nFaulty Generalization\nEveryone knows that teenagers are lazy (1) If we let teenagers wear whatever they want to school, they will no longer respect the rules and academic performance will decline.\n(1) If we allow a housing development to be built on Sunny Lake, a resort will come next, and soon we won't have any wilderness left!\n(2) If we don't teach teens to work harder, the human race is doomed (2) Michael is part of the Jackson Five. Without Tito and company, he will never make it.\n\nFaulty Generalization\nIf you forget to floss, you will get cavities, and if you get cavities, you will lose all your teeth by the time you're 30 (1) If you don't eat breakfast, you'll slouch in your desk. If you slouch in your desk, you'll hurt your back. If you hurt your back, you'll never become President.\n(1) If we allow gay people to get married, then the next thing you know people will be wanting to marry their pets!\n(2) four out of five dentists agree that brushing your teeth makes your life meaningful (2) You smoke pot? If you keep doing that, you'll be a heroin addict within two years. Table 15 shows per-class performance of our models on the fine-grained LOGIC task. Across the different classes, IBR performs best for six out of thirteen classes and CL comes second, which is consistent with the overall results (cf. Table 7). While we do not observe a clear pattern in terms of the superiority of methods in terms of coarsegrained classes, we do observe that the classes with more data points (first rows in Table 15) are handled better by the CL model, showing that the CL model is able to benefit when more data is available. This is somewhat counterintuitive, as we expect that CL can help the classes with more sparse data. However, we do observe that qualitatively CL has the best performance on the Ad classes: False Causality, Ad Populum, and Ad Hominem, indicating that the CL models are able to benefit from transferring knowledge within the same class from the coarse-to the fine-grained task. Observing that the two least populated classes are handled best by the methods KI and PBR indicates a potential for dataefficient reasoning with these methods. We also note the failure of our models against the baseline for the Faulty Generalization and Intentional logical fallacies. Upon closer observation of our best model, IBR, we see that it confuses Intentional mostly with Faulty Generalization, and Faulty Generalization with Ad Hominem, both of which are wellpopulated classes. However, the IBR model outperforms Faulty Generalization *If you forget to floss, you will get cavities, and if you get cavities, you will lose all your teeth by the time you're 30\n\nPer-Class Analysis\n(floss, used for, good oral hygiene), (floss, related to, teeth), (floss, related to, dental floss), (floss, related to, mouth) the baseline convincingly on all other classes, which shows that IBR is able to capture nuances of argumentation errors through its obtained examples.\n\nDiscussion\nOur evaluation shows that the methods perform relatively well across tasks and even on out-of-domain arguments, while further analysis shows that curriculum learning and data augmentation are promising components of a robust methodology for identifying logical fallacies in natural language ( §6.3 & §6.2). While our methods rely heavily on language models, the additional components such as retrieval, attention-based mechanisms, and prototype networks, provide a consistent advantage of the models over their corresponding baselines ( §6.4). Looking closer at the retrieved exemplars in the IBR and PBR methods, we observe that they are often from the same class even when they are not syntactically similar to the input case ( §6.5), which contributes to both the accuracy and the explainabil-ity of our models. Commonsense knowledge is also useful in particular cases, and potentially misleading in others, signifying the need for better grounding and path retrieval or generation. Looking at the performance per fallacy class, we observe that curriculum learning is able to benefit from knowledge transfer between Ad classes, while KI and PBR perform best in the most sparse classes. This paper pursues robust and explainable methods for reasoning about fallacies in arguments, a task that is not only understudied, but also vital to support critical thinking in an educational setting [107,37]. Our study points to research paths that should be addressed in future work.\nFurther Innovation on Robust and Explainable Methods. We observe that our models are often unable to perform abstraction and comprehend the classes in a more general sense. This has been apparent from the mixed prototype of PBR (see §6.4.2), the mixed relevance of the examples of IBR (see table 13), and the occasionally confusing triples retrieved by KI (see table 14). We note, however, that detecting and classifying logical fallacies is a challenging task both for modern-day AI as well as for humans, as it requires a complex (and possibly ambiguous) combination of a wide range of knowledge, including an understanding of rhetorical structures and inclusion of background knowledge about affordances and symbolism of concepts [56]. We see two parallel streams of AI methods that should be explored in depth for logical fallacies. On the one hand, a promising new stream relies on neural language models through methods like chain-of-thought reasoning [120], self-rationalization [87], and prompt decomposition [28], coupled with large language models like GPT-3 [16] and Codex [19]. On the other hand, neuro-symbolic methods that, e.g., pose reasoning as a soft logic problem [24] may provide an alternative approach to generalizable reasoning. We invite future work to explore these directions, as well as their intersection, for the challenge of logical fallacy identification.\nFocused Evaluation in Realistic and Open-Ended Settings. The task of logical fallacy identification, and even its related task of propaganda detection, have been introduced relatively recently in the field of AI. As such, not only the methods, but also the evaluation settings for these tasks are limited at present. In this study, we take a broad perspective, starting from theories of logical fallacies from social science disciplines, and we provide a unified framework that can support a more comprehensive evaluation of fallacies. We plan to extend the evaluation datasets in this paper by further annotation of data for the remaining categories like Begging the Question and Amphiboly in Figure 1. Moreover, beyond identifying fallacies in the context of propaganda and misinformation, we also propose that logical fallacy identification should be considered in a broader set of use cases, such as forecasting [85], where detecting wrong or misleading arguments may be central to the judgment of the trustworthiness of predictions. It is also important to consider the relation of (formal) logical fallacies to boolean satisfiability (SAT) problems [77], which have been proven to be NP-complete.\nApplication and Misuse of This Work. Logical falla-cies hold the promise to prevent the spread of propaganda, misinformation, and wrong argumentation among the very expansive content circulating daily on social media platforms. This could benefit both industry and governments, and ultimately ordinary social media users. However, strong logical fallacy identification models may also be misused to increase or enhance the diffusion of manipulative discourse [32,32,112]. We believe that analogously to the idea that encryption algorithms can be made robust if published and tested by the community [109], our social media systems and communication channels will become more resilient with the progress in developing methods and evaluation tasks for logical fallacy identification.\n\nConclusions\nThis paper represented an effort to consolidate social science work on logical fallacy organization into a formal framework that can be used to develop and evaluate AI methods. The framework consisted of three stages: fallacy detection, coarse-grained classification, and fine-grained classification. To the best of our knowledge, this paper is the first to propose methods that mitigate shortcomings of simple finetuning of pre-trained language models for the classification of logical fallacies. We applied neuro-symbolic thinking to devise three methods with native explainability and robustness, based on the ideas of instance-based reasoning, prototype learning, and commonsense knowledge injection. To deal with the inherent data sparsity, we paired our methods with approaches for data augmentation and curriculum learning. Extensive experiments on in-and out-of-domain data showed that our methods have the ability to perform robustly across tasks, and retain much of their accuracy on out-of-domain evaluation. Curriculum learning was most helpful for coarse-and fine-grained evaluation, whereas data augmentation brought clear benefits for the most difficult task of fine-grained classification. We found that the explanation by the models in terms of known training instances or structured knowledge is easy to interpret, however, we noticed that the models still largely rely on surface form patterns and similarity in their reasoning. Guided by these insights, we proposed that future research should focus on further innovation in building robust and explainable methods, extending the evaluation to more realistic and openended settings, and facilitating open-source applications for social good, while minimizing the possibility for misuse of the developed solutions."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-12"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 132184,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255026708"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "What of proposition 2.1 after the Tractatus ? Wittgenstein and the many ways “we make to ourselves pictures of facts”\n\nIn this paper I enquire 1) whether Wittgenstein retains the notion of picture after the Tractatus in a way that is more than simply an equivocation, so to speak, an ambiguous use of the term according to multiple meanings or senses; and, if so, 2) what the consequences of this might be for Wittgenstein’s understanding of philosophy as an activity of clarification. More precisely, I shall take into consideration three possible interpretations of the Tractatus’ (use of) ‘picture’ and adopt the one according to which what Wittgenstein notes in his later writings may be regarded as a further development of and a variation on it. My goal is to show that Wittgenstein’s new thoughts on pictures and on the many ways “ [w]e make to ourselves pictures of facts” (TLP: 2.1) reveal a certain aspect that is not immediately evident, yet is inherent in the Tractatus’ notion of picture: pictures lie at root of the\n\n\nIntroduction\nThis paper discusses the notion of picture in Wittgenstein's early and later philosophy. In particular, it addresses the question of whether the picture, whose role is undoubtedly central in the Tractatus Logico-Philosophicus, 1 can be considered a core theme of Wittgenstein's overall philosophical production, and whether it also occurs in his later works, remaining almost unchanged or maybe undergoing major changes in the way it is conceived. 2 As is widely known, in the Tractatus thoughts and propositions are understood as pictures (Bilder) (see TLP: 3, 4.01); and -it is worth noting right from the startthey are envisaged as such not only, or not so much, in some strange or peculiar sense, 1 Significantly, so-called 'Picture Theory' is often regarded as one of the cornerstones of the Tractatus. 2 For previous approaches to this question cf. Nyíri 2006, Phillips 2011and Westergaard 2003.\nWhat of proposition 2.1 after the Tractatus? 107 Rev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 105-122, out./dez. 2022 but \"even in the ordinary sense of the word\" (TLP: 4.011). 3 Pictures, however, also seem to lie at the centre of Wittgenstein's mature writings (produced from 1929 onwards), where they appear and are evoked in several contexts, in relation to different issues. In particular, pictures acquire a crucial role in connection to Wittgenstein's so-called metaphilosophy, i.e. to the genesis and possibility of a solution (or dissolution) of the kind of linguistic-conceptual confusion and dogmatism \"into which we fall so easily in doing philosophy\" (PI: I, § 131). From this perspective, a picture is that \"perspicuous representation\" (übersichliche Darstellung) which produces understanding -it is the tool enabling \"a clear view\" (übersehen) (PI: I, § 122).\nThe question to be addressed, then, can more precisely be formulated as follows: is there anything that remains of the Tractatus' reasoning in the way in which the notion of picture is used in subsequent texts, in a certain sense and for certain purposes? Conversely, is there anything in Wittgenstein's \"old thoughts\" that appears \"in the right light\" thanks to his \"new ones\" (PI: Preface, p. viii), 4 particularly in relation to the idea of philosophy as an activity of clarification (see TLP: 4.112)?\n\nTree ways of interpreting Tractatus' 'picture' and its fate\nIt is possible to identify three ways of answering the question of the fate of the topic of pictures in Wittgenstein, coinciding with as many interpretations of what pictures are according to Wittgenstein (that is, the first, but also the second, and possibly third, Wittgenstein), and of what role he assigns them.\n3 In a conversation with some members of the Wiener Kreis, Wittgenstein apparently acknowledged that, in speaking of propositions as pictures in the Tractatus, he had also had the mathematical notion of picture in mind and that \" [h]ere [that is, in speaking of propositions as pictures] the expression 'picture' is already taken in an extended sense\" (WVC: p. 185). 4 The topic of pictures can be included among those in relation to which Wittgenstein's new thoughts \"could be seen in the right light only by contrast with and against the background of [his] old way of thinking\" (PI: Preface, p. viii). in the sense of a model, 5 insofar as it represents reality in a truthful or false way.\nFurthermore, the idea of a picture as a model is closely related to two aspects which play a significant role in the Tractatus. The first is the fact that a picture anticipates, rather than follows, reality, so to speak; this is quite clear from proposition 4.01 where, after claiming that a proposition is a picture of reality, Wittgenstein, to avoid misunderstandings, feels the need to add that \"a proposition is a model of reality as we think it is\" (TLP: 4.01). The second, closely related aspect is the fact that no pictures (and hence no thoughts or propositions) are true and correct a priori. In sum, being a picture -in the technical and exclusive sense assigned to the term by the Tractatus, according to this first interpretation -means being a true or false representation of something; so it is by stating that a proposition is a picture that the Tractatus describes and justifies the bipolarity of propositions. In the analogy according to which each proposition is a picture, the notion of picture is used -and must be understood -in a specific sense, the specific sense assigned to it by the Tractatus. Normally, when I gaze at a painting, say, or watch a film, I don't believe (imagine) for a moment that the people I see in it really exist, or that there have really been people in that situation (PI: I, § 522).\nTruth (or falsehood) is not a relevant or salient aspect, in order for the picture to work as such.\n3. A third interpretation (the one that seems the most plausible to me) and corresponding way of addressing the question of what happens to pictures after the Tractatus is to acknowledge that this notion is subsequently taken up and enriched.\nThe answer to our question then becomes that Wittgenstein's later writings lend little support to the claim that the topic of pictures fades away after the Tractatus and that the importance assigned to it in this text should be included among those \"grave Likewise, the idea that the features assigned to pictures in the Tractatus are later lost or completely redefined finds little support. As we shall see, it is certainly true that, after the Tractatus, Wittgenstein acknowledges that pictures can sometimes serve as a good term or object of comparison (see PI: I, § 130) for propositions. \"The picture was the key. Or it seemed like a key\" (Z: § 240) to him, 6 but this is not enough to conclude -with the Tractatus -that propositions are pictures: to do so would be a sign of the kind of dogmatism \"into which we fall so easily in doing philosophy\", and which consists in taking a term of comparison \"as a preconceived idea to which reality must correspond\" (PI: I, § 131); moreover, it is true that the critical remarks which Wittgenstein formulates after the Tractatus often concern the attempt to assign pictures -particularly mental ones -a quasi-magical role and power that they do not and cannot have. But while all this is true, it is equally true that such critical remarks do not dismiss pictures completely, but only acknowledge that there are various other things that we call -and can call -'a picture', 'the making of a picture', 'the recognising of a picture as the picture of…', and so on.\nBesides, 'making to ourselves a picture' entails making many different things.\nFor example, it means adopting the most suitable medium, thereby anticipating in a way the reality of which we are making a picture. not coloured, but left blank, does not mean that the building or buildings will all be white, once they have been constructed. Or let us consider the case of photography.\nIt seems evident that a photograph -for example, the photograph of an Alpine landscape -comes after the reality it represents, so to speak. However, it may be argued that it anticipates it, in the sense that the resulting photograph will depend on many preliminary things -for instance, and primarily, from the choice of photographing the landscape rather than painting it, from the frame, and from the film (a colour or black & white one).\nThe fact that we make to ourselves pictures of reality does not mean that we can make the pictures we want or how we want them or in whatever way strikes our fancy. For instance, once we have chosen to craft the model (the picture) of a neighbourhood using polystyrene cubes, we are bound to this \"form of representation\" (TLP: 2.151). As we read in the Tractatus, [t]he picture can represent every reality whose form it has. The spatial picture, everything spatial, the coloured, everything coloured, etc. (TLP: 2.171).\nAs is widely known, Wittgenstein describes those pictures that are not bound to any specific form of representation (e.g. spatial, chromatic, etc.) as \"logical\" (TLP: 2.181). As we have already seen, thoughts and propositions are logical pictures.\nFor example, in the proposition \"On the brown table there is a book with a red cover\", the word 'table' is not written in brown ink and the word 'cover' is not written in red ink -nor is the word 'book' spatially located above the word ' (Tractatus Logico-Philosophicus, 4.5): \"The general form of propositions is: This is how things are.\"--That is the kind of proposition that one repeats to oneself countless times. One thinks that one is tracing the outline of the thing's nature over and over again, and one is merely tracing round the frame through which we look at it (PI: I, § 114).\n\nBesides, in the Big Typescript he poses the question quite openly:\nIn what sense can I say that a proposition is a picture? If I think about it, I'm inclined to say: It has to be a picture so that it can show me what I am supposed to do, so that I can be guided by it. But then all you really want to say is that you are guided by the proposition in the same sense in which you are guided by a picture. A picture is a description. Is every picture a proposition? And what does it mean to say, for example, that every picture can be used as a proposition?\n[…] Saying that the proposition is a picture emphasizes certain features in the grammar of the word 'proposition' (BT: § 21).\nMaking a proposition a picture, then, is simply a way to highlight or emphasise certain features of the word 'proposition'. Furthermore, what we have is not just the kind of picture whose features allow us to say that a proposition is a picture, but different kinds of pictures. In an annotation from 1937 whose first part came to be included in Zettel, Wittgenstein writes: As can be seen, although there are some reasons to say that Weltbild and Weltanschauung are not exactly the same, clearly they both have something to do with a form of representation (see Perissinotto 2021: p. 233).\n\"Propositions serve to describe how things are\", we think. Given all this, it is necessary to emphasise at least the three following points.\nThe first is that, both in the Tractatus and in subsequent writings, Wittgenstein uses the term 'picture' to refer to a wide range of different things: sketches, thoughts, paintings, propositions, drawings, mental images, mathematical and architectural models, photographs, maps, diagrams, etc., acknowledging that \"a picture, whatever it may be, can be variously interpreted\" (Z: § 236).\nThe second point is that Wittgenstein sees no significant difference -or no difference in principle -between material pictures and mental pictures, so to speak; indeed, one of his main concerns is to rule out that the latter, insofar as they are mental, may do something that the former, in their materiality, cannot do. In this respect, we might argue not only that one of Wittgenstein's recurrent polemical targets is the 'mythology' of the mental process -however and in whatever guise it may present itself -but also that his whole analysis of mental processes neither presupposes nor justifies any sort of substance dualism between the material and the mental, between body and mind. Certainly, in relation to logic, but also meaning and when someone tells me \"It's raining\", I normally look out of the window, rather than at this person. 11 Likewise, when I am gazing at a painting and someone asks me what I see, I don't usually answer \"A painting\", less still \"Some pigments applied to a canvas\"; rather, I will answer something along the lines of \"A port city\" -even though I obviously know the difference between gazing at the painting of a city and gazing at the same city from the window in my room. Clearly, there are some circumstances in which I would answer \"A painting\": for example, if I were a mover who has been entrusted with taking the painting from one museum to another. The same is true for \"It's raining\". If someone who is checking my hearing says \"It's raining\" and then asks me \"What did I say?\", my answer will not be \"You said that it's raining\", but \"You said: It's raining\". This point is neatly illustrated by an annotation in Zettel, where Wittgenstein points out that the people portrayed in a black & white photograph are not black & white, so to speak: Let us remember too that we don't have to translate such pictures into realistic ones in order to 'understand' them, any more than we ever translate photographs or film pictures into coloured pictures, although black-and-white men or plants in reality would strike us as unspeakably strange and frightful. Suppose we were to say at this point: \"Something is a picture only in a picture-language\"? (Z: 242).\nFrom the Tractatus Wittgenstein also borrow -albeit with significant variations -the idea that no pictures are true or correct a priori, and that a picture is such even if -as in the case of a genre painting -I do not believe or think that what it represents truly exists. Even in such cases, the picture has a hold on me: When I look at a genre-picture, it 'tells' me something, even though I don't believe (imagine) for a moment that the people I see in it really exist, or that there have really been people in that situation. But suppose I ask: \"What does it tell me, then?\" (PI: I, § 522).\nThe answer which Wittgenstein feels he should give to the question at the end of this passage is \"What the picture tells me is itself\" (PI: I, § 523). As he immediately goes on to explain, this answer amounts to saying that \"its telling me something consists in its own structure, in its own lines and colours\" (PI: I, § 523 In this sense, there is nothing mysterious in the picture. There is no need for some magic to turn shapes and colours into a picture. If I wish to make a sketch of a hill, I will grab a pencil and a sheet of paper, and start drawing. Then a picture will appear. However, we cannot stop here. What Wittgenstein is suggesting, somewhat hesitantly, is that perhaps we should say that \"[s]omething is a picture only in a picture-language\" (Z: § 242). Something is a picture because many other pictures exist, concern ourselves with its further interpretability. 13\n\nPicture and philosophical clarification\nBoth in the Tractatus and in the Investigations (and other later works) Wittgenstein envisages philosophy as an activity of clarification. According to the Tractatus, [t]he object of philosophy is the logical clarification of thoughts. Philosophy is not a theory but an activity.\n[…] The result of philosophy is not a number of 'philosophical propositions', but to make propositions clear (TLP: 4.112).\nWe find something very similar, twenty years later, in the Investigations: Philosophy simply puts everything before us, and neither explains nor deduces anything.\n[…] One might also give the name 'philosophy' to what is possible before all new discoveries and inventions (PI: I, § 126).\nWe must do away with all explanation, and description alone must take its place."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 132737,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255055476"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Neither Theory nor Science Metaphilosophical Remarks on Philosophical Elucidations\n\nThis article examines Wittgenstein’s philosophical reflection on philosophy: its method, its scope\n\n\nAbstract\nThis article examines Wittgenstein's philosophical reflection on philosophy: its method, its scope, and its relationship with other knowledge as central elements of the philosophical proposal the author developed in Tractatus Logico-Philosophicus. Therefore, my proposal is to rehearse a metaphilosophical reading of Wittgenstein's remarks about philosophy in TLP focusing, on the one hand, on his reflections on philosophy as an activity and not as a theory; on the other hand, on his categorical differentiation established between philosophical elucidations and scientific explanations. This proposal differs from the readings that interpret it is through the construction of a theory of meaning -or logical doctrine-that one can distinguish philosophical nonsense and see the world correctly.\nIt also differs from Tractatus's non-theoretical or quietist readings which understand philosophical elucidations as exclusively negative or critical nonsense. My aim is to highlight a positive aspect of philosophical elucidations that I will call performative. From my metaphilosophical reading, not only does the activity of clarification work as a\n\nIntroduction\nIn a brief text, at the end of the 1970s, Gottfried Gabriel (2007Gabriel ( [1978) grouped the great variety of interpretations that already existed of the Tractatus Logico-Philosophicus [TLP] into two large groups: the logical-epistemological ones, centered around the aspects that brought Wittgenstein closer to Carnap and the Vienna Circle; and the existential ones, which brought him closer to Heidegger. The first ones focused on the initial part of TLP, on the notions of world, thought, knowledge, language, meaning, logical structure, logic as doctrine, philosophy of science. The second ones focused on the final part of TLP, on the ethical aspects, on the saying-showing distinction and its relation to mysticism, on the meaning of life, on how to interpret silence (Cf. GABRIEL, 2007[1978. Beyond Carnap and Heidegger, to whom it could be appropriate to add Frege, Russell, Schopenhauer, Tolstoy, among other relevant thinkers, the division remains current.\nIt is also important to note that this division, between logical-epistemological and ethical-existential readings, does not always imply excluding terms.\nOver 100 years after the first edition of TLP, a third reading comes to stage: the exegetical-historiographical one. It takes TLP as a whole and as a textual source; and it seeks to highlight the opaque aspects contained therein and its relation with the historical context of production 2 .\nMy proposal for this text focuses on another aspect: Wittgenstein's philosophical reflection on philosophy, its method, its scope and its relationship with other knowledge as central elements of his philosophical proposal contained in TLP. My aim, based on these topics, is to offer a metaphilosophical reading of Wittgenstein's remarks on philosophy in TLP, focusing on two considerations: his way of understanding philosophy as an activity and not as a theory ( §4.112); and his categorical differentiation established between philosophical elucidations (Erläuterungen) and scientific explanations (Erklärungen) ( §4.111; 4.112; 4.113). This 2 There are also the readings that deal with the different periods of Wittgenstein's thought, and evaluate their continuities and discontinuities: the therapeutic, doctrinal or theoretical, elucidatory readings, among others. This paper only deals with some of those addressed in TLP. 20 REINOSO, G.\nTherefore, from this metaphilosophical reading, not only does the activity of elucidation function as a critical sieve that elucidates and shows ( §4.022, 41212) the conditions of possibility of sense ( §4.115, 4.116), but also a modification, a transformation, is produced in the one who makes them. This means emphasizing that the exercise of philosophical elucidation produces a change in the way we see the world but also in the way we \"see\" language, meaning, logic, science, philosophy, life, etc. Thus, neither does philosophy elaborate a theory nor offers scientific explanations; it does not make the philosophical elucidations of TLP a mode of self-destructive attack on all kinds of philosophy either. Rather, philosophical elucidations are presented as a defense of a particular way of practicing philosophy.\nFor this aim, this itinerary is followed: the first section is a critical analysis of quietist readings of TLP; the second, a development of elements that support an alternative metaphilosophical reading of TLP; the third, the distinction between science and scientism in order to clarify Wittgenstein's approach to scientific explanations; and the last one, an exposition of the performative aspects of philosophical elucidations that I consider vital for a non-quietist understanding of Wittgenstein's philosophical proposal.\n\nI. Tractatus's Readings: Quietism\nIn philosophy, quietism can be assumed as the view that involves avoiding substantive philosophical theorizing. In particular, it seeks to avoid postulating positive theses or dogmas and to develop constructive arguments. In the context of contemporary philosophy, quietism is directly related to a certain interpretation of Wittgenstein's work that emphasizes the negative purpose of his therapeutic-proposal. In this interpretation, philosophy conceived of as an activity without substantive theses (CRARY, 2000;MCDOWELL, 2009;WRIGHT, 1989WRIGHT, , 1992WRIGHT, , 2001. Thus, in Wittgenstein, philosophy seems to assume a role centered on the exercise of clarifying philosophical nonsense - §4.0031- (CRARY, 2000;CONANT & DIAMOND, 2004;DIAMOND, 1988;MCDOWELL, 2009;WRIGHT, 1989WRIGHT, , 1992WRIGHT, , 2001). Wright explains it succinctly: quietism involves the claim that \"meaningful metaphysical debate is impossible\" (Cf. WRIGHT, 1989WRIGHT, , 1992WRIGHT, , 2001. In J. McDowell's analogous expression, it implies \"avoidance of any substantive philosophy\" (MCDOWELL, 2009). Nonetheless, McDowell warns that quietism has sometimes been understood only as a critical moment in Wittgenstein's philosophy. In this way, there would be another moment in which he elaborates substantial views against the theory of meaning, against certain theses in philosophy of mind, and so on. He settles: It has acquired currency in readings in which Wittgenstein is complimented (a bit backhandedly) for uncovering a requirement, in connection with such topics as acting on an understanding, for substantive philosophy, which, however, in deference to a supposed antecedent commitment to quietism, he does not himself give. In a variant version of this tendency, Crispin Wright credits Wittgenstein with an 'official' quietism-leaving room for the suggestion that, inconsistently with his 'official' stance, Wittgenstein actually at least adumbrates the supposedly needed substantive philosophy, (MCDOWELL, 2009, p. 370) Beyond this caveat, quietism seems to be literally committed to the idea that philosophy does not provoke any progress or modifications, as Wittgenstein states in his second period: \"[Philosophy] leaves everything as it is\", (PI §124). Interpreted 22 REINOSO, G.\nIn their critical response to M. Williams and P. Sullivan, Conant and Diamond (2004) focus on paragraph §4.112, where Wittgenstein points out that philosophy is not about constructing a doctrine or a theory, but about practicing an activity. He seeks to emphasize that this particular activity does not result in philosophische Sätzein propositions of philosophy-but in das Klarwerden von Sätzen -in achieving clarity in our relation to the sentences of our language (Cf. CONANT & DIAMOND, 2004, p. 46). In this sense, philosophy is an activity characterized as a work of elucidation or clarification, an activity of \"making clear\". Thus, they propose that TLP should be read in an ironic key; in other words, philosophical elucidations do not have a special status. A correct reading of its statements should lead to acknowledging that these elucidations do not say anything, they are simply nonsense ( §4.0031). What is more, they are not a special type of nonsense, they are simply absurd propositions. For Conant, in order to understand §6.54, we must consider elucidations exclusively as Unsinn -nonsense. However, according to him, this approach cannot be made without appreciating the structure of the book and the philosophical method ( §6.53) as a whole, \"through which alone we can come to some understanding of what Wittgenstein meant by 'elucidation' and of how he was deploying the term 'nonsense' in the book\" (Cf. CONANT & DIAMOND, 2004, p. 68). In these resolute readings, the ladder metaphor -\"throw away the ladder after Rev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 17-33, out./dez. 2022 he has climbed up it\" ( §6.54) -is interpreted as the end of philosophy. This way of reading seems to focus exclusively on the ladder metaphor rather than on the structure of the book and the philosophical method as a whole.\nAlthough I think that the \"resolute readings\" clarify a significant aspect of TLP, that is, the relation between the notion of nonsense and the methodological reflections on the philosophical activity of elucidating, I consider that this can result into an anti-philosophical interpretation. According to Hutto (2009), for resolute readers, Wittgenstein's philosophy is understood extremely as a negative therapy that implies the end of clarification. The emphasis on the non-theoretical approach defended and exercised by Wittgenstein may lead us to think that his purpose was only destructive. From my metaphilosophical reading, Wittgenstein raises important, not only ironic or absurd, philosophical elucidations that produce modifications on the way of practicing philosophy, its limits and its scope. In the next section, I will present a metaphilosophical approach as an alternative reading to the resolute ones.\n\nII. Metaphilosophy\nIn contrast to this quietist reading and from my metaphilosophical reading, I consider that, although Wittgenstein assumes a critical attitude towards certain dogmatic ways of doing philosophy, he should not be read as an anti-philosopher.\nIn this sense, I am detaching myself from quietist readings and I am advocating aspects that I call performative. By performative, I understand those elements that are  Rev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 17-33, out./dez. 2022 explanation of the absence of uncontested philosophical claims and arguments\" (LAZEROWITZ, 1970, p. 1). The method of investigation consisted in \"translating philosophical statements back into the verbal idiom\", (REESE, 1990, p. 28).\nIn Reese's reconstruction of Lazerowitz's position, the prefix \"meta\" means \"beyond\": \"metaphilosopher goes beyond philosophy, dissolving philosophical statements back into those of ordinary language\" (Ibid). His proposal is \"in\" philosophy in the sense that it operates on material which he calls philosophical; it is \"beyond\" philosophy in the sense that it dissolves that material from the outside; and it is \"about\" philosophy because it makes a judgment about the entire philosophical enterprise, (Cf. Ibid. p. 29). Lazerowitz based his position on Wittgenstein's remark of Philosophical Investigations, §116: \"what we do is to bring words back from their metaphysical to their everyday use\". From my point of view, a literal interpretation of this remark, such as the one Lazerowitz seems to try, can wrongly reduce philosophy to ordinary language or even consider that common sense offers answers to philosophical questions. In his Philosophical Investigations, Wittgenstein explicitly stated that: \"if philosophy speaks of the use of the word 'philosophy\" there must be a second-order philosophy. But it is not so: it is, rather, like the case of orthography, which deals with the word 'orthography' among others without then being second-order\" (PI §121). In this way, I understand that metaphilosophy is not a second-order reflection or language, but a way of offering philosophical elucidations (Erläuterungen), philosophical remarks (Bemerkungen), which take philosophy itself as part of its reflection. This seems to be in tune with the way he describes philosophy as an activity and not as a theory or doctrine (TLP §4.112). Hence, this proposal focuses on the philosophical reflection on philosophy, not committed to a meta-level, meta-language, or external standpoint.\nIn a more systematic and subsequent exposition of the term metaphilosophy and of the particular methodology proposed from it, Lazerowitz (1977Lazerowitz ( [1971) emphasizes that the research focuses on philosophical utterances. The aim is to grab a satisfactory understanding of what, in their nature, allows for the \"intractable disagreements\" that invariably accompany them. It seems to be suggested that, when understood, philosophical problems or philosophical disagreements can be solved. Understanding a philosophical problem rightly = solving the problem. No one is cured, but our understanding is enlarged. The important thing to be grasped about the nature of a philosophical problem, which makes it utterly unlike a mathematical or a scientific problem, is not that understanding it is a prerequisite for its solution but that it is its solution, (LAZEROWITZ, 1977, p. 30).\nFrom Lazerowitz's perspective, it is essential to understand the nature of philosophical problems in order to understand the permanence of disagreements in philosophy. Precisely, this understanding makes it possible to solve these problems -because it is revealed that they are formulated in a confused way; consequently, the disagreements are resolved. From my point of view, the main goal of philosophical reflection on philosophy is not to solve all kinds of problems, but to understand, within the framework of TLP, its logical structure in order to determine whether its formulations make sense. An important remark must be made before proceeding: with my suggestion I am not assuming that Wittgenstein proposes a metaphilosophy in TLP. Instead, I am proposing a metaphilosophical reading -or metaphilosophical remarks-in order to highlight his philosophical reflections on philosophy in a positive sense. This positive aspect arises when his strategy, which differentiates philosophy from other disciplines, is understood as a defense of a way of doing philosophy.\nTaking performative aspects into account, and from my suggestion, metaphilosophy cannot be only reduced to an initial propaedeutic instance. Rather, it should be seen as a philosophical orientation that regards the examination of philosophy itself as cardinal. Therefore, this examination of philosophy is also philosophical. This allows us to understand that the philosophical reflection proposed by Wittgenstein is not reduced to setting the limits of sense, in a purely destructive way for philosophy. As we shall see in the next section, this in turn relates to the categorical contrast between the method of philosophical investigation and that of natural science. For Wittgenstein, philosophy as an activity of clarifying the limits of sense, and his proposal of philosophical elucidations as nonsense, does not imply accepting the method of natural science as a model for all kinds of investigation. Therefore, neither does Wittgenstein abolish philosophy in favor of science nor he subordinates the philosophical method to the scientific one. In order to show how Wittgenstein, from his non-theoretical perspective, does not attack 26 REINOSO, G.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 17-33, out./dez. 2022 every way of doing philosophy but defends a particular one, these reflections should be connected to his views on the status of scientific explanations.\n\nIII. Scientific Explanations: Science and Scientism\nRegarding the anti-theoretical approach of Wittgenstein's philosophy, and from the reading I propose, the distinction between philosophical clarifications or elucidations and scientific explanations is crucial. This distinction is directly related to both his philosophical reflection on philosophy and the protection he seeks for philosophy against scientism. In TLP, he made a distinction between the explanations that the natural sciences can offer and the elucidations (Erläuterungen) that philosophy offers ( §4.111; 4.112). At the end of the 19th century and the beginning of the 20th, a great dispute arose over the distinction between what W. Dilthey  called the sciences of nature (Naturwissenschaften) and the sciences of spirit (Geisteswissenschaften). Many authors of the time sought to establish the specific methods each of them uses based on the distinction between explanation (Erklären) and understanding (Verstehen). The sciences of nature were dedicated to explaining the world; those of spirit, to understanding it. The categorical distinction between explanation and understanding, between causes (the general) and reasons (the particular), outlined the debate on the methods that characterized both the natural and the social and human sciences of that time.\nBeyond the specific debate that is established by Dilthey and the further development of hermeneutics, I am interested in highlighting that Wittgenstein presented family resemblance with these irreducible differences between philosophy and science. However, not necessarily do these differences have to be understood in terms of confrontation. From my reading, Wittgenstein's proposal is to make a difference that he considers essential to avoid any kind of reductionism and any kind of foundationalism for philosophy. The tendency that he, and many authors of that time, find suspicious -not to say dangerous -is to subsume all \"scientific\" disciplines to a unique model: the empiricist scientific method of the natural sciences. As K. Kraus considered, the error of his time was to conceive progress based only on the model of scientific and technical progress, (Cf. BOUVERESSE, 2006, p. 192).\nThe distinctions between reasons and causes, descriptions and explanations, might imply that Wittgenstein was interested in reestablishing the opposition between the sciences of nature and those of spirit -like Dilthey or Spengler.\nHowever, this apparent reestablishment is not given in the terms in which these authors understood the opposition. Wittgenstein's aim is rather to reject the classical model of \"science\" -even for social science, whose clear example is Frazer's anthropological work-as the only model to be followed (Cf. Ibid. p. 210).\nPhilosophy is a fundamentally elucidatory activity that operates on language. Hence, neither does it produce \"novelties\" -\"discoveries\"-, such as those implied by science, nor it elaborates \"theories\" since these imply a kind of explanations that Wittgenstein considers to be beyond the scope of philosophy.\nOn the other hand, and in a brief digression, Wittgenstein's TLP should also be placed in what is known as the \"language crisis\" of the Vienna of the 900s. This book seems to be a response to the crisis captured in The Lord Chandos Letter (1902) by Hugo von Hofmannsthal (1874-1929), whom Fritz Mauthner (1849-1923 considered one of his owns best interpreters. The author of the letter completely abandons writing not only books but also letters because he can no longer formulate concepts given the variety of experience subject to constant change. In fact, it falls into a \"deep state of speechlessness\", of silence, as words lose their references; it represents \"an escape from language\". Mauthner (2001Mauthner ( [1903) was convinced that language deceives us. This idea explains his epistemological radical skepticism about the reliability of language as part of the cultural pessimism of the early twentieth century in the Austro-German tradition. This delusion or superstition supposes the idea that, although there are words that claim to represent objects in the world, they have lost their anchorage in it; then, they distort it. With this digression, I only want to emphasize that the philosophical elucidations separated from the model of REINOSO, G.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 17-33, out./dez. 2022 science that Wittgenstein defends do not commit him either to an abandonment of language or to a commitment to literature. From my reading, Wittgenstein is interested in a defense of a way of practicing philosophy, not in its cancellation or its definitive dissolution.\nFinally, the reading I propose does not pretend to deny that TLP provides elements for developing a philosophy of science. His critical vision of the modern world, of progress and scientism, did not prevent Wittgenstein from expressing a deep admiration for the more theoretical or abstract reflections on science or the philosophy of science outlined by, for example, Hertz and Boltzmann (Cf. MONK, 1997, p. 40-1). Tomasini Bassols (2020) details the different ways in which a philosophy of science can be done in terms of external or internal to the scientific endeavor, and points out the important contributions that Wittgenstein made to both variants. However, the fact that the book allows for this possibility should not imply that this is the aim of TLP. Wittgenstein did not seek to develop a philosophy of science; he did not understand that philosophy is reduced to such a task as positivist readings tended to misunderstand, either nor it implies that TLP was\n\nIV. Philosophical Elucidations: Performative Aspects\nAs we have seen in the previous sections, for Wittgenstein, philosophical activity consists fundamentally in presenting elucidations (Erläuterungen). As such, philosophy does not aim to offer \"philosophical propositions\" -philosophische Sätze-but to make the propositions clear -das Klarwerden von Sätzen ( §4.112). The term elucidations appear few times in TLP: §3.263, 4.112, and 6.54. It is linked to the activity of making clear ( §3.251, 4.112, 4.115, and 4.116) and the way of presenting philosophy ( §3.324,4.003,4.0031,4.122,6.211,Neither  6.53). From the reading that I propose, the notion of elucidations is also linked to the difference established with scientific explanations ( §4.111; 4.112). Given this constellation of links, I do not interpret that philosophical elucidations should be assumed in their negative aspects, as absurd nonsense products of a destructive clarification -or therapeutic -activity, but in their performative aspects.\nThe term \"performative\" has enormous importance in the philosophy of language since J. L. Austin (1911Austin ( -1960 it presented. Austin proposed the concept of performativity to establish an inseparable connection between language and action.\nFor Austin, performativity occurs when an act of speech or communication not only uses \"words\", but it necessarily involves an action at the same time.\nAccordingly, the performative stresses that some expressions serve to effect a transaction or constitute the performance of the act specified by virtue of their utterance -for instance, to make a promise. Austin intended to highlight the pragmatic dimension of natural language and the elements that come into play in a linguistic transaction or speech act. It is worth clarifying that, when I mention of performative aspects, I am not indicating that the analysis of this pragmatic dimension of natural language appears in the context of TLP. What I am trying to emphasize is that philosophical elucidations have the effect of performing an action, the action of seeing differently.\nI think a correct way of describing the positive aspects of philosophical elucidations is to assume that performative expressions are not primarily about exchanging information but about producing an effect. On the other hand, this way seems to be in tune with understanding that the sharp distinction with scientific explanations that Wittgenstein proposes is not a destructive attack on philosophy but a particular defense of it. For all these reasons, I choose the performative term as a way of understanding philosophical elucidations because it is a term linked to language and action, though not in the Austin's sense. As regards language, it is through an activity of language clarification that we can discover and set the limits of sense and understand philosophical elucidations as nonsense. As regards action, two levels are distinguished: the first one, that clarification is a practice, not a theory; the second, that it produces an effect; defining elucidations as nonsense is not to indicate that they are something empty or passive. Elucidations are not simply a 30 REINOSO, G.\nRev. Filos., Aurora, Curitiba, v. 34, n. 63, p. 17-33, out./dez. 2022 theoretical distinction that we contemplate; instead, they have an active effect by producing a change or transformation in the way we see or understand certain phenomena such as language, meaning, logic, ethics, etc. According to my reading, Wittgenstein makes a distinction between philosophical elucidations -nonsense-and scientific explanations -propositions with sense-, without seeking to favor science over philosophy. Rather, he seems to defend both that can only occur if they are properly differentiated. Therefore, it is the metaphilosophical reading that allows for highlighting the performative aspects of philosophical elucidations the one which explores the connection between the status of philosophy as an activity of elucidation and the defense of philosophy against scientism. In TLP's preface Wittgenstein already indicated that his book deals with philosophical problems and shows that: the reason why these problems are posed is that the logic of our language is misunderstood (…) Thus the aim of the book is to draw a limit to the expression of thoughts (…) only be in language that the limit can be drawn, and what lies on the other side of the limit will simply be nonsense (WITTGENSTEIN, 2001, p. 4). This is why, in a metaphilosophical remark, it can be pointed out that the book also offers valuable elucidations, not simply ironies -as the resolute readings proposed. In this way, these elucidations can produce changes in how we understand, how we see, concepts that are part of the philosophical discussions of the early twentieth century around language, meaning, logic, logical constants, and the problem of analysis, among others. At the end of §6.211, between brackets, Wittgenstein states that: \"(In philosophy the question, 'What do we actually use this word or this proposition for?' repeatedly leads to valuable insights [wertvollen Einsichten])\". These insights [Einsichten], the elucidations (Erläuterungen) that are reached, do not claim to be recognized as true, since they are not part of a correct theory of language but intended to clarify problems concerning the symbolism of TLP which must speak for itself (Cf. ENGELMANN, 2013).\n\nFinal Considerations\nTherefore, from this metaphilosophical reading, not only does the activity of elucidation function as a critical sieve that separates what makes sense from what does not, but also a modification, a transformation, is produced in the one who makes them. This means emphasizing the performative aspects of philosophical elucidations because they produce a change in the way we see the world but also in the way we \"see\" language, meaning, logic, science, philosophy, life, and so on.\nThereby, philosophical elucidations are presented as a defense of a particular way of doing philosophy.\nFinally, and beyond TLP and the specificity in its philosophical proposal, I consider that the philosophical question of philosophy and its relationship with science and scientism are still valid. Given the place that science and technology have in our lives, both at a theoretical and practical level, it is still essential and urgent not only to rethink about the status, the scope, the limits, the divulgation of quality, of science, but also of philosophy. I consider that these metaphilosophical remarks are inviting us to take the ways in which Wittgenstein thought of tractarian philosophical elucidations -their purposes and effects-in relation to science as tools that we can use, not to replicate the ladder he built but to reflect philosophically on the place that philosophy has -or should have -in our own contexts and in the face of our own historical challenges."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-07"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 132799,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254430339"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "The Body among Neoplatonists and Christians at the End of the Fourth Century: Synesius of Cyrene’s and Eunapius of Sardis’ Perspective\n\n: This brief study addresses the controversial issue of the relationship with the body, with the ﬂesh, on the part of pagan and Christian thinkers at a particularly important point in their evolution, in the late fourth and early ﬁfth centuries, a time in which Neoplatonic thinkers had to defend their doctrinal positions against the increasingly hegemonic position of the triumphant Christianity. In this sense, it is particularly interesting to approach the perspective of two authors who are not strictly speaking philosophers: in particular, Synesius of Cyrene, a thinker in the Neoplatonic tradition who became a Christian bishop, complemented also by some interesting reﬂections by Eunapius of Sardis, historian and biographer of Neoplatonic philosophers. In the light of this analysis, it becomes clear that the discussion on the value of the body and carnality is an essential point of doctrinal discrepancy in this period and, contrary to what sometimes appears, the discrepancy also pertains to the formation of the intellectual, and Christianity clearly appears as a doctrine obsessed with the ﬂesh to the detriment of the soul.\n\n\nA Word on Aims, Methodology and Delimitations\nThe controversy about the body, the flesh, matter itself, is one of the main axes of the ideological and doctrinal confrontation between pagan philosophy-mostly Neoplatonicand early Christianity, during the first centuries of our era. Despite the various studies on Christianity's disdain for the flesh and the body 1 , it may be said, however, that the usual criticism against Christian thinkers, in Neoplatonic sources, focuses precisely on the-in their opinion-excessive attachment to the carnal, to the body. In this sense, it is particularly significant to address the question at the end of the fourth century and the beginnings of the fifth, when Christian philosophy had already consolidated some of its major doctrines and thinkers, precisely at the time of the emergence of the asceticism represented by the desert anchorites-which is some kind of \"democratization\" of ancient Greek philosophy-and also when the pagan philosophical schools withdrew, not without having notably influenced Christianity itself. In this period, figures emerge that help to analyze these doctrinal discrepancies in a privileged, particular way: I am referring to Synesius of Cyrene (373-414), an intellectual trained in the major pagan tradition, a disciple of the famous Neoplatonic philosopher Hypatia of Alexandria, and a prolific writer in practically all the traditional Greek literary genres, who ended his days, moreover, as a Christian bishop and to Eunapius of Sardis (347-414), sophist and historian, author of a collective biography of almost contemporary Neoplatonic philosophers, an ardent defender of the pagan tradition in the face of triumphant Christianity. The fact that they have an excellent knowledge of the contemporary Neoplatonic tradition without, however, being philosophers belonging to a school and that they are well acquainted with the Christianity of the time, allows us the approach that we intend here.\nThe main aim of this analysis is to show evidence of the controversy between Neoplatonists and Christians concerning the body and carnality at the end of the fourth century from the perspective of two leading intellectual figures of the period who do not belong to any particular philosophical school, that is, who do not practice as professional philosophers. They approach the problem from a different point of view, which can be described as more generic, more rooted in traditional pagan παιδεία, and therefore more interesting in a literary and even sociological sense. Such an approach will allow us to see that the dispute over carnality affected, in this period, not only the most obvious philosophical positions but also the very notion of intellectual training, of παιδεία itself, at a time of widespread discussion about valid pedagogical models. It will also allow us to see that the perception of Christianity by pagan intellectuals was-contrary to what is often perceived-that of a group of uneducated barbarians obsessed with an excessive attachment to the body and carnality to the detriment of the soul and contemplation.\n\nThe (Neo)platonic Problem with the Body: A Very Short Retrospective\nAs is well known-and, of course, the subject is completely beyond the scope of this study-the Platonic dualism between body and soul, plausibly Pythagorean or Orphic in origin, contributes to the development of a disdain for the body in favor of the soulsomething markedly new in ancient Greek culture 2 . Perhaps the most emblematic image of this new conception appears in the Charmides, where Plato presents Socrates fervent with desire for the young Charmides, barely able to restrain himself in his eagerness to see what is hidden under his cloak, but who nevertheless finally manifests his deep desire to strip the young man not of his cloak, but of his body, to have a look at his soul (154e). The idea, of course, is revolutionary with respect to the whole previous Greek tradition: 3 instead of contemplating the desirable body of a young man, it is preferable for (the Platonic) Socrates to contemplate his soul of which the body is nothing more than a mere garment. The Platonic novelty is, whatever its antecedents, very significant: unlike in Homer, the soul (ψυχή) does not end in Hades but can be reincarnated again in a new body; 4 and the body (σῶµα) does not receive its name only after the death of the person but as the soul inhabits it. Even more innovative is the relationship established between body and soul in the Platonic texts, markedly unfriendly throughout life, to the extent that the separation of the soul from the body after death is described as a real liberation of the soul from its body/grave-the famous σῶµα σῆµα 5 .\nFrom these assumptions 6 -along with their development in the cosmic speculations of the Timaeus and despite the peculiar drifts of the Middle and New Academy regarding the soul 7 -the notion of the preponderance of the soul over the body and the even abject character of the body are systematically emphasized in the Neoplatonic tradition. Indeed, for the Neoplatonists, the terrible consequences of the incarnation of the soul in a body can be only neutralized by a strong asceticism which forces all sorts of physical renunciations and allow the philosopher to reach, with the minimum possible distraction due to the claim of matter, the union with the divine through pure intellectual labors, which include the contemplation of the universe and the analysis of the principles of human knowledge: this is the postulate by Plotinus (Enneads III 5, 1), to take only the most conspicuous example 8 , and according to Porphyry's most literary description (The Cave of the Nymphs 35), when the bodiless souls will return to their true spiritual homeland beyond the stars, even the inconvenience of possessing a body will be forgotten, as if it had never been.\nThis contempt for carnality on the part of the Neoplatonic intellectuals is clearly evident in Eunapius of Sardis 9 , who takes up the general feeling of his confreres regarding the complete disregard for the body. In the Lives of the Philosophers and Sophists, usually dated in approximately 399 10 , he presents his heroes clearly in this light: Porphyry, during his stay at Rome together with Plotinus, began to hate his body and his very humanity (VS IV 1, 7: τὸ τε σῶµα καὶ τὸ ἄνθρωπoς εἶναι ἐµίσησεν); Aedesius turns away from all that is human, to the extent that he seems to be all soul (VS II 3, 5); Antoninus despised his body (VS VI 10, 6); and we could add further examples. Eunapius also echoes, with great scandal, the Christians' taste for the carnal, especially in the cult of the martyrs, in the context of a furious attack, typical of one who feels the very survival of paganism to be deeply threatened, at a time of destruction of temples and restrictions on the public expression of pagan cults (VS VI 11, 8): Next, into the sacred places they [sc. the Christians] imported monks, as they called them (τoὺς καλoυµένoυς µoναχoύς), who were men in appearance but led the lives of swine, and openly did and allowed countless unspeakable crimes (ἀνθρώπoυς µὲν κατὰ τὸ εἶδoς, ὁ δὲ βίoς αὐτoῖς συώδης, καὶ ἐς τὸ ἐµϕανὲς ἔπασχóν τε καὶ ἐπoίoυν µυρία κακὰ καὶ ἄϕραστα). But this they accounted piety, to show contempt for things divine. For in those days every man who wore a black robe and consented to behave in unseemly fashion in public, possessed the power of a tyrant (τυραννικὴν γὰρ εἶχεν ἐξoυσίαν τóτε πᾶς ἄνθρωπoς µέλαιναν ϕoρῶν ἐσθῆτα, καὶ δηµoσίᾳ βoυλóµενoς ἀσχηµoνεῖν), to such a pitch of virtue had the human race advanced! [. . . ] For they collected the bones and skulls of criminals who had been put to death for numerous crimes, men whom the law courts of the city had condemned to punishment, made them out to be gods, haunted their sepulchres, and thought that they became better by defiling themselves at their graves (ὀστέα γὰρ καὶ κεϕαλὰς τῶν ἐπὶ πoλλoῖς ἁµαρτήµασιν ἑαλωκóτων συναλίζoντες, oὓς τὸ πoλιτικὸν ἐκóλαζε δικαστήριoν, θεoύς τε ἀπεδείκνυσαν, καὶ πρoσεκαλινδoῦντo τoῖς ὀστ oῖς καὶ κρείττoυς ὑπελάµβανoν εἶναι µoλυνóµενoι πρὸς τoῖς τάϕoις). \"Martyrs\" (µάρτυρες) the dead men were called, and \"ministers\" (διάκoνoι) of a sort, and \"ambassadors\" (πρέσβεις) from the gods to carry men's prayersthese slaves in vilest servitude, who had been consumed by stripes and carried on their phantom forms the scars of their villainy (δεδoυλευκóτα κακῶς, καὶ µάστιξικαταδεδαπανηµένα, καὶ τὰς τῆς µoχθηρίας ὠτειλὰς ἐν τoῖς εἰδώλoις ϕέρoντα). [Trans. Wilmer Cave Wright] In the same vein, for Eunapius, attachment to the corporeal (τὸ ϕιλoσώµατoν) is not only considered to be opposed to philosophical practice (VS VIII, 1, 3) but also something that belongs to barbarians (VS VI 5, 8).\n\nEarly Christian Attitudes on the Body\nIn the early Christian thought, on the other hand, the body-despite being opposed to the soul according to the same shared Neoplatonic precepts and having to subordinate itself to the soul through hard asceticism (See Brown 1998)-nevertheless acquires an importance unthinkable in Neoplatonic terms, insofar as the body must be the reflection of the soul with which it forms a unity: it must reflect God as much as the soul 11 . No doubt this importance of the body, which is not simply to be neutralized by the soul, stems from the paramount importance of the resurrection of the flesh, which was the subject of debate in Christian circles at precisely this time. Plotinus, although he never mentions the Christians by name, undoubtedly has them in mind when he writes (Enneads III, vi, 6, 71): The true waking is the true resurrection, not with the body, but from the body (ἡ δ' ἀληθινὴ ἐγρήγoρσις ἀληθινὴ ἀπὸ σώµατoς, oὐ µετὰ σώµατoς, ἀνάστασις), because resurrecting with the body only mean getting out (µετάστασις) of one sleep into another.\nThe resurrection of the flesh is the exact opposite of what the ancient Greek philosophical tradition, from the Pythagoreans and Plato to the Neoplatonists, put forward-which is no doubt why the fourth-century Church had to assert it forcefully against all kinds of pagan philosophical approaches 12 . The three objections by Porphyry in his treatise Against the Christians, preserved fragmentarily, are well known: (a) the universe is eternal, uncreated and indestructible, (b) souls cannot be created at birth, and (c) resurrection of the body is impossible 13 . It can be argued that this is the basic doctrinal apparatus shared by the entire Neoplatonic tradition, the same one that led to the rejection of the idea of a bodily resurrection in the Neoplatonic Christian John Philoponus, of whose treatise On the Resurrection only a few fragments have survived in Syriac. In addition, undoubtedly, his Neoplatonic training is what motivated the reluctance of bishop Synesius of Cyrene to accept the literalness of the resurrection of the flesh, for example in his Epistle 105, where he describes it as \"a sacred and mysterious allegory\", ἱερóν τι καὶ ἀπóρρρητoν (89) (See Bregman 1982, pp. 98-111, 160-61). Even the semi-corporeal element that remains attached to the soul after death according to Porphyry (Sententiae ad intelligibilia ducentes XXIX) is only a physical remnant that adheres to souls that have not sufficiently purified themselves of worldly concerns, since the ultimate goal of the Neoplatonic tradition is precisely that the soul is finally completely freed from the burden of the body, which distracts it from living in the element that is proper to it. It is especially significant that the Neoplatonist Celsus, in attacking Christians, refers to them as \"people who love the body\" (ϕιλoσώµατoν γένoς ) 14 , exactly the same word that Eunapius, as we have seen above, used to define the opposite of philosophical practice, no doubt in a veiled reference to Christianity; and the same claim made by Plotinus: we need to remain pure and without affection for the body (ϕιλoσωµατεῖν) 15 .\nIn the same vein, it is worth remembering that, at the first Council of Constantinople in 381, the humanity of Christ is precisely emphasized, against those who, as Apollinaris of Laodicea, defended that He was the incarnate divinity, but without fully assuming human flesh, will and reason; and, above all, that the Council of Ephesus, in 431, insisted to the point of redundancy on the carnality of Christ, and on the total belonging of the human flesh of Christ to the divine Logos: the presence of the derivatives of σάρξ in its decrees and in its anathemas is simply overwhelming. It seems clear that the question was the subject of intense debate among the intellectuals of the day, and, no doubt, carnality presented problems especially for Christians more dependent on a Neoplatonist background. Suffice it to consider the extreme example of Origen, who, commenting on the proximities between Genesis (3, 21) and Plato's Phaedrus (246c), considered that the same phenomenon-the incarnation of the naked soul-was described in both, and went so far as to assert that Adam did not actually have a body until he committed the first sin: 16 not that Adam and Eve realized that they were naked, but that it was only then that their soul was clothed by a body of flesh, so that the flesh is thus excluded even from the original creation.\nOn the contrary, early Christianity affirms that corporeality is the place of spirituality, since God has become incarnate and has risen in the flesh: the new Christian philosophical life-represented particularly by the monks who populated the deserts during the fourth and fifth centuries-clearly presents the paradox of the flesh, tortured as a cause of fall and sin, but at the same time the only place where spirituality and salvation are possible, in the sense of Paul in 1 Corinthians 6, 15: \"Do you not know that your bodies are members of Christ?\", oὐκ oἴδατε ὅτι τὰ σώµατα ὑµῶν µέλη Xριστoῦ ἐστιν (See Margel 2016).\n\nSynesius of Cyrene's and Eunapius of Sardis' Perspective: The Dangers of Christian Obsession about the Body against the Soul, and the True Formation of an Intellectual\nThe contemporary reaction of some Neoplatonists at the end of the fourth century allows us to see even more clearly the specificity and novelty that early Christianity represented in comparison with the Neoplatonic philosophical tradition that was contemporary to it. In particular, the perspective of Synesius of Cyrene, a Neoplatonic intellectual who ended his days as a Christian bishop, provides an insight worthy of comment. In fact, as he himself declares in Letter 41, written in approximately 412, at the end of his life, he did not in fact practice philosophy in public, in the Cynic style, nor did he open a school, nor did he act as a sophist before audiences: he cannot therefore be described as a philosopher, although his doctrines are clearly of Neoplatonic origin 17 , as he was always a faithful disciple of the famous Hypatia of Alexandria, who survived him by only a few years-Synesius did not, therefore, witness her gruesome end.\nHe was no ordinary bishop either: 18 in recent years there has been a certain consensus among scholars that Synesius was already a Christian at least since his marriage, blessed by the Patriarch of Alexandria in 403, or perhaps even earlier, after his return from his embassy to Constantinople in 401 19 ; but his Christianity cannot be described as orthodox. He himself honestly expressed his reluctance to accept the episcopal charge of his city in an open letter to the patriarch Theophilus in 410 (Letter 105, 73-99): It is difficult, if not quite impossible, that convictions should be shaken, which have entered the soul through knowledge to the point of demonstration (τὰ δι' ἐπιστήµης εἰς ἀπóδειξιν ἐλθóντα δóγµατα). Now you know that philosophy rejects many of those convictions which are cherished by the common people (πoλλὰ ϕιλoσoϕία τoῖς θρυλoυµένoις τoύτoις ἀντιδιατάττεται δóγµασιν). For my own part, I can never persuade myself that the soul is of more recent origin than the body. Never would I admit that the world and the parts which make it must perish. This resurrection, which is an object of common belief, is nothing for me but a sacred and mysterious allegory, and I am far from sharing the views of the vulgar crowd thereon (ἀµέλει τὴν ψυχὴν oὐκ ἀξιώσω πoτὲ σώµατoς ὑστερoγενῆ νoµίζειν. τὸν κóσµoν oὐ ϕήσω καὶ τἄλλα µέρη διαϕθείρεσθαι. τὴν καθωµιληµένην ἀνάστασιν ἱερóν τι καὶ ἀπóρρητoν ἥγηµαι, καὶ πoλλoῦ δέω ταῖς τoῦ πλήθoυς ὑπoλήψεσιν ὁµoλoγῆσαι). The philosophic mind, albeit the discerner of truth, admits the employment of falsehood. [ . . . ] What can there be in common between the ordinary man and philosophy? Divine truth should remain hidden, but the vulgar need a different system (τὴν µὲν ἀλήθειαν τῶν θείων ἀπóρρητoν εἶναι δεῖ, τὸ δὲ πλῆθoς ἑτέρας ἕξεως δεῖται). I shall never cease repeating that I think the wise man, to the extent that necessity allows, should not force his opinions upon others, nor allow others to force theirs upon him. No, if I am called to the priesthood, I declare before God and man that I refuse to preach dogmas in which I do not believe (oὐκ ἀξιῶ πρoσπoιεῖσθαι δóγµατα). Truth is an attribute of God, and I wish in all things to be blameless before Him. [Trans. Augustine Fitzgerald] In addition to having to give up hunting and his wife, Synesius found it difficult to accept certain Christian dogmas, and, just as a good Neoplatonist, he clung to the preexistence of the soul, the eternity of the uncreated universe and the immortality of the soul, but he did not accept the resurrection of the flesh, and, above all, he attacked head-on the possibility of popularizing, of vulgarizing, the truth. Basically, however, it is nothing other than the reluctance of a Neoplatonist to abandon the pure theoretical life, the βίoς θεωρητικóς proper to a philosopher, and to adopt the practical life of busy politics that corresponds to a bishop (Letter 105,(25)(26)(27)(28)(29)(30) 20 .\nIt is precisely in his work Dio 21 , which he sent to his revered master Hypatia in 405, that Synesius discusses his personal proposal of παιδεία, closely linked to his notion of what is to be understood by philosophy and by a philosophical life in progress towards contemplation 22 . Although it was undoubtedly written a few years earlier-in response to the criticism he had received for the publication of his lost work Cynegeticus in 392, as he himself explains in Letter 154-the work is presented as a protreptic to philosophy addressed to his future son Hesychius, who was to be born in 404. In reality, it is a vital and literary apology, which defends the unity of knowledge, overcoming the old conflict between philosophy and rhetoric, along the lines of his model, who gave his name to the book, the philosopher and sophist Dio Chrysostom. For Synesius, a full education should include the study and practice of literature, contrary to the opinion of the uneducated, whether pagan or Christian, each characterized by the color of his cloak, as was common at the time (Letter 154, to Hypatia, dated in 405): Some of those who wear the white [the Neoplatonists] or dark [the Christian monks] mantle 23 have maintained that I am faithless to philosophy (µε παρανoµεῖν εἰς ϕιλoσoϕίαν), apparently because I profess grace and harmony of style (ἐπαΐoντα κάλλoυς ἐν λέξεσι καὶ ῥυθµoῦ), and because I venture to say something concerning Homer and concerning the figures of the rhetoricians. In the eyes of such persons, one must hate literature in order to be a philosopher and must occupy himself with divine matters only (ὡς δὴ τὸν ϕιλóσoϕoν µισoλóγoν εἶναι πρoσῆκoν καὶ µóνα περιεργάζεσθαι τὰ δαιµóνια πράγµατα). [Trans. Augustine Fitzgerald] In chapters 7 and 8, in particular, the way to the contemplation of the divine is contrasted between Neoplatonists and Christians. Both have identical aims, but the method employed by the philosopher makes him superior (Dio 9, 48cd): But as regards the intervening process, our native philosopher has shown himself the sounder thinker (τἀν µέσῳ δὲ ὁ ἡµεδαπὸς ϕιλóσoϕoς ἄµεινoν ἔσκεπται), for he has prepared himself a road and ascends it as if it were a ladder (ὁδὸν γὰρ παρεσκευάσατo καὶ κλιµακηδὸν ἄνεισιν), so that the ascent is in some degree his own achievement, since as he advances he will probably encounter somewhere his soul's desire. And even if he does not encounter it, at all events he has advanced on his road, and this is no small matter; even thus he would differ from the bulk of mankind as much as they do from the beasts of the field. [Trans. Augustine Fitzgerald] Christian monks, in contrast to the Neoplatonic philosophers, are markedly antisocial (Dio 7, 45d): I have ere observed even men of foreign race, of both these noble classes (βαρβάρoυς ἀνθρώπoυς ἐξ ἀµϕoῖν τῶν ἀρίστων γενῶν), men who professed a contemplative existence (θεωρίαν µὲν ὑπεσχηµένoυς), and for that reason took no part in public life and became unsociable (κατὰ τoῦτo ἀπoλιτεύτoυς τε καὶ ἀκoινωνήτoυς ἀνθρώπoις) in their haste to release themselves from nature. They had sacred songs, holy symbols, and certain ordered approaches to the Divinity (πρóσoδoι πρὸς τὸ θεῖoν). All these things cut the men off from turning to matter, and they pass their lives apart from each other (βιoτεύoυσι χωρὶς ἀπ' ἀλλήλων), so as neither to see nor to hear anything pleasant (µή τι χάριεν ἰδεῖν ἢ ἀκoῦσαι).\nIn saying so much about the men in question, one would not overshoot the mark.\n\n[Trans. Augustine Fitzgerald]\nThe image of these asocial men living apart from each other is of course reminiscent of the Cyclops, as presented by Homer in the Odyssey (ix, 112-115), although the Homeric verse quoted here by Synesius 24 , besides fitting well for the Cyclops themselves, is usually applied, as is well known, to the gods, so that Synesius must have also had in mind the famous passage from Aristotle (Politics 1253a): Man is by nature a political animal (ὁ ἄνθρωπoς ϕύσει πoλιτικὸν ζῷoν), and a man that is by nature and not merely by fortune citiless is either low in the scale of humanity or above it-like the \"clanless, lawless, hearthless\" (ἀϕρήτωρ ἀθέµιστoς ἀνέστιoς) man reviled by Homer (Iliad IX 63). [Trans. H. Rackham] This denunciation of the antisocial and inhuman character, more characteristic of beasts or gods than of men, is a criticism similar to that already made of the monks by the emperor Julian named the Apostate, although with a fierier tone (Letter to the high priest Theodorus 89b6-9): There are those who seek the deserts instead of the cities (τὰς ἐρηµίας ἀντὶ τῶν πóλεων)-although man is by nature a social and civilised being (ὄντoς ἀνθρώπoυ ϕύσει πoλιτικoῦ ζῴoυ καὶ ἡµέρoυ)-led astray by the power of evil spirits, by whom they are forced into such hatred of their own humanity (µισανθρωπίαν).\nIn contrast to the careful education of the Neoplatonic elites, the Christian monks must certainly have appeared to be rude and profoundly uneducated peasants. For Synesius, the monks are βάρβαρoι, while the philosophers, trained in the traditional παιδεία, are the only Greeks, Hellenes, in the double sense already known in this period: pure Greek for some, pagan for the others. The association with the barbarians is not trivial: Eunapius of Sardis, in his History (written in approximately 400), believed that the barbarians had used Christian monks in their attacks against the Roman Empire (fr. 48, 2. 14-19 Blockley): They also had with them some of the tribe of so-called \"monks\" (τῶν καλoυµένων µoναχῶν), whom they had decked out in imitation of the monks amongst their enemies. The imitation was neither laborious nor difficult, but it sufficed for them to trail along grey cloaks and tunics (ἐξήρκει ϕαιὰ ἱµάτια σύρoυσι καὶ χιτώνια) to both become and be accepted as evildoers. The barbarians used these devices to deceive the Romans, since they shrewdly observed that these things were respected amongst them. [Trans. R. C. Blockley] The historical context is particularly harsh in this respect: in 395, Alaric's troops had invaded parts of Greece and devastated Corinth, Argos and Sparta, among other cities, where temples were destroyed, and pagan intellectuals murdered. Regardless of the historical reality of the mimetic phenomenon for delusory purposes that Eunapius denounces here, it is clear that the association between monks, evildoers and barbarians was common among the Neoplatonists of the time.\nOn the other hand, as is well known, Christian thinkers unhesitatingly adhered to the defence of the so-called βάρβαρoς ϕιλoσoϕία: in debate with various pagan intellectuals, echoes of which can still be read in the preface of Diogenes Laertius' Lives of the eminent philosophers (I 1-3), many Christian authors-such as Tatian, Eusebius of Caesarea, Clement of Alexandria, Athanasius of Alexandria and Theodoret of Cyrus, to give only the most obvious examples-defended Egyptian, Indian, Persian and Jewish philosophy as the source from which the first Greek philosophy drew and even identified the mythical poet Musaeus with Moses. There were also Neoplatonic philosophers who favored the exoticism of non-Hellenic peoples, in particular the Chaldeans, who were seen as purer and simpler, especially when it came to the practice of theurgy: well-known are the statements of Iamblichus in favor of the Barbarian philosophy in his opuscule On the Mysteries, and the insistence of the Chaldean Oracles on preserving even the ὀνóµατα βάρβαρα because they lose their theurgic efficacy if translated into Greek 25 .\nOn the contrary, Synesius defends the formative path of the traditional Greek παιδεία, which allows philosophers to be better prepared to reach union with divinity, because they are trained for it and rationally control the process, while monks reach it only by divine possession, as in a completely irrational and uncontrollable Bacchic frenzy 26 (Dio 8, 47d-48c): But their procedure is like Bacchic frenzy-like the leap of a man mad or possessed (βακχείᾳ καὶ ἅλµατι µανικῷ δή τινι καὶ θεoϕoρήτῳ)-the attainment of a goal without running the race, a passing beyond reason without the previous exercise of reasoning (καὶ µὴ κατὰ λóγoν ἐνεργήσαντας εἰς τὸ ἐπέκεινα λóγoυ γενέσθαι). For the sacred matter [contemplation] is not like attention belonging to knowledge, or an outlet of mind, nor is it like one thing in one place and another in another. On the contrary-to compare small and greater-it is like Aristotle's view (About philosophy, fr. 15 Rose) that men being initiated have not a lesson to learn, but an experience to undergo and a condition into which they must be brought, while they are becoming fit (for revelation). Now, the state of fitness for revelation also is irrational, and if reason play no part in preparing it, much more so (ἡ ἐπιτηδειóτης δὲ ἄλoγoς· εἰ δὲ µηδὲ λóγoς αὐτὴν παρασκευάζoι, πoλὺ µᾶλλoν). [Trans. Augustine Fitzgerald] Thus, Synesius follows Plotinus, who defended rationalism against those who claimed to have special revelations (Enneads II 9) 27 . The only ones who are dispensed from this long process of παιδεία in their ascent to the contemplation of the divine are, for Synesius, four representatives of religious excellence, divine men who are capable of direct contact, without prior preparation, with the divinity: Hermes (Trismegistus, probably), Amus, Zoroaster and Anthony-surely saint Anthony the first hermit, according to the tradition inaugurated by the famous biography dedicated to him by St. Athanasius (Dio 9, 48d) 28 .\nThere is, moreover, a theme that is also fundamental in the παιδεία that Synesius defends: since the human being is not a pure intellect, philosophy must be sensitive to beauty, the philosopher must want to experience pleasure (Dio 6, 45a): For God has made pleasure to be a fastening for the soul by which it supports the proximity of the body (ὁ γὰρ θεὸς τὴν ἡδoνὴν περóνην ἐπoίησε τῇ ψυχῇ, δι' ἧς ἀνέχεται τὴν πρoσεδρείαν τoῦ σώµατoς). Such then is the beauty of literature. It does not go down towards matter, nor does it dip the mind in the lowest powers, but rather gives it force to rise up in a moment and to hasten upwards to real being, for even the low part of such a life is high. [Trans. Augustine Fitzgerald] This is the meaning of χαρίειν in Synesius, contrary, of course, to the way of life of the Christian monks, who distrust leisure, τὴν σχoλὴν ὑϕεωρῶντo (Dio 7, 46c), and who live separated from the world precisely \"so as neither to see nor to hear anything pleasant\" (µή τι χάριεν ἰδεῖν ἢ ἀκoῦσαι), as we have seen above (Dio 7, 45d). On the contrary, according to Synesius (Dio 8, 47c): Thus the Greek trains his perceptions by his pleasures, and even out of sport derives advantage for his most important object (ἀνὴρ ῞Ελλην καὶ oἷς τρυϕᾷ τὴν ἐπιβoλὴν γυµνάζει, καὶ ἀπὸ τῆς παιδιᾶς εἰς τὴν πρώτην ὑπóθεσιν ὄϕελoς ἄρνυται). Further, to exercise the critical faculty, to compose a prose or poetical work, is not outside of the province of mind (oὐκ ἔξω νoῦ). Again, to purify and polish one's style, to find the main argument, to arrange it in order, and to recognize it when arranged by another, how can all these things be matters devoid of interest, and mere toys (ἀσπoύδαστα παίγνια)? [Trans. Augustine Fitzgerald] The model for this vindication of the pleasure of the wise man, against Christians and some Neoplatonists, is sought by Synesius (Dio 15, 59a) in none other than Aspasia of Miletus 29 , the famous mistress of Pericles, usually described as a hetaira, whom Socrates claims to have been his master of rhetoric (Plato, Menexenus 236bc) 30 and with whom a certain erotic relationship is also attributed to him (Athenaeus V 219de; XIII 599ab). Synesius' argument in favor of the pleasure that the philosopher must wish to derive from literature, therefore, provocatively covers the image of the relations between Plato's own hero, Socrates, and a prostitute, while at the same time attacking the practices of the Christian monks: when they descend from the heights of divine contemplation, they run the risk of sinking into matter after the fall, whereas the philosopher can rest his spirit in an intermediate stage, that is the cultivation of literature (Dio 8, 47a), which allows him to rest without sinking into the mire of matter (Dio 6, 44c-45b). Instead, the anchorites fill their idleness by making wicker baskets (Dio 7, 46c): What, after all, is the meaning of their baskets (κάλαθoι) and of the wickerwork objects which they handle, if not to signify first of all that they were human beings at a given moment; in other words, were paying attention to matters here below? For they are not in a state of contemplation at the moment when they are dealing craftily with the wicker objects. [Trans. Augustine Fitzgerald] Of course, Synesius is here accusing the monks-particularly the Egyptian monks, who are reputed for this kind of manual labor-of something that goes directly back to Plato: βαναυσία. As Andrea Wilson Nightingale (1995, pp. 55-59) convincingly argued, Plato seizes on the rhetoric of βαναυσία, typical of aristocrats in expressing their contempt for manual, menial labor, and reuses it to define the opposition between the philosopher, engaged in the divine, and the non-philosopher, engaged in manual labor. The most significant passage is Symposium 203a: 31 The gods do not mingle with men, but all dealings and dialogue between the gods and men takes place through him, whether while they are awake or while they are asleep. And he who is skilled in such things is a demonic man, while he who is skilled in anything else, whether in a trade or in manual work, is a craftsman (ὁ µὲν περὶ τὰ τoιαῦτα σoϕὸς δαιµóνιoς ἀνήρ, ὁ δὲ ἄλλo τι σoϕὸς ὢν ἢ περὶ τέχνας ἢ χειρoυργίας τινὰς βάναυσoς). The daily grind of manual labor, then, associated with the baseness of the material and the corporeal in its coarsest and most pressing sense, forms part of the Neoplatonic critique against the habits of the supposed Christian philosophers: although the aims are similar, Synesius is blunt in describing the asceticism practiced by Christian monks as excessive and irrational, in contrast to the Neoplatonic tradition. On the contrary, he advocates the defense of gradual and rationally controlled asceticism, which implies the safeguarding of pagan παιδεία, and particularly of pagan literature (Dio 9, 49d-50a; 10, 51ab; 52a; 52d), in any case not tied to manual work, to the material: to the body. Literature is, for Synesius, that intermediate state between the soul and the body that allows us to flee from the corporeal even in the moments when pure contemplation ends. Not having this option, monks run the constant risk of falling into the vileness of the body from the heights of contemplation: they are thus always more subject to the flesh, since, despising literature and the pleasures associated with it, they have no middle way between vile manual labor and pure contemplation. In this sense too they are barbarians, because of their deliberate lack of Hellenic παιδεία, which is the very criticism also by Eunapius, as we have seen: their attachment to the corporeal (τὸ ϕιλoσώµατoν) is not only considered to be opposed to philosophical practice (VS VIII, 1, 3) but also something that belongs to barbarians (VS VI 5,8).\nIn the same vein, the definitive argument by Synesius focuses on what is, as we pointed out, the most notable difference between Christians and Neoplatonists at the end of the fourth and the beginning of the fifth century: the monks-this supposedly new way of philosophy claimed by Christians-seek to reach the divine with their whole body, when it is only the pure intellect that is capable of rising in this way. This is how Synesius puts it (Dio 9, 49c; 50c): For it seems dangerously near impiety to suggest that the Divinity will dwell in any other part of us than in the mind (ἄλλῳ τῳ τῶν ἐν ἡµῖν oἴεσθαι τὸν θεὸν ἐνδηµήσειν ἀντὶ τoῦ νoῦ), since that is God's own temple (νεὼς γὰρ oὗτoς oἰκεῖoς θεῷ). [. . . ] In very truth we should gain benefit from the virtues in becoming disentangled of a partiality for matter (τῆς ὑλικῆς πρoσπαθείας). But an uplifting force is needed (δεῖ δὲ καὶ ἀναγωγῆς), for it is insufficient that a man be not evil, he must even be a god (δεῖ καὶ θεὸν εἶναι). And this state most resembles the turning away from the body and as many things as are of the body, and the turning, through the intellect, to God (καὶ ἔoικεν εἶναι τὸ µὲν oἷoν ἀπεστράϕθαι τὸ σῶµα καὶ ὅσα τoῦ σώµατoς, τὸ δὲ oἷoν ἐπεστράϕθαι διὰ νoῦ πρὸς θεóν). [Trans. Augustine Fitzgerald] It is the same statement by Plotinus, as we have seen above: contemplation of the divine can only be exercised from the body, never with the body. Thus, the monks, according to Synesius, made a serious mistake, despite, so to say, their good intentions and their proximity to the aim of Neoplatonic philosophy, because of their attachment to the corporeal, which is the result of their lack of a true Hellenic philosophical education-an absolutely repulsive love of the corporeal, especially in relation to the cult of the martyrs, in the less sympathetic view of Eunapius.\n\nSome Concluding Remarks\nThus, the point of view of the Greek intellectuals trained in Neoplatonism, in the transition from the fourth to the fifth century, seems quite clear and generalized: the Christian majority-in particular its most exalted members, the monks-is irrational, abhors culture, inhumanly despises the pure pleasures associated with the cultivation of literature, and, while practicing an asceticism similar to their own, does so without knowing exactly why, while loving the corporeal, indulging in menial manual labor and resembling the barbarians, in their immediate historical representation but also, of course, in their literary image, common in the tradition of ancient Greek culture. All of which prevents them from adequately reaching the contact with divinity, which is the shared goal of both groups. Attachment to the body and the corporeal thus characterizes Christians at this time as a burden that prevents them from ascending to the heights of true Neoplatonic contemplation, which can be exercised only with the soul. Even worse: this markedly \"philosomatic\" character of early Christianity puts at risk the very formation of the intellectual and the continuation of the παιδεία itself.\nThese voices, before being silenced, still resounded loudly at a time when the Cappadocian Fathers-no less Neoplatonist than Synesius or Eunapius, but of a Neoplatonism already filtered by the Alexandrian Christians, which accepts only what is not contrary to Scripture and Christian dogma-claimed that the true philosophy was Christian philosophy, identified with contemplation and monastic life 32 , and opposed precisely the cultivation of rhetoric and traditional παιδεία, which are understood as sophistications that kill the simplicity and plainness of the Christian message. On the contrary, for Synesius, no less than for Eunapius, the way of life of Christian monks is not and cannot be, in any way, a philosophical life, which can only be possible if it is purely spiritual, completely detached from the despicable garment of the body and its daily grind. Additionally, Christianity is undoubtedly, at this moment, a body friendly religion: the religion of the ϕιλoσώµατoν γένoς.\n\nConflicts of Interest:\nThe author declares no conflict of interest.\nNotes 1 I will only note, among the abundant bibliography, the already canonical studies by Brown (1988) and Elm (1994). For a more particularly sexually oriented approach, the following are essential references: Clark (1999); Gaca (2003); and Harper (2013). 2 For a detailed analysis of Plato's conceptions of the soul, among the vast bibliography, the studies by Robinson (1970);and Steiner (1992) are particularly interesting. For connections with Orphism and Pythagoreanism, see, specially, Long (1948); Burkert (1962); Kalogerakos (1996); Casadesús Bordoy (2008). See also a discussion in Claus (1981, pp . 1-7). 3 The main study about these traditional conceptions about the soul in ancient Greece is still Claus (1981). For a brief and clear summary of the evolution of the concepts of body and soul from Homer to Plato, Bartoš (2006) is particularly useful. 4 See specially Plato, Phaedo 81e-82b, 113a; Menexenus 81b; Resp. 620e; Phaedrus 248c-249b; Timaeus 41e-42d. However, there is some evidence for the existence of a transmigration theory before Plato: see Burkert's (1962) and Claus (1981)  This is not the place, of course, to enter into the controversy about the extent to which dualism as it is presented in the philosophical tradition really belongs to the Platonic postulates or if it is a later philosophical construct: see Vasiliu (2015). 7 See specially El Murr (2018); and Tarrant (2020). On the most controversial point, that of the sceptical drift of Platonism in the evolution of the Academy, see Bonazzi (2003). 8 See also the main argumentation in Plotinus, Enneads I, vi, 6, 13; III vi, 5, 18; IV iii, 4, 23, and Porphyry, Sententiae ad intelligibilia ducentes 7. 9 Geffcken (1920, p. 217), pointed out the contrast between the criticism of Synesius, which, as we shall see, is more measured, and the furious and unmitigated attack of a pure pagan like Eunapius. 10 Eunapius is usually dated between 347-349 and 414, although the dates cannot be said to be certain: see Goulet (1980); Blockley (1981, I, ix, 1); Banchich (1987, pp. 164-67); Penella (1990, pp. 2-4); and, especially, the longer and more recent discussion of the testimonies in Goulet (2014, pp. 5-34). 11 On this issue, see the recent comprehensive study by Loudovikos (2019). 12 On early Christian philosophy, see Karamanolis (2021, pp. 166-99). 13 See specially Against the Christians, fr. 35, 92 and 94 Harnack = 117D Becker. 14 Apud Origen, Contra Celsum VII 39. 15 Enneads II ix, 18, 41-42. 16 Origin, Contra Celsum IV 40. 17 His Neoplatonism is fundamentally Porphyrian, as is especially evident in the Hymns, where the difficulties of fitting the Neoplatonic triad habitual in the Chaldean Oracles with the Christian Trinity are also evident, as rightly explained by Vollenweider (1985); see also Garzya (1981). For an excellent summary of Synesius' position in the intellectual context of his time, see Di Pasquale Barbanti (1994). 18 See the seminal study by Lacombrade (1951), the summary by Ramos Jurado (1992), and, especially, the recent thorough analysis by Criscuolo (2016). 19 As proposed by Cameron and Long (1993, pp. 28-34), following Marrou (1952, p. 479), andMarrou (1963, pp. 141-42). Bregman (1982) and Tanaseanu-Döbler (2008, p. 286), considered Synesius a Neoplatonist who accepted the bishop's mitre out of pure pragmatism, without ever abandoning his pagan convictions, but these conceptions no longer seem to have any credibility among specialists today. 20 See the excellent presentation of the problem by Toulouse (2016). 21 For a thorough analysis of the Dio, see the essential studies by Treu (1958); Garzya (1972); Piñero (1975); Quevedo Blanco (2011). 22 In order to properly contextualise Synesius ideological position, the study by Garzya (1968) is still important. More recently, Op de Coul (2012) has revised the subject. 23 Or perhaps also cynical street philosophers, according to the comparison made by Julian the Apostate, Orat. VII, 244ac, as Treu (1958) remarked in his commentary on the passage, which is also argued by Vollenweider (1985, pp. 19-20). On the form of philosophical cloak dressing also adopted by Christians, see Urbano (2013). 24 Synesius often uses Homeric passages in his exposition, for purposes well analysed by Pizzone (2006). 25 According to the testimony of Michael Pselos (expos. Or. Chald. 1132C). It is the same idea that reappears in the Corpus Hermeticum xvi 2. 26 It is worth remembering at this point that Orphic images are frequent both in Neoplatonism and among the authors of early Christianity: we refer to the study on the subject by Herrero De Jáuregui (2010). 27 For Plotinus, even mystical union must follow a path of discipline of the intellect that has nothing to do with theurgy or magic, very similar to what Synesius describes here, although with particular nuances, of course. See, above all, Enneads I, 6,9;VI,7,34. 28 Some scholars have tried to identify Amus and Zoroaster with two Christian Gnostics: see Lacombrade (1988). Most likely, however, it is Amon, the Egyptian king of Plato, Phaedrus 274c, and the Iranian Zoroaster, well known in the Greek philosophical tradition. 29 It has been clearly analysed by Azevedo (2003). Azevedo goes so far as to affirm that, under the figure of Aspasia, we must understand a transcript of Hypatia, Synesius' master, who is perhaps the one who would have instilled in him this desirable harmony between philosophy and rhetoric, between wisdom and pleasure."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-06"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2orc/valid"
            ]
          }
        ]
      },
      {
        "id" : 135364,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254331857"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Between Chemism and Life: Is Hegel's Teleology Misplaced?\n\n\n In this paper, I raise a question concerning the place of ‘Teleology’ in Hegel's system of logic and ask whether ‘Teleology’ as a logical category can and should come immediately before ‘Life’. I offer two main reasons to think that the category of ‘Teleology’ might be misplaced. The first and the indirect reason is inspired by a difference between the logical system and the Philosophy of Nature concerning the immediate precursors and the emergence of life as a logical category and real determinacy. In Hegel's Logic, ‘Teleology’ is interposed between ‘Chemism’ and ‘Life’, while in his Philosophy of Nature, ‘Organics’ immediately follows ‘The Chemical Process’. Although the systematic order of the natural determinacies laid out in the Philosophy of Nature has no authority over the sequence of logical determinacies, and although there does not have to be a one-to-one correspondence between the logical categories and natural determinacies of Hegel's system, I argue that the smooth transition from the chemical process to the self-sustaining totality of the geological organism in the Philosophy of Nature, is an incentive to consider a parallel transition in the logical exposition, which I show to be workable. The second and the direct reason is that Hegel's category of ‘Teleology’ cannot but make a crucial reference to the initial determinacy of the category of ‘Life,’ without which it is inconceivable. By explaining why this reference is untenable and how, by contrast, the initial determinacy of life is conceivable independently of the process in which some subjective end is realized in objectivity through external means, I conclude that the logic of ‘Life’ and internal teleology should precede the logic of external teleology, allowing for a direct passage from ‘Chemism’ to ‘Life’."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-05"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 138747,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255213689"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "A Critical Edition of the Works by Gaston Bachelard\n\nWorks by Gaston Bachelard, both in the field of the philosophy of science and in the field of the philosophy of poetic imagination, were reissued in France many times. These reissues were unchanged reprints of the origi‐ nal editions, which were always very interesting in terms of content, but sometimes underdeveloped in terms of form. Obviously, this is not the most important aspect of these works, as we can undoubtedly agree with Jean‐Jacques Wunenburger. 1 Nevertheless, a disordered form, especially in terms of bibliographic references, is a certain difficulty for researchers of this philosophy and may be a prelude to its unauthorized readings. Let us add that Bachelard’s works originally came out in various publishing houses, including Librairie José Corti, Les Presses universitaires de France, Librairie philosophique J. Vrin, Librairie Félix Alcan, Les Éditions Gallimard. It had an impact on the availability of individual titles, as they were reis‐ sued on different terms and with different frequency."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-27"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 141865,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255026072"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Althusser’s Spinozism: A Philosophy for the Future?\n\nThe ‘second’ Spinoza-Renaissance generally refers to 1960s France, where a small group of scholars—Gueroult,1 Matheron,2 and Deleuze3 among them—advanced a new image of Spinoza that broke with the then still widely dominant one developed by Hegel and his students and commentators. More attentive reconstructions of this period include Althusser among the Parisian musketeers of Spinozism, if only as a marginal reference. In fact, any effort to identify a text of Althusser’s specifically on Spinoza must surely end in disappointment. The works published in his lifetime include only a handful of brief references to Spinoza—none longer than a paragraph. And neither his extensive posthumous work nor his archived writings contain texts dedicated to Spinoza.4 An entirely different task was occupying Althusser’s time in the sixties: the theoretical renewal of Marxism. It is his later recollection of this task that contains his now famous confession of Spinozism:"
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-22"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 142251,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255290867"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "POSTMODERN PARODY AS ONTOLOGIZATION OF REALITY (BASED ON THE STORY BY D. BARTHELMY \"A SHOWER OF GOLD\")\n\nThe article examines the problem of postmodern parody from the point of view of its ontologizing potential, namely the interpretation within its limits of the problem of the subject. The concept of ontologization is interpreted in the tradition of the opposition to the paradigm of the philosophy of presence to extra-subjective versions of ontology, in particular the postmodern concept of intertextuality. The actualization of the parody concept as a formative tool along with the explicit or implicit accentuation of the integrity of the world picture is interpreted as a search for metaphysical grounds for the aesthetic system of postmodernism and D. Barthelemy's poetics. The article substantiates the philosophical aspect of the interpretation of parody accepted in the scientific literature as a mode of organization of the structure of the text and the form-content of the text in general for postmodern aesthetics. The universal nature of parody as an interpretive optics and postmodern technique of textual collage for literature in the conditions of the visual turn of the 20th-21st centuries is outlined. It has been found that the use of the collage technique in the form of cataloging reality as its reformatting is inherent in the poetics of D. Barthelemy. It is shown that parody for the writer is a formal and substantive principle, an artistic and aesthetic instruction, and not just a technique. The demonstrativeness, or \"nakedness\" of the parody is achieved by changing phrasal modes, alternating dialogues and descriptions, and visualizing the action. The structural levels of parody overlap both at the formal intertextual level and at the level of numerous pretexts and supporting semantic and symbolic series. Therefore, the transgression of parody manifests itself through the unity and separation of the coder and the decoder at the moment of forming the ethos of the fragmented text as its experience. It is concluded that grotesque and parody can be considered as synesthetic sense-generative models of postmodern literature. Parody as a vital receptive and cognitive form can ontologize the subject of the text. The grotesque is a paradoxical, finalized, objectified entelechy of parody, which is practically never ontologized and does not acquire features of subjectivity. Postmodern parody reformats the subject of the narrative and the text as a formal structure, thus giving it additional semantic dimensions."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-23"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 154926,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "255211729"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "On the Question of Author and Hero in M.M. Bakhtin’s Philosophical Anthropology of the Early Period\n\nThis article is an ontological and existential interpretation of Mikhail Bakhtin’s early works (Art and Answerability; Toward a Philosophy of the Act; Author and Hero in Aesthetic Activity), containing a deep philosophical-anthropological and innovative, in historical and philosophical terms, dialectic of author and hero. The study suggests that during his years in Nevel and Vitebsk Bakhtin strove for an ontological and ethical deepening of the aesthetics of verbal creativity, asserting (in the ontological plane) the being of the hero and, accordingly, the ontological status of the author and the literary work. Unlike with being (an objectively outside position), it is possible to argue with the “author” (a subjectively involved position); the existing (genuine) author is present in the literary work in the form of events that are transgredient (out-of-reach) to the hero’s consciousness. While a hero’s life unfolds as a sequence of actions, the real author is hidden in the hero’s fate (in fact, his own fate) and must be responsible for him. In order to embrace his life in a holistic event, an individual needs to “disappear” as a psychological subject in the practical procedure of ontological conversion of consciousness and relate himself to the moments of his life that are transgredient to his personality. The sum of such moments can be called human destiny. In order to be a full-ledged, real author for the hero, the “subject” must become an author for himself (regardless of whether he is going to become an artist or not). As a result, an objective ontological and existential situation arises, in which the hero and the author undergo an ontological and aesthetic conversion (neutralization of bad subjectivity)."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-15"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      },
      {
        "id" : 156466,
        "content" : [
          {
            "name" : "id",
            "values" : [
              "254395099"
            ]
          },
          {
            "name" : "text",
            "values" : [
              "Interdisciplinary Links of Speech Therapy for Individuals or Children with Special Needs\n\nThe article highlights the issue of interdisciplinary links in speech therapy, in particular, the philosophy of language. Given that the number of children with special educational needs is increasing, the relevance of the article is indeed justified. Therefore, it is necessary to analyze the problem in question more in detail. The article aims to a) clarify such concepts as “language”, “philosophy of language”, “hermeneutics”, “speech therapy”, b) analyze an interdisciplinary approach to preventing and correcting speech disorders in children and adults and c) study the causes of such disorders. Research methods include a detailed analysis of scientific sources, as well as a systematic analysis. As shown by the relevant recent work in this area, the interaction of methods from neurology, neurophysiology and neuropsychology contributes to restoring the functional system of language and speech. Speech disorders are mostly typical for people diagnosed with autism, attention deficit disorder, hyperactivity disorder, Huntington’s disease, sclerosis, dementia, and mouth or throat cancer. The novelty lies in the fact that speech therapists should be able to use neurostimulation technologies in the course of corrective work. In conclusion, speech therapy should follow an interdisciplinary approach so that specialists can make an effective diagnosis of speech disorders in children and adults."
            ]
          },
          {
            "name" : "date",
            "values" : [
              "2022-12-06"
            ]
          },
          {
            "name" : "source",
            "values" : [
              "s2ag/valid"
            ]
          }
        ]
      }
    ]
  },
  "status" : {
    "status" : "AVAILABLE",
    "elapsedMs" : 0,
    "progress" : [
      {
        "task" : "Resolving selector query",
        "status" : "SKIPPED",
        "attributes" : [
          {
            "name" : "Skipped",
            "value" : "cached"
          }
        ]
      },
      {
        "task" : "Fetching candidate labels",
        "status" : "SKIPPED",
        "attributes" : [
          {
            "name" : "Skipped",
            "value" : "cached"
          }
        ]
      },
      {
        "task" : "Scoring candidate labels",
        "status" : "SKIPPED",
        "attributes" : [
          {
            "name" : "Skipped",
            "value" : "cached"
          }
        ]
      },
      {
        "task" : "Computing coverage",
        "status" : "SKIPPED",
        "attributes" : [
          {
            "name" : "Skipped",
            "value" : "cached"
          }
        ]
      }
    ]
  },
  "spec" : {
    "scope" : {
      "limit" : "10000",
      "selector" : {
        "type" : "byId",
        "ids" : [
          120716,
          132737,
          91297,
          116752,
          10796,
          132184,
          104652,
          127134,
          77736,
          132799,
          105105,
          93909,
          44525,
          11281,
          10870,
          23282,
          142251,
          5681,
          117968,
          15350,
          106097,
          58677,
          438,
          100518,
          154926,
          112380,
          83401,
          141865,
          47489,
          23795,
          28965,
          61474,
          44356,
          83935,
          128455,
          43875,
          130758,
          135364,
          138747,
          75412,
          2792,
          156466,
          70875,
          110251,
          8248,
          13424,
          87120,
          16487,
          17522,
          18140,
          54219,
          7794,
          95319,
          95034
        ]
      }
    },
    "labels" : {
      "minLabels" : 0,
      "maxLabels" : 20000,
      "labelCountMultiplier" : 3.5,
      "source" : {
        "fields" : [ ]
      },
      "surface" : {
        "minWordCount" : 1,
        "maxWordCount" : 8,
        "minCharacterCount" : 4,
        "minWordCharacterCountAverage" : 2.9,
        "preferredWordCount" : 2.5,
        "preferredWordCountDeviation" : 2.5,
        "singleWordLabelWeightMultiplier" : 0.5,
        "multiWordLabelPriority" : false,
        "capitalizedLabelWeight" : 1.0,
        "uppercaseLabelWeight" : 1.0,
        "acronymLabelWeight" : 1.0,
        "withEmbedding" : false,
        "exclude" : [
          {
            "type" : "all"
          }
        ]
      },
      "frequencies" : {
        "minAbsoluteDf" : 2,
        "minRelativeDf" : 0.0,
        "maxRelativeDf" : 0.4,
        "maxLabelsPerDocument" : 5,
        "truncatedPhraseThreshold" : 0.2
      },
      "probabilities" : {
        "autoStopLabelRemovalStrength" : 0.35,
        "autoStopLabelMinCoverage" : 0.4
      },
      "scorers" : {
        "tokenCountScorerWeight" : 1.0,
        "tfScorerWeight" : 1.0,
        "idfScorerWeight" : 1.0,
        "completePhrasesScorerWeight" : 1.0,
        "truncatedPhrasesScorerWeight" : 1.0,
        "tokenCaseScorerWeight" : 1.0
      },
      "arrangement" : {
        "enabled" : false,
        "algorithm" : {
          "type" : "ap",
          "ap" : {
            "maxIterations" : 2000,
            "minSteadyIterations" : 100,
            "threads" : "1-2",
            "softening" : 0.9,
            "damping" : 0.9,
            "minPruningGain" : 0.3,
            "inputPreference" : 0.0,
            "preferenceInitializer" : "NONE",
            "preferenceInitializerScaling" : 1.0
          }
        },
        "relationship" : {
          "type" : "cooccurrences",
          "cooccurrences" : {
            "cooccurrenceWindowSize" : 32,
            "cooccurrenceCountingAccuracy" : 0.5,
            "similarityWeighting" : "INCLUSION",
            "threads" : "1-4"
          },
          "embeddings" : {
            "maxSimilarLabels" : 64,
            "minSimilarity" : 0.5,
            "threads" : "1-4"
          }
        }
      },
      "direct" : [ ]
    },
    "documents" : {
      "arrangement" : {
        "enabled" : false,
        "algorithm" : {
          "type" : "ap",
          "ap" : {
            "maxIterations" : 2000,
            "minSteadyIterations" : 100,
            "threads" : "1-2",
            "softening" : 0.5,
            "damping" : 0.9,
            "minPruningGain" : 0.3,
            "inputPreference" : -1.0,
            "addSelfSimilarityToPreference" : false
          },
          "maxClusterLabels" : 3,
          "maxLabelsPerDocument" : 10
        },
        "relationship" : {
          "type" : "mlt",
          "mlt" : {
            "maxSimilarDocuments" : 8,
            "minDocumentLabels" : 1,
            "maxQueryLabels" : 4,
            "minQueryLabelOccurrences" : 1,
            "minMatchingQueryLabels" : 1,
            "maxScopeSizeForSubIndex" : 0.3,
            "maxInMemorySubIndexSize" : 8000000,
            "threads" : "1-4"
          },
          "embeddingCentroids" : {
            "maxSimilarDocuments" : 16,
            "minDocumentLabels" : 1,
            "maxQueryLabels" : 4,
            "minQueryLabelOccurrences" : 1,
            "threads" : "1-4"
          }
        }
      },
      "embedding" : {
        "enabled" : false,
        "algorithm" : {
          "type" : "lv",
          "lv" : {
            "maxIterations" : 300,
            "negativeEdgeCount" : 5,
            "negativeEdgeWeight" : 2.0,
            "negativeEdgeDenominator" : 1.0,
            "threads" : "1-4"
          }
        },
        "relationship" : {
          "type" : "mlt",
          "mlt" : {
            "maxSimilarDocuments" : 8,
            "minDocumentLabels" : 1,
            "maxQueryLabels" : 4,
            "minQueryLabelOccurrences" : 1,
            "minMatchingQueryLabels" : 1,
            "maxScopeSizeForSubIndex" : 0.3,
            "maxInMemorySubIndexSize" : 8000000,
            "threads" : "1-4",
            "maxSimilarDocumentsPerLabel" : 5
          },
          "embeddingCentroids" : {
            "maxSimilarDocuments" : 16,
            "minDocumentLabels" : 1,
            "maxQueryLabels" : 4,
            "minQueryLabelOccurrences" : 1,
            "threads" : "1-4",
            "maxSimilarDocumentsPerLabel" : 5
          }
        }
      }
    },
    "output" : {
      "format" : "json",
      "parameters" : { },
      "pretty" : false,
      "labels" : {
        "enabled" : true,
        "labelFormat" : "ORIGINAL",
        "documents" : {
          "enabled" : false,
          "maxDocumentsPerLabel" : 10,
          "outputScores" : false
        }
      },
      "documents" : {
        "enabled" : true,
        "onlyWithLabels" : true,
        "onlyAssignedToLabels" : false,
        "labels" : {
          "enabled" : false,
          "maxLabelsPerDocument" : 2147483647,
          "minLabelOccurrencesPerDocument" : 0
        },
        "content" : {
          "enabled" : true,
          "fields" : [
            {
              "name" : "id",
              "maxValues" : 3,
              "maxValueLength" : 10000000,
              "highlighting" : {
                "criteria" : false,
                "scope" : false,
                "truncationMarker" : "…",
                "startMarker" : "⁌%s⁍",
                "endMarker" : "⁌\\%s⁍"
              },
              "valueCount" : false
            },
            {
              "name" : "text",
              "maxValues" : 3,
              "maxValueLength" : 10000000,
              "highlighting" : {
                "criteria" : false,
                "scope" : false,
                "truncationMarker" : "…",
                "startMarker" : "⁌%s⁍",
                "endMarker" : "⁌\\%s⁍"
              },
              "valueCount" : false
            },
            {
              "name" : "date",
              "maxValues" : 3,
              "maxValueLength" : 10000000,
              "highlighting" : {
                "criteria" : false,
                "scope" : false,
                "truncationMarker" : "…",
                "startMarker" : "⁌%s⁍",
                "endMarker" : "⁌\\%s⁍"
              },
              "valueCount" : false
            },
            {
              "name" : "source",
              "maxValues" : 3,
              "maxValueLength" : 10000000,
              "highlighting" : {
                "criteria" : false,
                "scope" : false,
                "truncationMarker" : "…",
                "startMarker" : "⁌%s⁍",
                "endMarker" : "⁌\\%s⁍"
              },
              "valueCount" : false
            }
          ]
        }
      }
    },
    "performance" : {
      "threads" : "1-4"
    },
    "summary" : {
      "labeledDocuments" : true
    },
    "debug" : {
      "logCandidateLabelPartialScores" : false
    }
  }
}